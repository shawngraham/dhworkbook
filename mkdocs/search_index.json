{
    "docs": [
        {
            "location": "/", 
            "text": "This workbook supports \n#HIST3814o\n in the Late Summer term at \nCarleton University\n. This is the third major reworking of the workbook.\n\n\nThe original \nWinter 2015 version is here.\n\n\nFor more advanced tutorials and help, please see\n\n\n\n\n\n\nThe Programming Historian\n\n\n\n\n\n\nDigital History Methods in R\n\n\n\n\n\n\nShawn Graham, shawn dot graham at carleton dot ca, \n@electricarchaeo\n\n\n\n\n\n\nOriginal content by Shawn Graham is \n\n\nlicensed under a \nCreative Commons Attribution-NonCommercial 4.0 International License\n.\n\n\nCite this workbook:", 
            "title": "Home"
        }, 
        {
            "location": "/introduction/crafting-digital-history/", 
            "text": "Getting yourself ready July 4th - 9th\n\n\n\n\n\"'Getting Ready for the Kaiser' Bain Collection, Library of Congress hdl.loc.gov/loc.pnp/ggbain.10412 Call Number: LC-B2- 2400-7\"\n\n\nWelcome! This workbook is made by converting several plain-text files into a fully-operational website using the 'mkdocs' \nstatic website generator\n. That means, if you want to keep a copy of all these files for your own records, you may. Simply click on the 'edit on github' link at the top right. This will bring you to the repository that houses this workbook. Then, when you're signed into 'github', you can 'fork' (that is, make a copy) the repo into your own account. Why 'forking'? It seems an odd phrase. Think of it like this:\n\n\nWriting, crafting, coding: these are like a small river, flowing in one direction into the future. You get new ideas, new hunches: the river branches. There's a fork in its path. Sometimes new ideas, new hunches fold back into that original stream: they merge.\n\n\nGithub is a way of mapping that stream, and a guide to revisiting the interesting parts of it.\n\n\nIt's not the best metaphor, but it'll do. No doubt you've had that experience where, after working on an essay for days, you make a change that you regret. You wish you could go back to fix it, but ctrl+z only goes so far. You realize that everything you've done for the last week needs to be thrown out, and you start over.\n\n\nWell, with 'versioning control', you can travel back upriver, back to where things were good. There are two tools that we will use to make this happen: \ngit\n and \ngithub\n. You'll learn more about making those things work in Module 1. You'll see why you'd want to do that, and how to future-proof your work, writing things in a plain-text format called \n'markdown'\n.\n\n\nIn module 2, we'll start exploring the wide variety of historical data out there. We'll talk about some of the ethical dilemmas posed by having so much data out there. 'Data' are not neutral 'things given'; rather, they are 'capta': things \ntaken\n.\n\n\n(Caveat: I am assuming, in this course, that the digital materials we want have already been digitized. Digitizing, adding meta-data (information that describes the data), and structuring it \nproperly\n are very complex topics on their own that could be the subject of a complete course! If you are interested in those problems, a good place to start is this open course from SOAS on \ndatabase design for historians\n.)\n\n\nIn module 3, we'll see that data/capta are \nmessy\n, and that they make a kind of illusory order that as historians, we are not normally in the habit of thinking about. The big secret of digital history is that the majority of your time on \nany\n digital history project won't be finding the capta, won't be analyzing the capta, won't be thinking about the historical meaning of the capta. We'll be \ncleaning it up\n. The exercises in this module will help you do that more efficiently (and be aware that 'efficiency' in computing terms is not a theoretically neutral idea!)\n\n\nWith module 4, we talk about doing the analysis. I present finding, cleaning, and analyzing as if they were sequential steps, but in reality they are circular. Movement in one aspect often requires revisiting another one! This module explores how we do this, and what it means for us as historians.\n\n\nIn module 5, we begin at last to think about how we communicate all of this to our audiences. Look at how \none university lays out the expectations for digital history work\n (and, do you see how this ties back to ideas about \nparadata\n?). We will think about things like typography and layout. We'll look at ways of making our visualizations more compelling, more effective.\n\n\nFinally, while there is no formal requirement for you to do this, it would be a great way of launching yourself as a digital historian to think about how you would formally reveal your project work in this class to the wider world: and then do it. There's a lot of noise on the internet, especially when it comes to history. How do you, as a digital historian, make the strongest possible signal?\n\n\nWhat you need to do this week\n\n\n\n\nFollow the instructions below to set up your digital history workspace\n\n\nAnnotate the course manual for any parts of it that are unclear (or alternatively, that have you excited)\n\n\nRespond to the readings and the reading questions by annotating the readings themselves - see the instructions below.\n\n\nSubmit your work \nhere\n\n\n\n\nSetting up a your workspace\n\n\nA digital historian needs to have a digital workshop/lab/studio/performance space. Such a space serves a number of functions:\n\n\n\n\na scratch pad / fail log and code repository so that we remember what we were doing, or (more importantly) \nwhat\n we we did - that is to say, the actual commands we typed, the sequence of manipulations or \ndata moves\n\n\na narrative that connects the dots, that explains the \nwhy\n of that what and how. You can use this narrative to point to when sharing your work with others. Digital history is not done in isolation or in a vaccuum. Sometimes, you will need to share a link to your work (often on twitter) asking, 'does anybody know why this isn't working?' or, 'does anybody know a better way of accomplishing this?', or, 'hey, I'm the first to do this!'\n\n\na way of keeping notes on things we've read/come across on the web. There are a number of ways of accomplishing this. In this course, I will mandate one particular solution: (\nhypothesis\n).\n\n\nwhen you're working with academic databases such as JSTOR, then you'll also need a bibliography manager. We don't go into this aspect very much in this course (if you take other courses with me, you will) but you might want to check out \nZotero\n.\n\n\nit can sometimes be useful to make little videos of your work to explain when something isn't working - \nScreen-cast-o-matic\n is free and does a good job\n\n\n\n\nNow, the final part of your studio/lab/workshop is a domain (website + complete access to the webserver that powers it, so that you can install other platforms/services) of your own. A typical setup will be something along the lines of:\n\n\n\n\nyour-domain-name.org for your narrative, annotations, and anything you build regarding your work\n\n\ngithub.com/your-name for your scratch pad and code repository\n\n\n\n\nOn your blog/narrative, you'll have a page that collates all of your web annotations as well. You'll also have - typically - an 'about' page where you can signal the kinds of history you're interested in, and the preferred way for people to get in touch with you. You do not have to use your real name. Remember the \nreal names policy\n\n\nBecause you have complete access and control over your domain, you can install other services as well. For instance, maybe you use Dropbox or Google Drive to sync your files across machines, or to function as a backup? You can install a service called 'OwnCloud' on your own domain that does the same thing, so that you have control over all your own materials.\n\n\nSo let's get started.\n\n\nReclaim Hosting\n\n\nIn the course space for cuLearn, I gave you a code to use to pay for a domain of your own. I have already purchased domain space for you from an academic web hosting service, \nReclaim Hosting\n. This space will last for one year, at which point you have the option of paying to renew it or letting it die. Previous students in this course have used their domain to help with their applications for jobs and graduate school. One such is \nMelissa\n.\n\n\n\n\nYou will be asked for the name you want to have for your space. You need to be thinking of branding here. Think of a professional name that conveys something of your personality and approach to history. I for instance own the domain, 'Electric Archaeology', which I chose to convey that I'm interested in digital archaeology, but also, that I move fast and cover a lot of breaking develops in the field (hey. It's my blog. I like the name). Please choose wisely. Some combination of your first and last name is often the best idea, since your own name is your own best calling card ('shawn graham' is such a generic name, that it was already long gone by the time I started doing digital history). Type in a name, select a top-level domain (ie, .com, .ca, .org, etc. I'll suggest .ca), and click on the 'check availability' button.\n\n\nIf the pop-up says 'congratulations, domain available!' then click on the continue button. (You may be offered free id protection, where Reclaim Hosting will hide your details is someone does a 'who-is' search on the domain name. If it does, then tick off the check box to confirm that you want this, and hit continue).\n\n\nOn the next screen, (the billing screen) fill in all of the information. The balance should be $0. At the bottom left, it will also say that you\u2019ve used a one-time promotional code. Hit the green button at the bottom to complete the purchase (which is not costing you anything).\n\n\n\n\nCongratulations! You now own your very own domain. \nIt might take a bit of time for your domain to appear live on the web\n. During this time, you can log into your cPanel and install wordpress and so on - see below.\n\n\nGiving you a space of your own is my political act of protest against the centralization of learning inside learning management systems. Learning isn't 'managed', it's cultivated. Unlike cuLearn, I cannot monitor your activity inside your own domain. I can only see what you choose to make public. Unlike Facebook or Snapchat or Instagram, I am not trying to monitize your personality on the web.\n\n\nWordpress for your blog\n\n\nWordpress is probably the best option for a platform for writing the narrative bits of your digital history work.\n\n\n\n\nClick the 'client area. It will tell you that you have one active account (with Reclaim Hosting) and one active domain. When the time comes to renew your account or to close it down, this is where you do it. Note also that there is a link to 'Support', which will put you in touch with Reclaim Hosting's help desk. They are extremely fast and good at providing support; always treat any help request you make with them \nas if\n you were writing a formal email to me. Be polite and considerate, and thank them. The owners of the business often are the ones who provide the help! Without them, we couldn't do this class.\n\n\nGo to 'cPanel' - this is where you can install all sorts of different kinds of software on your account. Search for and select 'web applications'\n\n\nClick on Wordpress. Then click on 'install this application'.\n\n\nThe next screen presents you with a number of options. Leave these set to their defaults. For 'location', leave this blank. That tells the installtron to put Wordpress at your-domain.ca . (When/if you install other pieces of software, you'd change this variable so that the new software doesn't overwrite this software!)\n\n\nFurther down the page, under 'settings', you need to change 'administrator username', 'administrator password', 'web site title', 'website tagline'. \nthis is the username and password to actually do things with your blog, and the name of your blog itself\n. Leave everything else alone.\n\n\nClick Install!\n\n\nOnce it's done, the installer will remind you of the url to your site, and the url to the blog's dashboard (where you go to write posts, make changes to the look and feel of the site). Open these in a new browser window, and bookmark them. To login to your blog, remember to go to the dashboard URL (eg, http://your-domain.ca/wp-admin), enter your blog administrator username and password.\n\n\n\n\nYou can close the cPanel webpage now (log out first).\n\n\nCustomising your blog\n\n\nIf you look at the dashboard for your blog, you'll see a lot of options down the left hand side of your screen. If you want to change the look of your blog, click on 'appearance' then click on 'themes'. A number of themes are pre-installed. You can click on the different themes and select 'preview' to see how your blog might look. When you find one you like, select 'activate'. Go to the other browser window where you have your-domain.ca open. Reload the page to see the changes take effect!\n\n\n\n\nIf you're the sort of person who likes to sketch ideas out on paper, Lauren Heywood of the \nDisruptive Media Learning Lab\n has designed a paper-based exercise to prototype ideas. Why not give it a try? \nPrint this file out\n and follow the instructions.\n\n\n\n\nTo write new content, know that there is a difference between a 'post' and a 'page'. A page is a new link on your site, while posts appear in chronological order on a page, with the most recent on top. Most themes show the most recent post by default, and pages appear in any menus on the site. \nWhen you are logged into your blog\n any post or page will have an 'edit' button at the bottom that you can click. You'll then be presented with an editing box with the standard toolbar across the top (allowing you to change the font, insert images, change the alignment of the text and so on). At the top right will be a button to save or publish/update the post/page.\n\n\n\n\nYour blog will have a default 'about' page. Change that default text now to reflect something about who you are and why you are taking this course\n\n\n\n\nTo create new pages, you click on the 'pages' link in your dashboard and select 'add new'\n\n\nTo create new posts, you click on the 'posts' link in  your dashboard and select 'add new'.\n\n\nExplore the options for your blog; customize and make the space your own.\n\n\nPassword protected posts\n If for any reason you feel that you don't want a post to be viewable by the web-at-large, you can hide it behind a password. At the top right where the 'publish' button hides, click on 'visibility' and select 'password protected'. Remember though: you'll have to share the password with me for grading purposes.\n\n\n\n\nFor more information about controlling visibility of your posts and so on, \nsee this help page\n.\n\n\nHypothesis\n\n\nHypothesis is an overlay on the web that allows you to highlight or annotate any text you come across (including \ninside pdfs\n). All of your annotations can then be collected together. It is a very effective research tool.\n\n\n\n\nCreate an account with \nhypothes.is\n.\n\n\nGet the plugin for \nchrome\n. If you don't have/use chrome, go to \nthis page\n and click on the 'other browsers' link.\n\n\n\n\nOnce you're logged into Hypothes.is, and you have the plugin installed, highlight THIS TEXT and leave an annotation! Who will be first? There are a few different kinds of annotations you can make; \nhere is a list with videos showing them\n.\n\n\nIf you need step-by-step instructions for installing and using Hypothes.is, please \nsee this help page\n and/or watch this video:\n\n\n\n\n\nAnnotations are public by default. When you are making a new annotation you can toggle the visibility so that they are private and so visible only to you.\n\n\nYou can also 'tag' any annotation you make. If many people use the same set of tags, you can collect annotations by tag. This can make it easier to do group research projects, for instance.\n\n\nPlease always tag your annotations with 'hist3814o'. That way, everyone's tags will show up on this page: \nhttp://jonudell.net/h/facet.html?facet=tag\nmode=documents\nsearch=hist3814o\n\n\nCollecting your own annotations on your blog\n\n\nHypothesis has an \napi\n that allows you to do some neat things. 'API' stands for 'application programming interface', which is just a fancy way of saying, 'you can write a program that interacts with this web service'. \nKris Shaffer\n, a professor at the University of Mary Washington, has written a \nplugin\n for Wordpress that allows you to automatically collect annotations you make across the web and to display them all on a single page on your blog. So, we'll go get that plugin and install it on your blog:\n\n\nOpen \nhttps://github.com/kshaffer/hypothesis_aggregator\n in a new browser window.\n\n\n\n\nClick \u201cClone or Download\u201d (the green button at the top right).\n\n\nIn the pop-up, click 'Download ZIP'\n\n\nGo over to the dashboard for your blog (if you closed this, you can get there again by opening a new browser window and going to your-domain.ca/wp-admin)\n\n\nIn the dashboard, click on Plugins \n Add New\n\n\nClick \u201cUpload Plugin\u201d. It will open a window asking you to find and select the zip file you downloaded. This is probably in your 'downloads' folder. Select and click ok.\n\n\nOnce it\u2019s uploaded and installed, click \u201cActivate\"\n\n\nIn the dashboard, click on 'pages' then add a new page. Call it 'Web Notes' or 'Annotations' or something similar.\n\n\nShaffer has created a 'shortcode' that tells Wordpress to go over to Hypothes.is and grab the latest information. So, in the text of your new page (in the editor window, make sure to click the 'text' button or else this won't work), enter the shortcode that will grab all of your public annotations: \n[hypothesis user = 'kris.shaffer']\n where you remove the \nkris.shaffer\n and put in your own Hypothes.is username.\n\n\nHit the 'publish' button. Wordpress will report that the page has been published and give you a link to it; click on this link to see your new always-updating list of annotations! Of course, if you haven't made any public annotations yet, nothing will display. Go annotate something, then come back and reload the page.\n\n\n\n\nGithub\n\n\nFinally, you need a github account. We will go into more detail about how to use Github in the next module. For now, go to \nGithub.com\n and sign up for a new account. Follow all the prompts, and make a note of the direct URL to your account (mine for instance is \nhttp://github.com/shawngraham\n). Put this link in your 'about' page on your blog. You'll learn how to use this space in exercise 3 in module 1.\n\n\nYour digital history lab/studio/workshop\n\n\nYou now have a digital history lab equipped with all of the necessary ingredients for doing digital history. You have an open notebook for recording what you are up to (both your narrative and your annotations). In Github, you have a scratch pad for keeping track of the actual nuts-and-bolts of any digital work you do (and note that it is entirely possible to do digital history successfully without having to do anything that a computer scientist for instance would call coding). You have a domain that you control completely, and where you can install other platforms and services as seems necessary.\n\n\nVPN \n DHBox\n\n\nTo access our virtual computer, the DHBox, you will need to use Carleton's VPN service. Please go to \nthis page\n and follow the instructions for your particular computer. Once you've got it installed, you will need to connect to Carleton through the VPN with your mycarletone credentials. Indeed, you should always connect via a VPN whenever you're using a public wifi point (like in coffee shops). The VPN acts like a private tunnel from your computer to Carleton's servers. To the rest of the internet, it will now look as if you actually are on campus. Once you're connected via the VPN, you can access the DHBox \nhere\n. Bookmark the site; you'll use it in the exercises in module 1.\n\n\nUsing the DHBox\n\n\n\n\nclick the 'sign up' button\n\n\nfill in the form. Choose a username and password that you'll remember. You don't have to use a real email by the way, just something that looks email-like (this is handy if, like me, you end up creating multiple DHBoxes - it's a bad idea to have more than one DHBox with the \nsame\n email address)\n\n\nselect the most time available (which will either be 1 or 2 months).\n\n\nyour personal DHBox will be created. Your username will now appear in the top right hand side of the purple bar. To enter the DHBox, click the username, select 'apps'.\n\n\nA new tool ribbon appears below the purple bar. Most of what you will do in this course involves the 'command line', 'R studio', and 'File Manager'.\n\n\nanytime the command line or R Studio should ask for your username or password, you use the DHBox username and password you just created.\n\n\n\n\nA note on using the university computer labs\n if you are using an official University computer lab computer to access DHBox, aspects of the University's security system might block the RStudio aspect. I am working on a solution to this problem. If you know that you are going to have to use Carleton computers, get in touch right away.\n\n\nSome Readings To Kick Things Off\n\n\nWhat is digital history anyway? How is it connected to so-called 'big data'? Read the following pieces. Annotate with Hypothes.is anything that strikes you as interesting; annotate anything that puzzles you - feel free to just say, 'I'm not sure what this means; does it mean.... does anybody have any ideas?' \nand\n if you see someone is asking questions, you can reply to that annotation with thoughts of your own!\n\n\nnb\n each week, I expect you to respond to at least someone else's annotation in a \nsubstantive\n way. No \"i agree!\" or \"right on!\" or that sort of thing. Make a \nmeaningful\n contribution.\n\n\nOnce you have read and annotated the works, \nwrite a post on your blog that poses the question 'what is digital history for me anyway?'\n . Explain why you're in this class, your level of comfort with digital tech, the kinds of history you're interested in, and what you hope to get out of this course. Your post should link to relevant annotations made by you or by your peers. (Every hypothesis annotation has a direct link visible when you click on the 'share' icon for an existing annotation).\n\n\n\n\nExcerpts from Chapter 1, the Historian's Macroscope \noriginal draft\n; read from 'Joys of Big Data' to 'Chapter One Conclusion'. Use Hypothesis to annotate rather than the 'commenting' function on the site.\n\n\nJames Baker 'The soft digital history that underpins my book' \nhttps://cradledincaricature.com/2017/05/24/the-soft-digital-history-that-underpins-my-book/\n\n\nJames Baker 'The hard digital history that underpins my book' \nhttps://cradledincaricature.com/2017/06/06/the-hard-digital-history-that-underpins-my-book/\n\n\nJo Guldi and David Armitage, 'The History Manifesto: \nChapter 4\n'\n\n\nTim Hitchcock 'Big Data for Dead People' \nhttps://historyonics.blogspot.ca/2013/12/big-data-for-dead-people-digital.html\n\n\nOn Diversity in Digital History \nThe Macroscope\n Read and follow through the footnotes to at least two more articles\n\n\n\n\nAcknowledgements\n\n\nThe writing of this workbook took place alongside the writing of my more formal book on \ndigital methods\n co-authored with the exceptional \nIan Milligan\n and \nScott Weingart\n. I learned far more about doing digital history from them than they ever from me, and someday, I hope to repay the debt. Other folks who've been instrumental in getting this workbook and course off the ground include Melodee Beals, John Bonnett, Chad Gaffield, Tamara Vaughan, the staff of the EDC at Carleton University, \neCampusOntario\n and of course, the digital history community on Twitter. My thanks to you all.\n\n\nThis class was first offered in the Winter 2015 semester at Carleton University in Ottawa Canada as HIST3907b. I am grateful to the participants in that class for the feedback and frank discussions of what worked and what didn't. To see the earlier version of the course, please feel free to browse its \ngithub repository", 
            "title": "Getting Started"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#getting-yourself-ready-july-4th-9th", 
            "text": "\"'Getting Ready for the Kaiser' Bain Collection, Library of Congress hdl.loc.gov/loc.pnp/ggbain.10412 Call Number: LC-B2- 2400-7\"  Welcome! This workbook is made by converting several plain-text files into a fully-operational website using the 'mkdocs'  static website generator . That means, if you want to keep a copy of all these files for your own records, you may. Simply click on the 'edit on github' link at the top right. This will bring you to the repository that houses this workbook. Then, when you're signed into 'github', you can 'fork' (that is, make a copy) the repo into your own account. Why 'forking'? It seems an odd phrase. Think of it like this:  Writing, crafting, coding: these are like a small river, flowing in one direction into the future. You get new ideas, new hunches: the river branches. There's a fork in its path. Sometimes new ideas, new hunches fold back into that original stream: they merge.  Github is a way of mapping that stream, and a guide to revisiting the interesting parts of it.  It's not the best metaphor, but it'll do. No doubt you've had that experience where, after working on an essay for days, you make a change that you regret. You wish you could go back to fix it, but ctrl+z only goes so far. You realize that everything you've done for the last week needs to be thrown out, and you start over.  Well, with 'versioning control', you can travel back upriver, back to where things were good. There are two tools that we will use to make this happen:  git  and  github . You'll learn more about making those things work in Module 1. You'll see why you'd want to do that, and how to future-proof your work, writing things in a plain-text format called  'markdown' .  In module 2, we'll start exploring the wide variety of historical data out there. We'll talk about some of the ethical dilemmas posed by having so much data out there. 'Data' are not neutral 'things given'; rather, they are 'capta': things  taken .  (Caveat: I am assuming, in this course, that the digital materials we want have already been digitized. Digitizing, adding meta-data (information that describes the data), and structuring it  properly  are very complex topics on their own that could be the subject of a complete course! If you are interested in those problems, a good place to start is this open course from SOAS on  database design for historians .)  In module 3, we'll see that data/capta are  messy , and that they make a kind of illusory order that as historians, we are not normally in the habit of thinking about. The big secret of digital history is that the majority of your time on  any  digital history project won't be finding the capta, won't be analyzing the capta, won't be thinking about the historical meaning of the capta. We'll be  cleaning it up . The exercises in this module will help you do that more efficiently (and be aware that 'efficiency' in computing terms is not a theoretically neutral idea!)  With module 4, we talk about doing the analysis. I present finding, cleaning, and analyzing as if they were sequential steps, but in reality they are circular. Movement in one aspect often requires revisiting another one! This module explores how we do this, and what it means for us as historians.  In module 5, we begin at last to think about how we communicate all of this to our audiences. Look at how  one university lays out the expectations for digital history work  (and, do you see how this ties back to ideas about  paradata ?). We will think about things like typography and layout. We'll look at ways of making our visualizations more compelling, more effective.  Finally, while there is no formal requirement for you to do this, it would be a great way of launching yourself as a digital historian to think about how you would formally reveal your project work in this class to the wider world: and then do it. There's a lot of noise on the internet, especially when it comes to history. How do you, as a digital historian, make the strongest possible signal?", 
            "title": "Getting yourself ready July 4th - 9th"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#what-you-need-to-do-this-week", 
            "text": "Follow the instructions below to set up your digital history workspace  Annotate the course manual for any parts of it that are unclear (or alternatively, that have you excited)  Respond to the readings and the reading questions by annotating the readings themselves - see the instructions below.  Submit your work  here", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#setting-up-a-your-workspace", 
            "text": "A digital historian needs to have a digital workshop/lab/studio/performance space. Such a space serves a number of functions:   a scratch pad / fail log and code repository so that we remember what we were doing, or (more importantly)  what  we we did - that is to say, the actual commands we typed, the sequence of manipulations or  data moves  a narrative that connects the dots, that explains the  why  of that what and how. You can use this narrative to point to when sharing your work with others. Digital history is not done in isolation or in a vaccuum. Sometimes, you will need to share a link to your work (often on twitter) asking, 'does anybody know why this isn't working?' or, 'does anybody know a better way of accomplishing this?', or, 'hey, I'm the first to do this!'  a way of keeping notes on things we've read/come across on the web. There are a number of ways of accomplishing this. In this course, I will mandate one particular solution: ( hypothesis ).  when you're working with academic databases such as JSTOR, then you'll also need a bibliography manager. We don't go into this aspect very much in this course (if you take other courses with me, you will) but you might want to check out  Zotero .  it can sometimes be useful to make little videos of your work to explain when something isn't working -  Screen-cast-o-matic  is free and does a good job   Now, the final part of your studio/lab/workshop is a domain (website + complete access to the webserver that powers it, so that you can install other platforms/services) of your own. A typical setup will be something along the lines of:   your-domain-name.org for your narrative, annotations, and anything you build regarding your work  github.com/your-name for your scratch pad and code repository   On your blog/narrative, you'll have a page that collates all of your web annotations as well. You'll also have - typically - an 'about' page where you can signal the kinds of history you're interested in, and the preferred way for people to get in touch with you. You do not have to use your real name. Remember the  real names policy  Because you have complete access and control over your domain, you can install other services as well. For instance, maybe you use Dropbox or Google Drive to sync your files across machines, or to function as a backup? You can install a service called 'OwnCloud' on your own domain that does the same thing, so that you have control over all your own materials.  So let's get started.", 
            "title": "Setting up a your workspace"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#reclaim-hosting", 
            "text": "In the course space for cuLearn, I gave you a code to use to pay for a domain of your own. I have already purchased domain space for you from an academic web hosting service,  Reclaim Hosting . This space will last for one year, at which point you have the option of paying to renew it or letting it die. Previous students in this course have used their domain to help with their applications for jobs and graduate school. One such is  Melissa .   You will be asked for the name you want to have for your space. You need to be thinking of branding here. Think of a professional name that conveys something of your personality and approach to history. I for instance own the domain, 'Electric Archaeology', which I chose to convey that I'm interested in digital archaeology, but also, that I move fast and cover a lot of breaking develops in the field (hey. It's my blog. I like the name). Please choose wisely. Some combination of your first and last name is often the best idea, since your own name is your own best calling card ('shawn graham' is such a generic name, that it was already long gone by the time I started doing digital history). Type in a name, select a top-level domain (ie, .com, .ca, .org, etc. I'll suggest .ca), and click on the 'check availability' button.  If the pop-up says 'congratulations, domain available!' then click on the continue button. (You may be offered free id protection, where Reclaim Hosting will hide your details is someone does a 'who-is' search on the domain name. If it does, then tick off the check box to confirm that you want this, and hit continue).  On the next screen, (the billing screen) fill in all of the information. The balance should be $0. At the bottom left, it will also say that you\u2019ve used a one-time promotional code. Hit the green button at the bottom to complete the purchase (which is not costing you anything).   Congratulations! You now own your very own domain.  It might take a bit of time for your domain to appear live on the web . During this time, you can log into your cPanel and install wordpress and so on - see below.  Giving you a space of your own is my political act of protest against the centralization of learning inside learning management systems. Learning isn't 'managed', it's cultivated. Unlike cuLearn, I cannot monitor your activity inside your own domain. I can only see what you choose to make public. Unlike Facebook or Snapchat or Instagram, I am not trying to monitize your personality on the web.", 
            "title": "Reclaim Hosting"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#wordpress-for-your-blog", 
            "text": "Wordpress is probably the best option for a platform for writing the narrative bits of your digital history work.   Click the 'client area. It will tell you that you have one active account (with Reclaim Hosting) and one active domain. When the time comes to renew your account or to close it down, this is where you do it. Note also that there is a link to 'Support', which will put you in touch with Reclaim Hosting's help desk. They are extremely fast and good at providing support; always treat any help request you make with them  as if  you were writing a formal email to me. Be polite and considerate, and thank them. The owners of the business often are the ones who provide the help! Without them, we couldn't do this class.  Go to 'cPanel' - this is where you can install all sorts of different kinds of software on your account. Search for and select 'web applications'  Click on Wordpress. Then click on 'install this application'.  The next screen presents you with a number of options. Leave these set to their defaults. For 'location', leave this blank. That tells the installtron to put Wordpress at your-domain.ca . (When/if you install other pieces of software, you'd change this variable so that the new software doesn't overwrite this software!)  Further down the page, under 'settings', you need to change 'administrator username', 'administrator password', 'web site title', 'website tagline'.  this is the username and password to actually do things with your blog, and the name of your blog itself . Leave everything else alone.  Click Install!  Once it's done, the installer will remind you of the url to your site, and the url to the blog's dashboard (where you go to write posts, make changes to the look and feel of the site). Open these in a new browser window, and bookmark them. To login to your blog, remember to go to the dashboard URL (eg, http://your-domain.ca/wp-admin), enter your blog administrator username and password.   You can close the cPanel webpage now (log out first).  Customising your blog  If you look at the dashboard for your blog, you'll see a lot of options down the left hand side of your screen. If you want to change the look of your blog, click on 'appearance' then click on 'themes'. A number of themes are pre-installed. You can click on the different themes and select 'preview' to see how your blog might look. When you find one you like, select 'activate'. Go to the other browser window where you have your-domain.ca open. Reload the page to see the changes take effect!   If you're the sort of person who likes to sketch ideas out on paper, Lauren Heywood of the  Disruptive Media Learning Lab  has designed a paper-based exercise to prototype ideas. Why not give it a try?  Print this file out  and follow the instructions.   To write new content, know that there is a difference between a 'post' and a 'page'. A page is a new link on your site, while posts appear in chronological order on a page, with the most recent on top. Most themes show the most recent post by default, and pages appear in any menus on the site.  When you are logged into your blog  any post or page will have an 'edit' button at the bottom that you can click. You'll then be presented with an editing box with the standard toolbar across the top (allowing you to change the font, insert images, change the alignment of the text and so on). At the top right will be a button to save or publish/update the post/page.   Your blog will have a default 'about' page. Change that default text now to reflect something about who you are and why you are taking this course   To create new pages, you click on the 'pages' link in your dashboard and select 'add new'  To create new posts, you click on the 'posts' link in  your dashboard and select 'add new'.  Explore the options for your blog; customize and make the space your own.  Password protected posts  If for any reason you feel that you don't want a post to be viewable by the web-at-large, you can hide it behind a password. At the top right where the 'publish' button hides, click on 'visibility' and select 'password protected'. Remember though: you'll have to share the password with me for grading purposes.   For more information about controlling visibility of your posts and so on,  see this help page .", 
            "title": "Wordpress for your blog"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#hypothesis", 
            "text": "Hypothesis is an overlay on the web that allows you to highlight or annotate any text you come across (including  inside pdfs ). All of your annotations can then be collected together. It is a very effective research tool.   Create an account with  hypothes.is .  Get the plugin for  chrome . If you don't have/use chrome, go to  this page  and click on the 'other browsers' link.   Once you're logged into Hypothes.is, and you have the plugin installed, highlight THIS TEXT and leave an annotation! Who will be first? There are a few different kinds of annotations you can make;  here is a list with videos showing them .  If you need step-by-step instructions for installing and using Hypothes.is, please  see this help page  and/or watch this video:   Annotations are public by default. When you are making a new annotation you can toggle the visibility so that they are private and so visible only to you.  You can also 'tag' any annotation you make. If many people use the same set of tags, you can collect annotations by tag. This can make it easier to do group research projects, for instance.  Please always tag your annotations with 'hist3814o'. That way, everyone's tags will show up on this page:  http://jonudell.net/h/facet.html?facet=tag mode=documents search=hist3814o  Collecting your own annotations on your blog  Hypothesis has an  api  that allows you to do some neat things. 'API' stands for 'application programming interface', which is just a fancy way of saying, 'you can write a program that interacts with this web service'.  Kris Shaffer , a professor at the University of Mary Washington, has written a  plugin  for Wordpress that allows you to automatically collect annotations you make across the web and to display them all on a single page on your blog. So, we'll go get that plugin and install it on your blog:  Open  https://github.com/kshaffer/hypothesis_aggregator  in a new browser window.   Click \u201cClone or Download\u201d (the green button at the top right).  In the pop-up, click 'Download ZIP'  Go over to the dashboard for your blog (if you closed this, you can get there again by opening a new browser window and going to your-domain.ca/wp-admin)  In the dashboard, click on Plugins   Add New  Click \u201cUpload Plugin\u201d. It will open a window asking you to find and select the zip file you downloaded. This is probably in your 'downloads' folder. Select and click ok.  Once it\u2019s uploaded and installed, click \u201cActivate\"  In the dashboard, click on 'pages' then add a new page. Call it 'Web Notes' or 'Annotations' or something similar.  Shaffer has created a 'shortcode' that tells Wordpress to go over to Hypothes.is and grab the latest information. So, in the text of your new page (in the editor window, make sure to click the 'text' button or else this won't work), enter the shortcode that will grab all of your public annotations:  [hypothesis user = 'kris.shaffer']  where you remove the  kris.shaffer  and put in your own Hypothes.is username.  Hit the 'publish' button. Wordpress will report that the page has been published and give you a link to it; click on this link to see your new always-updating list of annotations! Of course, if you haven't made any public annotations yet, nothing will display. Go annotate something, then come back and reload the page.", 
            "title": "Hypothesis"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#github", 
            "text": "Finally, you need a github account. We will go into more detail about how to use Github in the next module. For now, go to  Github.com  and sign up for a new account. Follow all the prompts, and make a note of the direct URL to your account (mine for instance is  http://github.com/shawngraham ). Put this link in your 'about' page on your blog. You'll learn how to use this space in exercise 3 in module 1.", 
            "title": "Github"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#your-digital-history-labstudioworkshop", 
            "text": "You now have a digital history lab equipped with all of the necessary ingredients for doing digital history. You have an open notebook for recording what you are up to (both your narrative and your annotations). In Github, you have a scratch pad for keeping track of the actual nuts-and-bolts of any digital work you do (and note that it is entirely possible to do digital history successfully without having to do anything that a computer scientist for instance would call coding). You have a domain that you control completely, and where you can install other platforms and services as seems necessary.", 
            "title": "Your digital history lab/studio/workshop"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#vpn-dhbox", 
            "text": "To access our virtual computer, the DHBox, you will need to use Carleton's VPN service. Please go to  this page  and follow the instructions for your particular computer. Once you've got it installed, you will need to connect to Carleton through the VPN with your mycarletone credentials. Indeed, you should always connect via a VPN whenever you're using a public wifi point (like in coffee shops). The VPN acts like a private tunnel from your computer to Carleton's servers. To the rest of the internet, it will now look as if you actually are on campus. Once you're connected via the VPN, you can access the DHBox  here . Bookmark the site; you'll use it in the exercises in module 1.  Using the DHBox   click the 'sign up' button  fill in the form. Choose a username and password that you'll remember. You don't have to use a real email by the way, just something that looks email-like (this is handy if, like me, you end up creating multiple DHBoxes - it's a bad idea to have more than one DHBox with the  same  email address)  select the most time available (which will either be 1 or 2 months).  your personal DHBox will be created. Your username will now appear in the top right hand side of the purple bar. To enter the DHBox, click the username, select 'apps'.  A new tool ribbon appears below the purple bar. Most of what you will do in this course involves the 'command line', 'R studio', and 'File Manager'.  anytime the command line or R Studio should ask for your username or password, you use the DHBox username and password you just created.   A note on using the university computer labs  if you are using an official University computer lab computer to access DHBox, aspects of the University's security system might block the RStudio aspect. I am working on a solution to this problem. If you know that you are going to have to use Carleton computers, get in touch right away.", 
            "title": "VPN &amp; DHBox"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#some-readings-to-kick-things-off", 
            "text": "What is digital history anyway? How is it connected to so-called 'big data'? Read the following pieces. Annotate with Hypothes.is anything that strikes you as interesting; annotate anything that puzzles you - feel free to just say, 'I'm not sure what this means; does it mean.... does anybody have any ideas?'  and  if you see someone is asking questions, you can reply to that annotation with thoughts of your own!  nb  each week, I expect you to respond to at least someone else's annotation in a  substantive  way. No \"i agree!\" or \"right on!\" or that sort of thing. Make a  meaningful  contribution.  Once you have read and annotated the works,  write a post on your blog that poses the question 'what is digital history for me anyway?'  . Explain why you're in this class, your level of comfort with digital tech, the kinds of history you're interested in, and what you hope to get out of this course. Your post should link to relevant annotations made by you or by your peers. (Every hypothesis annotation has a direct link visible when you click on the 'share' icon for an existing annotation).   Excerpts from Chapter 1, the Historian's Macroscope  original draft ; read from 'Joys of Big Data' to 'Chapter One Conclusion'. Use Hypothesis to annotate rather than the 'commenting' function on the site.  James Baker 'The soft digital history that underpins my book'  https://cradledincaricature.com/2017/05/24/the-soft-digital-history-that-underpins-my-book/  James Baker 'The hard digital history that underpins my book'  https://cradledincaricature.com/2017/06/06/the-hard-digital-history-that-underpins-my-book/  Jo Guldi and David Armitage, 'The History Manifesto:  Chapter 4 '  Tim Hitchcock 'Big Data for Dead People'  https://historyonics.blogspot.ca/2013/12/big-data-for-dead-people-digital.html  On Diversity in Digital History  The Macroscope  Read and follow through the footnotes to at least two more articles", 
            "title": "Some Readings To Kick Things Off"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#acknowledgements", 
            "text": "The writing of this workbook took place alongside the writing of my more formal book on  digital methods  co-authored with the exceptional  Ian Milligan  and  Scott Weingart . I learned far more about doing digital history from them than they ever from me, and someday, I hope to repay the debt. Other folks who've been instrumental in getting this workbook and course off the ground include Melodee Beals, John Bonnett, Chad Gaffield, Tamara Vaughan, the staff of the EDC at Carleton University,  eCampusOntario  and of course, the digital history community on Twitter. My thanks to you all.  This class was first offered in the Winter 2015 semester at Carleton University in Ottawa Canada as HIST3907b. I am grateful to the participants in that class for the feedback and frank discussions of what worked and what didn't. To see the earlier version of the course, please feel free to browse its  github repository", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/module-1/Open-Access-Research/", 
            "text": "Open Access Research - July 10 - 16\n\n\nConcepts\n\n\nAs historians, we aren't all that accustomed to sharing our research notes. We go to the archives, we take our photographs, we spend \nhours\n pouring over documents, photographs, diaries, newspapers... why should someone else benefit from \nour\n work?\n\n\nThere are a number of reasons why you should want to do this. This week we will read and discuss the arguments advanced by various historians, including\n\n\n\n\nTrevor Owens\n\n\nCaleb McDaniel\n\n\nIan Milligan\n\n\nanother post by Milligan\n\n\nMichelle Moravec\n\n\nKathleen Fitzgerald\n\n\nSheila Brennan\n\n\n\n\nBut most importantly, \nchange is coming whether historians like it or not\n. Here in Canada, SSHRC has a \nresearch data archiving policy\n\n\n\n\nAll research data collected with the use of SSHRC funds must be preserved and made available for use by others within a reasonable period of time. SSHRC considers \u201ca reasonable period\u201d to be within two years of the completion of the research project for which the data was collected.\n\n\n\n\nNote the \nconversation that ensued on Twitter after Milligan mentioned all this\n and also \nhere\n\n\nWe will explore\n\n\n\n\nwhy and how to make our research notes open,\n\n\nwhat that implies for how we do research,\n\n\nand how we can use this process to maintain our scholarly voice online.\n\n\n\n\nReally, it's also a kind of \n'knowledge mobilization'\n. In this module you will find exercises related to setting up your github account, how to commit, fork, push and pull files to your own repository and to others'. Really, it's about \nsustainable authorship\n and \npreserving your research data\n.\n\n\nBy the end of this module you will know:\n\n\n\n\nhow to work with github to foster collaboration\n\n\nhow to set up, fork, and make changes to files and repositories\n\n\nthe rationale for historians to make their work public\n\n\n\n\nRemember: I do expect you to click through every link I provide, and to read these materials.\n\n\nWhat you need to do this week\n\n\n\n\nRespond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so we can find them \nhere\n\n\nDo the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your github account - for more on that, see the exercises!).\n\n\nSubmit your work \nhere\n\n\n\n\nReadings\n\n\nAs you read the posts linked to above (Brennan, Fitzgerald, Guldi and Armitage, McDaniel, Milligan, Moravec, Owens) click through to their 'about' or 'portfolio' pages. How are these scholars portraying themselves? How do they approach the idea of 'openness'? How is your own work 'generous'? Please annotate their work with your observations and questions; please also respond to someone else's annotation with a substantive observation of your own.\n\n\nThen, make an entry on your blog that contrasts this picture of 'open access research' with what you may have learned about doing history in your other courses. Where are the dangers and where are the opportunities? What does 'open access' mean for you as a student?", 
            "title": "Why you should be open"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#open-access-research-july-10-16", 
            "text": "", 
            "title": "Open Access Research - July 10 - 16"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#concepts", 
            "text": "As historians, we aren't all that accustomed to sharing our research notes. We go to the archives, we take our photographs, we spend  hours  pouring over documents, photographs, diaries, newspapers... why should someone else benefit from  our  work?  There are a number of reasons why you should want to do this. This week we will read and discuss the arguments advanced by various historians, including   Trevor Owens  Caleb McDaniel  Ian Milligan  another post by Milligan  Michelle Moravec  Kathleen Fitzgerald  Sheila Brennan   But most importantly,  change is coming whether historians like it or not . Here in Canada, SSHRC has a  research data archiving policy   All research data collected with the use of SSHRC funds must be preserved and made available for use by others within a reasonable period of time. SSHRC considers \u201ca reasonable period\u201d to be within two years of the completion of the research project for which the data was collected.   Note the  conversation that ensued on Twitter after Milligan mentioned all this  and also  here  We will explore   why and how to make our research notes open,  what that implies for how we do research,  and how we can use this process to maintain our scholarly voice online.   Really, it's also a kind of  'knowledge mobilization' . In this module you will find exercises related to setting up your github account, how to commit, fork, push and pull files to your own repository and to others'. Really, it's about  sustainable authorship  and  preserving your research data .  By the end of this module you will know:   how to work with github to foster collaboration  how to set up, fork, and make changes to files and repositories  the rationale for historians to make their work public   Remember: I do expect you to click through every link I provide, and to read these materials.", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#what-you-need-to-do-this-week", 
            "text": "Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below. Remember to tag your annotations with 'hist3814o' so we can find them  here  Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your github account - for more on that, see the exercises!).  Submit your work  here", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-1/Open-Access-Research/#readings", 
            "text": "As you read the posts linked to above (Brennan, Fitzgerald, Guldi and Armitage, McDaniel, Milligan, Moravec, Owens) click through to their 'about' or 'portfolio' pages. How are these scholars portraying themselves? How do they approach the idea of 'openness'? How is your own work 'generous'? Please annotate their work with your observations and questions; please also respond to someone else's annotation with a substantive observation of your own.  Then, make an entry on your blog that contrasts this picture of 'open access research' with what you may have learned about doing history in your other courses. Where are the dangers and where are the opportunities? What does 'open access' mean for you as a student?", 
            "title": "Readings"
        }, 
        {
            "location": "/module-1/Exercises/", 
            "text": "Module 1: Exercises\n\n\nThe exercises are designed to give you the necessary skills to engage with the doing of digital history. To ensure success in the course, please do make it through exercises 1 - 3. Exercise 4 is a bit more complex and not mission-critical. Push yourself if you can.\n\n\nThese exercises walk you through the process of using our DHBox, and of keeping notes about what you're doing, and making those notes open on the web. If you run into trouble, \nask for help\n in our Slack space. Annotate this page with where things are going wrong for you. Contact Dr. Graham. \nYou do not have to suffer in silence!\n To ask for help when doing this work is not a sign of weakness, but of maturity.\n\n\nAll 4 exercises are on this page. Remember to scroll!\n\n\nEXERCISE 1: learning markdown syntax with dillinger.io\n\n\nHave you ever fought with Word or another wordprocessor, trying to get things just \nright\n? Word processing is a mess. It conflates writing with typesetting and layout. Sometimes, you just want to get the words out. Othertimes, you want to make your writing as accessible as possible... but your intended recipient can't open your file, because they don't use the same wordprocessor. Or perhaps you wrote up some great notes that you'd love to have in a slideshow; but you can't, because copying and pasting preserves a whole lot of extra \ngunk\n that messes up your materials.\n\n\nThe answer is to separate your \ncontent\n from your tool.\n\n\nThis is where the \nMarkdown syntax\n shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \"goes out of business.\"\n\n\nWriting in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents (incidentally, Hypothes.is annotations can be written in Markdown). For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general.\n\n\nPopular general purpose plain text editors include TextWrangler and Sublime for Mac, Notepad++ for Windows, as well as Gedit and Kate for Linux. However, there are also editors that specialize in displaying and editing Markdown. \nnb\n a text editor is different from the default notepad app that comes with Windows or Mac. A text editor shows you \nexactly\n what is in a file.\n\n\nIn this exercise, I want you to become familiar with Markdown syntax (\nhere is a quick primer on markdown by Sarah Simpkin\n). There are a number of 'flavours' for Markdown, but in essence, they all mimic the kinds of conventions you would see in an email, using asterisks to indicate emphasis and so on.\n\n\n\n\nYou will need this \ncheatsheet\n.\n\n\nGo to \ndillinger.io\n in a new browser window. This looks like a wordprocessor. The left hand side of the screen is where you write, the right hand side shows you what your text will look like \nif you converted the text to html\n. Dillinger 'saves' your work in your browser's memory. You can also point it to save to your dropbox, google drive, or github account (under the cogwheel icon).\n\n\nWrite a short 200-500 word piece on the most interesting annotation you've seen one of your classmates make. Why is it interesting? Why has it struck you?\n\n\nHave at least two images (creative commons licensed for re-use - do you know how to \nfind these\n?) and link outwards to four websites that are relevant to your piece.\n\n\nMake sure you link to the annotation in question.\n\n\nIn the 'document name' slot, make sure to add the file type .md at the end\n\n\n'Export to' markdown to save a copy of the file in your downloads folder.\n\n\nTry 'exporting to' pdf or html. Since you've separated the content from the format, this illustrates how you can transform your text into other formats as necessary. (The converter is a piece of software called \npandoc\n)\n\n\n\n\nSee how easy that was? Don't worry about submitting this.... yet.\n\n\n\n\nnb: the next time you go to dillinger.io the last document you were working on will load up. That's because dillinger stashes your work in the browser cache. If you clear your cache (from your browser's tools or settings) you'll lose it, which is why in step 7 I suggested exporting..\n\n\n\n\n(For a richer discussion of some more ways markdown and pandoc can make your research sustainable, see Tenen and Wythoff, \nSustainable Authorship in Plain Text Using Pandoc and Markdown\n)\n\n\nEXERCISE 2: Getting familiar with DHBox\n\n\nMany of the exercises in this workbook work without issue on a Mac or Linux computer, at the terminal. (On a Mac, you can find the terminal under applications \n utilities). If you're on a Windows machine, it can be more difficult to get all of the various bits and pieces properly configured. If you're on a tablet, most digital history things you might like to do are not really feasible. One solution is for all of us to use the \nsame\n computer. The CUNY Graduate Centre has created a digital-humanities focused virtual computer for just such an occasion.\n\n\nIn this exercise, you are going to set up an instance of a DHBox for your own use.\n\n\nNB this workbook assumes that you are using a DHBox\n\n\n\n\nIf you are on campus or are logged into Carleton's systems via a VPN, go to \nhttp://134.117.26.132:5000/signup\n. If not, go to \nhttp://dhbox.org/signup\n. These are two separate installations of DHBox; whichever one you start with, continue to use.\n\n\nSelect a username, password, and your email address. \nYour username must be four characters or longer\n. Then select '1 month'. Then select launch.\n\n\n\n\nYou now have a virtual computer that you can use for the next month.\n\n\nWhenever you come back to the dhbox, you can now click 'login' and your personal dh computer and your work will be there waiting for you. Once you've logged in, the site will reload to the DHbox welcome screen, but your user name will show at the top right.\n\n\n\n\nClick on your username, and there will be a new option, 'apps'. Click here to go into your dhbox.\n\n\n\n\nInside the DHBox, you can click on 'home', 'file manager', 'command line', 'r studio', 'brackets', 'jupyter notebooks'. The 'home' page will tell you how many days until your dhbox expires. Keep an eye on this, as you'll want to get your materials out of dhbox before that happens. 'File manager' allows you to view all of your files, as well as uploading/downloading materials from dhbox to your local computer. 'Command line' allows you to interact with the computer at the terminal prompt or command line - this is where you type in commands to your machine. 'R Studio' is an environment for doing statistical computing, 'Brackets' is for web development, and 'Jupyter Notebooks' is an environment for creating documents that have running code inside of them. For HIST3814o, we will use the file manager, the command line, and R studio\n\n\nIn the previous exercise, \nDillinger.io\n could convert your markdown document into html or pdf. We will now add the pandoc program to DHBox so that you can convert things for yourself, using the \nwget\n command. Wget is a program that allows us to download materials off the web. It can be used to only grab certain file types, or all files within a certain directory, or even to take a compete copy of a website! We'll discussion wget more in module 2 (see also \nthe Programming Historian\n).\n\n\nFor now, select 'command line' in your dhbox. You will have to login again with your dhbox username and password. In many tutorials or how-tos, you will see the \n$\n sign at the beginning of some text you are meant to type. This is a convention to show that what follows the $ is to be typed at the command prompt.\n\n\n$ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb\n\n\n...so don't type the $, but rather, type the wget etc.\n\n\nThis command gets a copy of the Pandoc program that will work in DHBox. The .deb file extension tells us that this is a linux file. The next step is to unpack that file. We do this with this command:\n\n\n$ sudo dpkg -i pandoc-1.19.2.1-1-amd64.deb\n\n\nSome commands can have nasty effects, and the operating system won't let you do them unless you specify that you \nreally\n want to do them. That's what the \nsudo\n achieves. The next part with the -i 'flag' is a 'package manager' for installing software. The last part is the file that we used wget to copy to our own machine. To test that pandoc is now installed, type:\n\n\n$ pandoc -v\n\n\nIf all has gone well, DHBox will tell you what version of pandoc you have installed, who it was built by, and a copyright notice.\n\n\nNow, let's create a file to keep a record of what commands we have been typing. Type:\n\n\n$ history\n\n\nDHBox returns a list of every command that you've typed. You can 'pipe' that information into a new file like so:\n\n\n$ history \n dhbox-work-today.md\n\n\nWhen you hit enter, the computer seems to pause for a moment, and then it shows you the command prompt again. How do we know if anything happened? Generally, when working at the command prompt, no news is good news. There are two ways you can check to see if the new file \ndhbox-work-today.md\n was created. You can click on the 'file manager' (the first folder will have your username; click on that, and then you'll see a list of files) and there it is! Click on the file name, and it will download to your computer where you can open it with a text editor.\n\n\nThe other way is to type:\n\n\n$ ls\n\n\nat the command line. This 'lists' - ls - all the files in the directory. How do you know what directory you're in? You can use the \npwd\n command, which prints the working directory.\n\n\nLet's take a look inside that new file you created. There is a text editor that you can use, called 'nano'. Type:\n\n\n$ nano dhbox-work-today.md\n\n\nThis opens the text editor, and you can see that the history command has copied its output into the editor. \nIf you got an error:\n Sometimes, tools or commands we want to use are not present in the system. DHBox uses the 'Ubuntu' operating system (the underlying code that makes a PC a PC, a Mac a Mac, etc). We can install all sorts of useful things by using the \napt-get\n command. To get and install Nano, try re-installing nano with this command:\n\n\n$ sudo apt-get install nano\n.\n\n\nThe computer makes things a little more difficult sometimes when you're asking it to do something that changes or adds capabilities, so simply typing \napt-get\n wouldn't work. \nsudo\n tells the computer, I really do want to this (it stands for 'super-user do'). Any 'sudo' command will make the computer ask for your password to confirm that you really do want to run that command.\n\n\nHere is a guide to using the Nano text editor\n which you should check out now.\n\n\nIn Nano, with your file open, make a header (use \n#\n to indicate a header, remember) at the start of the file which has today's date in it, and add some text explaining what you're trying to do. Then, hit ctrl+x to edit Nano. Nano will ask you if you want to 'Save modified buffer?'. Hit \ny\n, then when it asks you for the file name, hit enter.\n\n\nUse the file manager to save a copy of \ndhbox-work-today.md\n onto your own computer. One last thing: let's convert the markdown file into both Word and HTML. Pandoc is capable of quite sophisticated transformations, but these are two of the easiest. Try this:\n\n\n$ pandoc -o todayscommands.docx dhbox-work-today.md\n\n\nThis says to pandoc, create an output file ( the -o) called 'todayscommands.docx' from 'dhbox-work-today.md'. Type \nls\n after running this command to see if you've made the file. Any guesses how to create an HTML file? Pandoc is smart enough to know the kind of output you want from the file extension, so retype the command but use .html instead of .docx this time. Use filemanager to save copies of the .docx and .html files to your own machine. (Incidentally, if you use the arrow up and arrow down keys on your keyboard when you're at the command line, you can page through commands that you've previously typed).\n\n\nYou've done some interesting work - you've installed software into a remote computer, you've copied all of the commands you typed into a new file, you've used markdown and a text editor to add information and context to those commands, and you've used pandoc to transform your basic text into the more complicated formats of Word or HTML.\n\n\nAs you do more work in this workbook, I want you to get in the habit of keeping these copies of what you've done. These are your actual lab notes, and they are invaluable for helping you keep track of what you've been doing and what you've been trying. If you remember the course manual, keeping a lab notebook is part of the assessment of the course.\n\n\nIn the next exercise, we learn to use another piece of software called 'git' which will enable you to set up a remote location for pushing copies of your work to, for safe keeping and for collaboration. For more on why we go to all this trouble, see \n'Preserving Your Research Data'\n.\n\n\nHere's a handy cheat-sheet of keyboard shortcuts and other useful commands for your DHBox command-line work\n.\n\n\nEXERCISE 3: setting up your github space\n\n\nIt's a familiar situation - you've been working on a paper. It's where you want it to be, and you're certain you're done. You save it as 'final.doc'. Then, you ask your friend to take a look at it. She spots several typos and that you flubbed an entire paragraph. You open it up, make the changes, and save as 'final-w-changes.doc'. Later that day it occurs to you that you don't like those changes, and you go back to the original 'final.doc', make some changes, and just overwrite the previous version. Soon, you have a folder like:\n\n\n|-project\n    |-'finalfinal.doc'\n    |-'final-w-changes.doc'\n    |-'final-w-changes2.doc'\n    |-'isthisone-changes.doc'\n    |-'this.doc'\n\n\n\n\nThings can get messy quite quickly. Imagine that you also have several spreadsheets in there as well, images, snippets of code... we don't want this. What we want is a way of managing the evolution of your files. We do this with a program called \nGit\n. Git is not a user-friendly piece of software, and it takes some work to get your head around. Git is also very powerful, but fortunately, the basic uses to which most of us put it to are more or less straightforward. There are many other programs that make use of Git for version control; these programs weld a graphical user interface on top of the main Git program. For now, we'll content ourselves with the Github website, which does the same thing more or less, but from a browser.\n\n\nFirstly, let's define some terms.\n\n\n\n\ngit is a program that keeps snapshots of the contents of a designated folder; it watches for 'difs' or differences between one snapshot and the next. You 'commit' these snapshots to a repository, which lives on your computer (it's just a folder).\n\n\nGithub\n is a webservice that allows you to share those repositories, and to keep track of those \nversions\n online, and what's more, to share the repositories and files with multiple collaborators, and \nkeep everyone's changes straight!\n There are other services that work like Github; Github is just one of the better known services. You can browse Github repositories for code that solves a particular problem for you, software, and data.\n\n\n\n\nNB. Because we are going to use the free version, we cannot make the repositories we create private.\n\n\nIn this exercise, we're going to create a repository via the Github website and use it as a kind of back-up space for the files you created in the previous exercise. In the follow up exercise, you will learn how to do this from the command line.\n\n\n\n\nGo to \nGithub\n and sign up for an account. Remember, you don't have to use your real name. If you use a pseudonym, please communicate to me privately what your account is called.\n\n\nOnce you're logged in, we will create a new repository called \nhist3814o\n. Click on the \n+\n at the top right of the screen, beside your avatar image.\n\n\nWrite a short description in the 'description box', and tick off the 'initialize the repository with a readme'. You can also select a license from the drop down box - this will put some standard wording on your repository page about the conditions under which someone else might use (or cite) your code.\n\n\nClick 'create repository'.\n\n\n\n\nAt this point, you now have a folder - a repository - on the Github website into which you can deposit your files. It will be at http://github.com/your-account-name/hist3814o. So let's put some materials into that repository.\n\n\n\n\nNotice, when you're on your repository's page, that there is a button to 'create new file' and another for 'upload files'. For now, click on 'upload files'.\n\n\nOn the page that opens, there is a large grey box in the center of the screen. You can drag and drop files into that box to upload them into your repository. Drag the html file you created in the previous exercise (the one you made with Pandoc, and then saved to your computer via the DHBox filemanager) into the grey box. It will upload the file; you can drag multiple files into the box.\n\n\nGit - and Github - attach messages to any 'commits' you make. These messages are brief notes explaining why you were making the commit. This way, if you ever had to roll back (go back to an earlier version) you can understand the evolution of the repository and find the spot you want. Enter a brief commit message in the commit message box. Then hit the green commit changes button.\n\n\n\n\nInstead of creating multiple versions of a file, you have a single file that has a version history. Neat, eh?\n\n\nThis is perhaps the simplest use case for Github. You can create files directly in the repository as well, by hitting the 'create new file' button, and following the prompts. Github has a brief tutorial on using the website to collaborate with other people on a repository that \nyou can explore here\n.\n\n\nfor the remainder of the course, use your hist3814o repository as your scratch pad, your fail log, your open notebook, for showing your work across these modules\n\n\nHere is an example 'fail log' as a model - \nhttps://github.com/shawngraham/example-faillog-hist3814\n. 'Fail' is a pretty harsh word: I use it to point out that for everything that works perfectly, there's an awful lot of trial-and-error that happened first \nupon which\n our successes are built. We need to keep track of this! James Baker calls this, '\nde-wizardification\n.'\n\n\nSome useful vocabulary when discussing git, github, and version control:\n\n\n\n\nrepository\n: a single folder that holds all of the files and subfolders of your project\n\n\ncommit\n: this means, 'take a snapshot of the current state of my repository'\n\n\npublish\n: take a folder on my computer, and copy it and its contents to the web as a repository at github.com/myusername/repositoryname\n\n\nsync\n: update the web repository with the latest commit from the folder on my computer\n\n\nbranch\n: make a copy of my repository with a 'working name'\n\n\nmerge\n: fold the changes I have made on a branch into another branch (typically, either \nmaster\n or \ngh-pages\n)\n\n\nfork\n: to make a copy of someone else's repo\n\n\nclone\n: to copy a repo online onto your own computer\n\n\npull\n request: to ask the original maker of a repo to 'pull' your changes into their master, original, repository\n\n\npush\n: to move your changes from your computer to the online repo\n\n\n\n\nAn aside\n\n\nMany websites - including this workbook - use a Github repository as a way of hosting a website. The video below by historian Jack Dougherty shows how this could be done. Note that the html code that he pastes into an index.html file (the first page of any website is usually called index.html) he got from a different service. You could write a document in markdown, then use pandoc to convert that into an index.html, for example.\n\n\n\n\n\nEXERCISE 4: a detailed look at using git on the command line\n\n\nAt its heart, Git is a way of taking 'snapshots' of the current state of a folder, and saving those snapshots in sequence. (For an excellent brief presentation on Git, see Alice Bartlett's \npresentation here\n; Bartlett is a senior developer for the Financial Times). In Git's lingo, as stated earlier, a folder on your computer is known as a \nrepository\n. This sequence of snapshots in total lets you see how your project unfolded over time. Each time you wish to take a snapshot, you make a \ncommit\n. A commit is a Git command to take a snapshot of the entire repository. Thus, your folder we discussed above, with its proliferation of documents becomes:\n\n\n|-project\n    |-'final.doc'\n\n\n\n\nBUT its commit history could be visualized like a string of pearls, where each pearl is a unique commit. Each one of those pearls represents a point in time when you the writer made a commit; Git compared the state of the file to the earlier state, and saved a snapshot of the \ndifferences\n. What is particularly useful about making a commit is that Git requires two more pieces of information about the git: who is making it, and when. The final useful bit about a commit is that you can save a detailed message about \nwhy\n the commit is being made. In our hypothetical situation, your first commit message might look like this:\n\n\nFixed conclusion\n\nJulie pointed out that I had missed\nthe critical bit in the assignment\nregarding stratigraphy. This was\nadded in the concluding section.\n\n\n\n\nThis information is stored in the history of the commits. In this way, you can see exactly how the project evolved and why. Each one of these commits has what is called a \nhash\n. This is a unique fingerprint that you can use to 'time travel' (in Bartlett's felicitous phrasing). If you want to see what your project looked like a few months ago, you \ncheckout\n that commit. This has the effect of 'rewinding' the project. Once you've checked out a commit, don't be alarmed when you look at the folder: your folder (your repository) looks like how it once did all those weeks ago! Any files written after that commit seem as if they've disappeared. Don't worry: they still exist!\n\n\nWhat would happen if you wanted to experiment or take your project in a new direction from that point forward? Git lets you do this. What you will do is create a new \nbranch\n of your project from that point. You can think of a branch as like the branch of a tree, or perhaps better, a branch of a river that eventually merges back to the source. (Another way of thinking about branches is that it is a label that sticks with these particular commits.) It is generally considered 'best practice' to leave your \nmaster\n branch alone, in the sense that it represents the best version of your project. When you want to experiment or do something new, you create a \nbranch\n and work there. If the work on the branch ultimately proves fruitless, you can discard it. \nBut\n, if you decide that you like how it's going, you can \nmerge\n that branch back into your master. A merge is a commit that folds all of the commits from the branch with the commits from the master.\n\n\nGit is also a powerful tool for backing up your work. You can work quite happily with Git on your own machine, but when you store those files and the history of commits somewhere remote, you open up the possibility of collaboration \nand\n a safe place where your materials can be recalled if -perish the thought- something happened to your computer. In Git-speak, the remote location is, well, the \nremote\n. There are many different places on the web that can function as a remote for Git repositories. You can even set one up on your own server, if you want. To get material \nout\n of Github and onto your own computer, you \nclone\n it. If that hypothetical paper you were writing was part of a group project, your partners could clone it from your Github space, and work on it as well!\n\n\nLet us imagine a scenario.... You and Anna are working together on the project. You have made a new project repository in your Github space, and you have cloned it to your computer. Anna has cloned it to hers. Let's assume that you have a very productive weekend and you make some real headway on the project. You \ncommit\n your changes, and then \npush\n them from your computer to the Github version of your repository. That repository is now one commit \nahead\n of Anna's version. Anna \npulls\n those changes from Github to her own version of the repository, which now looks \nexactly\n like your version. What happens if you make changes to the exact same part of the exact same file? This is called a \nconflict\n. Git will make a version of the file that contains text clearly marking off the part of the file where the conflict occurs, with the conflicting information marked out as well. The way to \nresolve\n the conflict is to open the file (typically with a text editor) and to delete the added Git text, making a decision on which information is the correct information.\n\n\nCaution\n what follows might take a bit of time. It walks you through setting up a git repository in your DHBox; making changes to it; making different branches; and publishing the repository to your space on Github.com.\n\n\ngit init\n\n\n4.1. How do you turn a folder into a repository? With the \ngit init\n command. At the command line (remember, the \n$\n just shows you the prompt; you don't have to type it!):\n\n\n\n\nmake a new directory: \n$ mkdir first-repo\n\n\ntype \n$ ls\n (list) to see that the director exists. Then change directory into it: \ncd first-repo\n. (remember: if you're ever not sure what directory you're in, type \n$ pwd\n, or print working directory).\n\n\nmake a new file called \nreadme.md\n. You do this by calling the text editor: \nnano readme.md\n. Type an explanation of what this exercise is about. Hit ctrl+x to exit, then y to save, leave the file name as it is. \nIf you get an error to the effect that nano is not found\n you just need to install it with \nsudo apt-get install nano\n. DHBox will ask you for your password again. Once the dust settles, you can make the new file with \nnano readme.md\n.\n\n\ntype \n$ ls\n again to check that the file is there.\n\n\ntype \n$ git init\n to tell the Git program that this folder is to be tracked as a repository. If all goes correctly, you should see a variation on this message: \nInitialized empty Git repository in /home/demonstration/first-repo/.git/\n. But type \n$ ls\n again. What do you (not) see?\n\n\n\n\nThe changes in your repo will now be stored in that \nhidden\n directory, \n.git\n. Most of the time, you will never have reason to search that folder out. But know that the config file that describes your repo is in that folder. There might come a time in the future where you want to alter some of the default behaviour of the git program. You do that by opening the config file (which you can read with a text editor). Google 'show hidden files and folders' for your operating system when that time comes.\n\n\ngit status\n\n\n4.2. Open your readme.md file again with the nano text editor, from the command line. Add some more information to it, then save and exit the text editor.\n\n\n\n\ntype \n$ git status\n\n\nGit will respond with a couple of pieces of information. It will tell you which \nbranch\n you are on. It will list any untracked files present or new changes that are unstaged. We now will \nstage\n those changes to be added to our commit history by typing \n$ git add -A\n. (the bit that says \n-A\n adds any new, modified, or deleted files to your commit when you make it. There are \nother options or flags\n where you add \nonly\n the new and modified files, \nor\n only the modified and deleted files.)\n\n\nLet's check our git status again: type \n$ git status\n\n\nYou should see something like this:\n\n\n\n\nOn branch master\nInitial commit\nChanges to be committed:\n  (use \ngit rm --cached \nfile\n...\n to unstage)\n        new file:   readme.md```\n\n\n\n\n\n\nLet's take a snapshot: type \n$ git commit -m \"My first commit\"\n. What happened? Remember, Git keeps track not only of the changes, but \nwho\n is making them. If this is your first time working with Git in the DHBox, Git will ask you for your name and email. Helpfully, the Git error message tells you exactly what to do: type \n$ git config --global user.email \"you\\@example.com\"\n and then type \n$ git config --global user.name \"Your Name\"\n. Now try making your first commit.\n\n\nThe command above represents a bit of a shortcut for making commit messages by using the \n-m\n flag to associate the text in the quotation marks with the commit. Open up your readme.md file again, and add some more text to it. Save and exit the text editor. Add the new changes to the snapshot that we will take. Then, type \n$ git commit\n. Git automatically opens up the text editor so you can type a longer, more substantive commit message. In this message (unlike in markdown) the \n#\n indicates a line to be ignored. You'll see that there is already some default text in there telling you what to do. Type a message indicating the nature of the changes you have made. Then save and exit the text editor. DO NOT change the filename!\n\n\n\n\nCongratulations, you are now able to track your changes, and keep your materials under version control!\n\n\ngit merge\n\n\n4.3. Go ahead and make some more changes to your repository. Add some new files. Commit your changes after each new file is created. Now we're going to view the history of your commits. Type \n$ git log\n. What do you notice about this list of changes? Look at the time stamps. You'll see that the entries are listed in reverse chronological order. Each entry has its own 'hash' or unique ID, the person who made the commit and time are listed, as well as the commit message eg:\n\n\ncommit 253506bc23070753c123accbe7c495af0e8b5a43\nAuthor: Shawn Graham \nshawn.graham@carleton.ca\n\nDate:   Tue Feb 14 18:42:31 2017 +0000\n\nFixed the headings that were broken in the about section of readme.md\n\n\n\n\n\n\n\nWe're going to go back in time and create a new branch. You can escape the \ngit log\n by typing \nq\n. Here's how the command will look: \n$ git checkout -b branchname \ncommit\n where \nbranch\n is the name you want the branch to be called, and \ncommit\n is that unique ID. Make a new branch from your second last commit (don't use \n or \n).\n\n\nWe typed \ngit checkout -b experiment 253506bc23070753c123accbe7c495af0e8b5a43\n. The response: \nSwitched to a new branch 'experiment'\n Check git status and then list the contents of your repository. What do you see? You should notice that some of the files you had created before seem to have disappeared - congratulations, you've time travelled! Those files are not missing; but they \nare\n on a different branch (the master branch) and you can't harm them now. Add a number of new files, making commits after each one. Check your git status, and check your git log as you go to make sure you're getting everything. Make sure there are no unstaged changes - everything's been committed.\n\n\n\n\n4.4. Now let's assume that your \nexperiment\n branch was successful - everything you did there you were happy with and you want to integrate all of those changes back into your \nmaster\n branch. We're going to merge things. To merge, we have to go back to the master branch: \n$ git checkout master\n. (Good practice is to keep separate branches for all major experiments or directions you go. In case you lose track of the names of the branches you've created, this command: \ngit branch -va\n will list them for you.)\n\n\n\n\nNow, we merge with \n$ git merge experiment\n. Remember, a merge is a special kind of commit that rolls all previous commits from both branches into one - Git will open your text editor and prompt you to add a message (it will have a default message already there if you want it). Save and exit and ta da! Your changes have been merged together.\n\n\n\n\ngit push\n\n\n4.5. One of the most powerful aspects of using Git is the possibility of using it to manage collaborations. To do this, we have to make a copy of your repository available to others as a \nremote\n. There are a variety of places on the web where this can be done; one of the most popular at the moment is \nGithub\n. Github allows a user to have an unlimited number of \npublic\n repositories. Public repositories can be viewed and copied by anyone. \nPrivate\n repositories require a paid account, and access is controlled. If you are working on sensitive materials that can only be shared amongst the collaborators on a project, you should invest in an upgraded account (note that you can also control which files get included in commit; see \nthis help file\n. In essence, you simply list the file names you do not want committed; here's an \nexample\n). Let's assume that your materials are not sensitive.\n\n\n\n\nlogin to Github\n\n\nOn the upper right part of the screen there is a large + sign. Click on that, and select \nnew public repository\n\n\nOn the following screen, give your repo a name.\n\n\nDO NOT 'initialize this repo with a readme.md'. Leave \nadd .gitignore\n and \nadd license\n set to NONE.\n\n\nClick the green 'Create Repository' button.\n\n\nYou now have a space into which you will publish the repository on your machine. At the command line, we now need to tell Git the location of this space. We do that with the following command, where you will change \nyour-username\n and \nyour-new-repo\n appropriately:\n\n\n\n\n$ git remote add origin https://github.com/YOUR-USERNAME/YOUR-NEW-REPO.git\n\n\n\n\ng. Now we push your local copy of the repository onto the web, to the Github version of your repo:\n\n\ngit push -u origin master\n\n\n\n\nNB\n If you wanted to push a \nbranch\n to your repository on the web instead, do you see how you would do that? If your branch was called \nexperiment\n, the command would look like this:\n\n\n$ git push origin experiment\n\n\n\n\n\n\n\nThe changes can sometimes take a few minutes to show up on the website. Now, the next time you make changes to this repository, you can push them to your Github account - which is the 'origin' in the command above.  Add a new text file. Commit the changes. Push the changes to your account.\n\n\n\n\ngit clone\n\n\n4.6. Imagine you are collaborating with one of your classmates. Your classmate is in charge of the project, and is keeping track of the 'official' folder of materials (eg, the repo). You wish to make some changes to the files in that repository. You can manage that collaboration via Github by making a copy, what Github calls a \nfork\n.\n- Make sure you're logged into your Github account on the Github website. We're going to fork an example repository right now by going to \nhttps://github.com/octocat/Spoon-Knife\n. Click the 'fork' button at top-right. Github now makes a copy of the repository in your own Github account!\n- To make a copy of that repository on your own machine, you will now clone it with the \ngit clone\n command. (Remember: a 'fork' copies someone's Github repo into a repo in your OWN Github account; a 'clone' makes a copy on your own MACHINE). Type:\n\n\n$ cd..\n$ pwd\n\n\n\n\nWe do that to make sure you're not \ninside\n any other repo you've made! Make sure you're not inside the repository we used in exercises 1 to 5, then proceed:\n\n\n$ git clone https://github.com/YOUR-USERNAME/Spoon-Knife\n$ ls\n\n\n\n\nYou now have a folder called 'Spoon-Knife' on your machine! Any changes you make inside that folder can be tracked with commits. You can also \ngit push -u origin master\n when you're inside it, and the changes will show up on your OWN copy (your fork) on Github.com.\nc. Make a fork of, and then clone, one of your classmates' repositories. Create a new branch. Add a new file to the repository on your machine, and then push it to your fork on Github. Remember, your new file will appear on the new branch you created, NOT the master branch.\n\n\npull request\n\n\n4.7. Now, you let your collaborator know that you've made a change that you want her to \nmerge\n into the original repository. You do this by issuing a \npull request\n. But first, we have to tell Git to keep an eye on that original repository, which we will call \nupstream\n. You do this by adding that repository's location like so:\n\n\n\n\ntype (but change the address appropriately):\n\n\n\n\n$ git remote add upstream THE-FULL-URL-TO-THEIR-REPO-ENDING-WITH-.git\n\n\n\n\n\n\nYou can keep your version of the remote up-to-date by fetching any new changes your classmate has done:\n\n\n\n\n$ git fetch upstream\n\n\n\n\n\n\nNow let's make a \npull\n request (you might want to bookmark this \nhelp document\n). Go to your copy of your classmate's repository at your Github account. Make sure you've selected the correct branch you pushed your changes to, by selecting it from the Branches menu drop down list.\n\n\nClick the 'new pull request' button.\n\n\nThe new page that appears can be confusing, but it is trying to double check with you which changes you want to make, and where. \n'Base Branch'\n is the branch where you want your changes to go, ie, your classmate's repository. \n'head branch'\n is the branch where you made \nyour\n changes. Make sure these are set properly. Remember: the first one is the TO, the second one is the FROM: the place where you want your changes to go TO, FROM the place where you made the changes.\n\n\nA pull request has to have a message attached to it, so that your classmate knows what kind of change you're proposing. Fill in the message fields appropriately, then hit the 'create pull request' button.\n\n\n\n\ngit merge again\n\n\n4.8. Finally, the last bit of work to be done is to accept the pull request and \nmerge\n the changes into the original repository.\n- Go to your repository on your Github account. Check to see if there are any 'pull requests' - these will be listed under the 'pull requests' tab. Click on that tab.\n- You can merge from the command line, but for now, you can simply click on the green 'merge pull request' button, and then the 'confirm merge' button. The changes your classmate has made have now been folded into your repository.\n- To get the updates on your local machine, go back to the command line and type\n\n\n$ git pull origin master\n\n\n\n\nPhew. You might want to do \n$ history \n recentcommands.md\n just to remember what you've done. And then commit that to a repository.\n\n\n\n\nConclusion\n\n\nIn the modules to come, we will be using DHBox as we find data, fetch data, wrangle data, analyze data, and visualize data. It becomes very important that you note the kinds of commands you use or try, the thinking that you were doing at that point, and so on. You want to leave yourself (and anybody who comes after) breadcrumbs so that you understand what you were doing. Quick notes written in markdown, piping the history of what you've done to a file, and keeping those files in a repository alongside any other code or files that you may make will set you on the path to open access research and computational reproducibility. As Martha might say, 'and that's a good thing'.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-1/Exercises/#module-1-exercises", 
            "text": "The exercises are designed to give you the necessary skills to engage with the doing of digital history. To ensure success in the course, please do make it through exercises 1 - 3. Exercise 4 is a bit more complex and not mission-critical. Push yourself if you can.  These exercises walk you through the process of using our DHBox, and of keeping notes about what you're doing, and making those notes open on the web. If you run into trouble,  ask for help  in our Slack space. Annotate this page with where things are going wrong for you. Contact Dr. Graham.  You do not have to suffer in silence!  To ask for help when doing this work is not a sign of weakness, but of maturity.  All 4 exercises are on this page. Remember to scroll!", 
            "title": "Module 1: Exercises"
        }, 
        {
            "location": "/module-1/Exercises/#exercise-1-learning-markdown-syntax-with-dillingerio", 
            "text": "Have you ever fought with Word or another wordprocessor, trying to get things just  right ? Word processing is a mess. It conflates writing with typesetting and layout. Sometimes, you just want to get the words out. Othertimes, you want to make your writing as accessible as possible... but your intended recipient can't open your file, because they don't use the same wordprocessor. Or perhaps you wrote up some great notes that you'd love to have in a slideshow; but you can't, because copying and pasting preserves a whole lot of extra  gunk  that messes up your materials.  The answer is to separate your  content  from your tool.  This is where the  Markdown syntax  shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \"goes out of business.\"  Writing in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents (incidentally, Hypothes.is annotations can be written in Markdown). For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general.  Popular general purpose plain text editors include TextWrangler and Sublime for Mac, Notepad++ for Windows, as well as Gedit and Kate for Linux. However, there are also editors that specialize in displaying and editing Markdown.  nb  a text editor is different from the default notepad app that comes with Windows or Mac. A text editor shows you  exactly  what is in a file.  In this exercise, I want you to become familiar with Markdown syntax ( here is a quick primer on markdown by Sarah Simpkin ). There are a number of 'flavours' for Markdown, but in essence, they all mimic the kinds of conventions you would see in an email, using asterisks to indicate emphasis and so on.   You will need this  cheatsheet .  Go to  dillinger.io  in a new browser window. This looks like a wordprocessor. The left hand side of the screen is where you write, the right hand side shows you what your text will look like  if you converted the text to html . Dillinger 'saves' your work in your browser's memory. You can also point it to save to your dropbox, google drive, or github account (under the cogwheel icon).  Write a short 200-500 word piece on the most interesting annotation you've seen one of your classmates make. Why is it interesting? Why has it struck you?  Have at least two images (creative commons licensed for re-use - do you know how to  find these ?) and link outwards to four websites that are relevant to your piece.  Make sure you link to the annotation in question.  In the 'document name' slot, make sure to add the file type .md at the end  'Export to' markdown to save a copy of the file in your downloads folder.  Try 'exporting to' pdf or html. Since you've separated the content from the format, this illustrates how you can transform your text into other formats as necessary. (The converter is a piece of software called  pandoc )   See how easy that was? Don't worry about submitting this.... yet.   nb: the next time you go to dillinger.io the last document you were working on will load up. That's because dillinger stashes your work in the browser cache. If you clear your cache (from your browser's tools or settings) you'll lose it, which is why in step 7 I suggested exporting..   (For a richer discussion of some more ways markdown and pandoc can make your research sustainable, see Tenen and Wythoff,  Sustainable Authorship in Plain Text Using Pandoc and Markdown )", 
            "title": "EXERCISE 1: learning markdown syntax with dillinger.io"
        }, 
        {
            "location": "/module-1/Exercises/#exercise-2-getting-familiar-with-dhbox", 
            "text": "Many of the exercises in this workbook work without issue on a Mac or Linux computer, at the terminal. (On a Mac, you can find the terminal under applications   utilities). If you're on a Windows machine, it can be more difficult to get all of the various bits and pieces properly configured. If you're on a tablet, most digital history things you might like to do are not really feasible. One solution is for all of us to use the  same  computer. The CUNY Graduate Centre has created a digital-humanities focused virtual computer for just such an occasion.  In this exercise, you are going to set up an instance of a DHBox for your own use.  NB this workbook assumes that you are using a DHBox   If you are on campus or are logged into Carleton's systems via a VPN, go to  http://134.117.26.132:5000/signup . If not, go to  http://dhbox.org/signup . These are two separate installations of DHBox; whichever one you start with, continue to use.  Select a username, password, and your email address.  Your username must be four characters or longer . Then select '1 month'. Then select launch.   You now have a virtual computer that you can use for the next month.  Whenever you come back to the dhbox, you can now click 'login' and your personal dh computer and your work will be there waiting for you. Once you've logged in, the site will reload to the DHbox welcome screen, but your user name will show at the top right.   Click on your username, and there will be a new option, 'apps'. Click here to go into your dhbox.   Inside the DHBox, you can click on 'home', 'file manager', 'command line', 'r studio', 'brackets', 'jupyter notebooks'. The 'home' page will tell you how many days until your dhbox expires. Keep an eye on this, as you'll want to get your materials out of dhbox before that happens. 'File manager' allows you to view all of your files, as well as uploading/downloading materials from dhbox to your local computer. 'Command line' allows you to interact with the computer at the terminal prompt or command line - this is where you type in commands to your machine. 'R Studio' is an environment for doing statistical computing, 'Brackets' is for web development, and 'Jupyter Notebooks' is an environment for creating documents that have running code inside of them. For HIST3814o, we will use the file manager, the command line, and R studio  In the previous exercise,  Dillinger.io  could convert your markdown document into html or pdf. We will now add the pandoc program to DHBox so that you can convert things for yourself, using the  wget  command. Wget is a program that allows us to download materials off the web. It can be used to only grab certain file types, or all files within a certain directory, or even to take a compete copy of a website! We'll discussion wget more in module 2 (see also  the Programming Historian ).  For now, select 'command line' in your dhbox. You will have to login again with your dhbox username and password. In many tutorials or how-tos, you will see the  $  sign at the beginning of some text you are meant to type. This is a convention to show that what follows the $ is to be typed at the command prompt.  $ wget https://github.com/jgm/pandoc/releases/download/1.19.2.1/pandoc-1.19.2.1-1-amd64.deb  ...so don't type the $, but rather, type the wget etc.  This command gets a copy of the Pandoc program that will work in DHBox. The .deb file extension tells us that this is a linux file. The next step is to unpack that file. We do this with this command:  $ sudo dpkg -i pandoc-1.19.2.1-1-amd64.deb  Some commands can have nasty effects, and the operating system won't let you do them unless you specify that you  really  want to do them. That's what the  sudo  achieves. The next part with the -i 'flag' is a 'package manager' for installing software. The last part is the file that we used wget to copy to our own machine. To test that pandoc is now installed, type:  $ pandoc -v  If all has gone well, DHBox will tell you what version of pandoc you have installed, who it was built by, and a copyright notice.  Now, let's create a file to keep a record of what commands we have been typing. Type:  $ history  DHBox returns a list of every command that you've typed. You can 'pipe' that information into a new file like so:  $ history   dhbox-work-today.md  When you hit enter, the computer seems to pause for a moment, and then it shows you the command prompt again. How do we know if anything happened? Generally, when working at the command prompt, no news is good news. There are two ways you can check to see if the new file  dhbox-work-today.md  was created. You can click on the 'file manager' (the first folder will have your username; click on that, and then you'll see a list of files) and there it is! Click on the file name, and it will download to your computer where you can open it with a text editor.  The other way is to type:  $ ls  at the command line. This 'lists' - ls - all the files in the directory. How do you know what directory you're in? You can use the  pwd  command, which prints the working directory.  Let's take a look inside that new file you created. There is a text editor that you can use, called 'nano'. Type:  $ nano dhbox-work-today.md  This opens the text editor, and you can see that the history command has copied its output into the editor.  If you got an error:  Sometimes, tools or commands we want to use are not present in the system. DHBox uses the 'Ubuntu' operating system (the underlying code that makes a PC a PC, a Mac a Mac, etc). We can install all sorts of useful things by using the  apt-get  command. To get and install Nano, try re-installing nano with this command:  $ sudo apt-get install nano .  The computer makes things a little more difficult sometimes when you're asking it to do something that changes or adds capabilities, so simply typing  apt-get  wouldn't work.  sudo  tells the computer, I really do want to this (it stands for 'super-user do'). Any 'sudo' command will make the computer ask for your password to confirm that you really do want to run that command.  Here is a guide to using the Nano text editor  which you should check out now.  In Nano, with your file open, make a header (use  #  to indicate a header, remember) at the start of the file which has today's date in it, and add some text explaining what you're trying to do. Then, hit ctrl+x to edit Nano. Nano will ask you if you want to 'Save modified buffer?'. Hit  y , then when it asks you for the file name, hit enter.  Use the file manager to save a copy of  dhbox-work-today.md  onto your own computer. One last thing: let's convert the markdown file into both Word and HTML. Pandoc is capable of quite sophisticated transformations, but these are two of the easiest. Try this:  $ pandoc -o todayscommands.docx dhbox-work-today.md  This says to pandoc, create an output file ( the -o) called 'todayscommands.docx' from 'dhbox-work-today.md'. Type  ls  after running this command to see if you've made the file. Any guesses how to create an HTML file? Pandoc is smart enough to know the kind of output you want from the file extension, so retype the command but use .html instead of .docx this time. Use filemanager to save copies of the .docx and .html files to your own machine. (Incidentally, if you use the arrow up and arrow down keys on your keyboard when you're at the command line, you can page through commands that you've previously typed).  You've done some interesting work - you've installed software into a remote computer, you've copied all of the commands you typed into a new file, you've used markdown and a text editor to add information and context to those commands, and you've used pandoc to transform your basic text into the more complicated formats of Word or HTML.  As you do more work in this workbook, I want you to get in the habit of keeping these copies of what you've done. These are your actual lab notes, and they are invaluable for helping you keep track of what you've been doing and what you've been trying. If you remember the course manual, keeping a lab notebook is part of the assessment of the course.  In the next exercise, we learn to use another piece of software called 'git' which will enable you to set up a remote location for pushing copies of your work to, for safe keeping and for collaboration. For more on why we go to all this trouble, see  'Preserving Your Research Data' .  Here's a handy cheat-sheet of keyboard shortcuts and other useful commands for your DHBox command-line work .", 
            "title": "EXERCISE 2: Getting familiar with DHBox"
        }, 
        {
            "location": "/module-1/Exercises/#exercise-3-setting-up-your-github-space", 
            "text": "It's a familiar situation - you've been working on a paper. It's where you want it to be, and you're certain you're done. You save it as 'final.doc'. Then, you ask your friend to take a look at it. She spots several typos and that you flubbed an entire paragraph. You open it up, make the changes, and save as 'final-w-changes.doc'. Later that day it occurs to you that you don't like those changes, and you go back to the original 'final.doc', make some changes, and just overwrite the previous version. Soon, you have a folder like:  |-project\n    |-'finalfinal.doc'\n    |-'final-w-changes.doc'\n    |-'final-w-changes2.doc'\n    |-'isthisone-changes.doc'\n    |-'this.doc'  Things can get messy quite quickly. Imagine that you also have several spreadsheets in there as well, images, snippets of code... we don't want this. What we want is a way of managing the evolution of your files. We do this with a program called  Git . Git is not a user-friendly piece of software, and it takes some work to get your head around. Git is also very powerful, but fortunately, the basic uses to which most of us put it to are more or less straightforward. There are many other programs that make use of Git for version control; these programs weld a graphical user interface on top of the main Git program. For now, we'll content ourselves with the Github website, which does the same thing more or less, but from a browser.  Firstly, let's define some terms.   git is a program that keeps snapshots of the contents of a designated folder; it watches for 'difs' or differences between one snapshot and the next. You 'commit' these snapshots to a repository, which lives on your computer (it's just a folder).  Github  is a webservice that allows you to share those repositories, and to keep track of those  versions  online, and what's more, to share the repositories and files with multiple collaborators, and  keep everyone's changes straight!  There are other services that work like Github; Github is just one of the better known services. You can browse Github repositories for code that solves a particular problem for you, software, and data.   NB. Because we are going to use the free version, we cannot make the repositories we create private.  In this exercise, we're going to create a repository via the Github website and use it as a kind of back-up space for the files you created in the previous exercise. In the follow up exercise, you will learn how to do this from the command line.   Go to  Github  and sign up for an account. Remember, you don't have to use your real name. If you use a pseudonym, please communicate to me privately what your account is called.  Once you're logged in, we will create a new repository called  hist3814o . Click on the  +  at the top right of the screen, beside your avatar image.  Write a short description in the 'description box', and tick off the 'initialize the repository with a readme'. You can also select a license from the drop down box - this will put some standard wording on your repository page about the conditions under which someone else might use (or cite) your code.  Click 'create repository'.   At this point, you now have a folder - a repository - on the Github website into which you can deposit your files. It will be at http://github.com/your-account-name/hist3814o. So let's put some materials into that repository.   Notice, when you're on your repository's page, that there is a button to 'create new file' and another for 'upload files'. For now, click on 'upload files'.  On the page that opens, there is a large grey box in the center of the screen. You can drag and drop files into that box to upload them into your repository. Drag the html file you created in the previous exercise (the one you made with Pandoc, and then saved to your computer via the DHBox filemanager) into the grey box. It will upload the file; you can drag multiple files into the box.  Git - and Github - attach messages to any 'commits' you make. These messages are brief notes explaining why you were making the commit. This way, if you ever had to roll back (go back to an earlier version) you can understand the evolution of the repository and find the spot you want. Enter a brief commit message in the commit message box. Then hit the green commit changes button.   Instead of creating multiple versions of a file, you have a single file that has a version history. Neat, eh?  This is perhaps the simplest use case for Github. You can create files directly in the repository as well, by hitting the 'create new file' button, and following the prompts. Github has a brief tutorial on using the website to collaborate with other people on a repository that  you can explore here .  for the remainder of the course, use your hist3814o repository as your scratch pad, your fail log, your open notebook, for showing your work across these modules  Here is an example 'fail log' as a model -  https://github.com/shawngraham/example-faillog-hist3814 . 'Fail' is a pretty harsh word: I use it to point out that for everything that works perfectly, there's an awful lot of trial-and-error that happened first  upon which  our successes are built. We need to keep track of this! James Baker calls this, ' de-wizardification .'  Some useful vocabulary when discussing git, github, and version control:   repository : a single folder that holds all of the files and subfolders of your project  commit : this means, 'take a snapshot of the current state of my repository'  publish : take a folder on my computer, and copy it and its contents to the web as a repository at github.com/myusername/repositoryname  sync : update the web repository with the latest commit from the folder on my computer  branch : make a copy of my repository with a 'working name'  merge : fold the changes I have made on a branch into another branch (typically, either  master  or  gh-pages )  fork : to make a copy of someone else's repo  clone : to copy a repo online onto your own computer  pull  request: to ask the original maker of a repo to 'pull' your changes into their master, original, repository  push : to move your changes from your computer to the online repo   An aside  Many websites - including this workbook - use a Github repository as a way of hosting a website. The video below by historian Jack Dougherty shows how this could be done. Note that the html code that he pastes into an index.html file (the first page of any website is usually called index.html) he got from a different service. You could write a document in markdown, then use pandoc to convert that into an index.html, for example.", 
            "title": "EXERCISE 3: setting up your github space"
        }, 
        {
            "location": "/module-1/Exercises/#exercise-4-a-detailed-look-at-using-git-on-the-command-line", 
            "text": "At its heart, Git is a way of taking 'snapshots' of the current state of a folder, and saving those snapshots in sequence. (For an excellent brief presentation on Git, see Alice Bartlett's  presentation here ; Bartlett is a senior developer for the Financial Times). In Git's lingo, as stated earlier, a folder on your computer is known as a  repository . This sequence of snapshots in total lets you see how your project unfolded over time. Each time you wish to take a snapshot, you make a  commit . A commit is a Git command to take a snapshot of the entire repository. Thus, your folder we discussed above, with its proliferation of documents becomes:  |-project\n    |-'final.doc'  BUT its commit history could be visualized like a string of pearls, where each pearl is a unique commit. Each one of those pearls represents a point in time when you the writer made a commit; Git compared the state of the file to the earlier state, and saved a snapshot of the  differences . What is particularly useful about making a commit is that Git requires two more pieces of information about the git: who is making it, and when. The final useful bit about a commit is that you can save a detailed message about  why  the commit is being made. In our hypothetical situation, your first commit message might look like this:  Fixed conclusion\n\nJulie pointed out that I had missed\nthe critical bit in the assignment\nregarding stratigraphy. This was\nadded in the concluding section.  This information is stored in the history of the commits. In this way, you can see exactly how the project evolved and why. Each one of these commits has what is called a  hash . This is a unique fingerprint that you can use to 'time travel' (in Bartlett's felicitous phrasing). If you want to see what your project looked like a few months ago, you  checkout  that commit. This has the effect of 'rewinding' the project. Once you've checked out a commit, don't be alarmed when you look at the folder: your folder (your repository) looks like how it once did all those weeks ago! Any files written after that commit seem as if they've disappeared. Don't worry: they still exist!  What would happen if you wanted to experiment or take your project in a new direction from that point forward? Git lets you do this. What you will do is create a new  branch  of your project from that point. You can think of a branch as like the branch of a tree, or perhaps better, a branch of a river that eventually merges back to the source. (Another way of thinking about branches is that it is a label that sticks with these particular commits.) It is generally considered 'best practice' to leave your  master  branch alone, in the sense that it represents the best version of your project. When you want to experiment or do something new, you create a  branch  and work there. If the work on the branch ultimately proves fruitless, you can discard it.  But , if you decide that you like how it's going, you can  merge  that branch back into your master. A merge is a commit that folds all of the commits from the branch with the commits from the master.  Git is also a powerful tool for backing up your work. You can work quite happily with Git on your own machine, but when you store those files and the history of commits somewhere remote, you open up the possibility of collaboration  and  a safe place where your materials can be recalled if -perish the thought- something happened to your computer. In Git-speak, the remote location is, well, the  remote . There are many different places on the web that can function as a remote for Git repositories. You can even set one up on your own server, if you want. To get material  out  of Github and onto your own computer, you  clone  it. If that hypothetical paper you were writing was part of a group project, your partners could clone it from your Github space, and work on it as well!  Let us imagine a scenario.... You and Anna are working together on the project. You have made a new project repository in your Github space, and you have cloned it to your computer. Anna has cloned it to hers. Let's assume that you have a very productive weekend and you make some real headway on the project. You  commit  your changes, and then  push  them from your computer to the Github version of your repository. That repository is now one commit  ahead  of Anna's version. Anna  pulls  those changes from Github to her own version of the repository, which now looks  exactly  like your version. What happens if you make changes to the exact same part of the exact same file? This is called a  conflict . Git will make a version of the file that contains text clearly marking off the part of the file where the conflict occurs, with the conflicting information marked out as well. The way to  resolve  the conflict is to open the file (typically with a text editor) and to delete the added Git text, making a decision on which information is the correct information.  Caution  what follows might take a bit of time. It walks you through setting up a git repository in your DHBox; making changes to it; making different branches; and publishing the repository to your space on Github.com.  git init  4.1. How do you turn a folder into a repository? With the  git init  command. At the command line (remember, the  $  just shows you the prompt; you don't have to type it!):   make a new directory:  $ mkdir first-repo  type  $ ls  (list) to see that the director exists. Then change directory into it:  cd first-repo . (remember: if you're ever not sure what directory you're in, type  $ pwd , or print working directory).  make a new file called  readme.md . You do this by calling the text editor:  nano readme.md . Type an explanation of what this exercise is about. Hit ctrl+x to exit, then y to save, leave the file name as it is.  If you get an error to the effect that nano is not found  you just need to install it with  sudo apt-get install nano . DHBox will ask you for your password again. Once the dust settles, you can make the new file with  nano readme.md .  type  $ ls  again to check that the file is there.  type  $ git init  to tell the Git program that this folder is to be tracked as a repository. If all goes correctly, you should see a variation on this message:  Initialized empty Git repository in /home/demonstration/first-repo/.git/ . But type  $ ls  again. What do you (not) see?   The changes in your repo will now be stored in that  hidden  directory,  .git . Most of the time, you will never have reason to search that folder out. But know that the config file that describes your repo is in that folder. There might come a time in the future where you want to alter some of the default behaviour of the git program. You do that by opening the config file (which you can read with a text editor). Google 'show hidden files and folders' for your operating system when that time comes.  git status  4.2. Open your readme.md file again with the nano text editor, from the command line. Add some more information to it, then save and exit the text editor.   type  $ git status  Git will respond with a couple of pieces of information. It will tell you which  branch  you are on. It will list any untracked files present or new changes that are unstaged. We now will  stage  those changes to be added to our commit history by typing  $ git add -A . (the bit that says  -A  adds any new, modified, or deleted files to your commit when you make it. There are  other options or flags  where you add  only  the new and modified files,  or  only the modified and deleted files.)  Let's check our git status again: type  $ git status  You should see something like this:   On branch master\nInitial commit\nChanges to be committed:\n  (use  git rm --cached  file ...  to unstage)\n        new file:   readme.md```   Let's take a snapshot: type  $ git commit -m \"My first commit\" . What happened? Remember, Git keeps track not only of the changes, but  who  is making them. If this is your first time working with Git in the DHBox, Git will ask you for your name and email. Helpfully, the Git error message tells you exactly what to do: type  $ git config --global user.email \"you\\@example.com\"  and then type  $ git config --global user.name \"Your Name\" . Now try making your first commit.  The command above represents a bit of a shortcut for making commit messages by using the  -m  flag to associate the text in the quotation marks with the commit. Open up your readme.md file again, and add some more text to it. Save and exit the text editor. Add the new changes to the snapshot that we will take. Then, type  $ git commit . Git automatically opens up the text editor so you can type a longer, more substantive commit message. In this message (unlike in markdown) the  #  indicates a line to be ignored. You'll see that there is already some default text in there telling you what to do. Type a message indicating the nature of the changes you have made. Then save and exit the text editor. DO NOT change the filename!   Congratulations, you are now able to track your changes, and keep your materials under version control!  git merge  4.3. Go ahead and make some more changes to your repository. Add some new files. Commit your changes after each new file is created. Now we're going to view the history of your commits. Type  $ git log . What do you notice about this list of changes? Look at the time stamps. You'll see that the entries are listed in reverse chronological order. Each entry has its own 'hash' or unique ID, the person who made the commit and time are listed, as well as the commit message eg:  commit 253506bc23070753c123accbe7c495af0e8b5a43\nAuthor: Shawn Graham  shawn.graham@carleton.ca \nDate:   Tue Feb 14 18:42:31 2017 +0000\n\nFixed the headings that were broken in the about section of readme.md   We're going to go back in time and create a new branch. You can escape the  git log  by typing  q . Here's how the command will look:  $ git checkout -b branchname  commit  where  branch  is the name you want the branch to be called, and  commit  is that unique ID. Make a new branch from your second last commit (don't use   or  ).  We typed  git checkout -b experiment 253506bc23070753c123accbe7c495af0e8b5a43 . The response:  Switched to a new branch 'experiment'  Check git status and then list the contents of your repository. What do you see? You should notice that some of the files you had created before seem to have disappeared - congratulations, you've time travelled! Those files are not missing; but they  are  on a different branch (the master branch) and you can't harm them now. Add a number of new files, making commits after each one. Check your git status, and check your git log as you go to make sure you're getting everything. Make sure there are no unstaged changes - everything's been committed.   4.4. Now let's assume that your  experiment  branch was successful - everything you did there you were happy with and you want to integrate all of those changes back into your  master  branch. We're going to merge things. To merge, we have to go back to the master branch:  $ git checkout master . (Good practice is to keep separate branches for all major experiments or directions you go. In case you lose track of the names of the branches you've created, this command:  git branch -va  will list them for you.)   Now, we merge with  $ git merge experiment . Remember, a merge is a special kind of commit that rolls all previous commits from both branches into one - Git will open your text editor and prompt you to add a message (it will have a default message already there if you want it). Save and exit and ta da! Your changes have been merged together.   git push  4.5. One of the most powerful aspects of using Git is the possibility of using it to manage collaborations. To do this, we have to make a copy of your repository available to others as a  remote . There are a variety of places on the web where this can be done; one of the most popular at the moment is  Github . Github allows a user to have an unlimited number of  public  repositories. Public repositories can be viewed and copied by anyone.  Private  repositories require a paid account, and access is controlled. If you are working on sensitive materials that can only be shared amongst the collaborators on a project, you should invest in an upgraded account (note that you can also control which files get included in commit; see  this help file . In essence, you simply list the file names you do not want committed; here's an  example ). Let's assume that your materials are not sensitive.   login to Github  On the upper right part of the screen there is a large + sign. Click on that, and select  new public repository  On the following screen, give your repo a name.  DO NOT 'initialize this repo with a readme.md'. Leave  add .gitignore  and  add license  set to NONE.  Click the green 'Create Repository' button.  You now have a space into which you will publish the repository on your machine. At the command line, we now need to tell Git the location of this space. We do that with the following command, where you will change  your-username  and  your-new-repo  appropriately:   $ git remote add origin https://github.com/YOUR-USERNAME/YOUR-NEW-REPO.git  g. Now we push your local copy of the repository onto the web, to the Github version of your repo:  git push -u origin master  NB  If you wanted to push a  branch  to your repository on the web instead, do you see how you would do that? If your branch was called  experiment , the command would look like this:  $ git push origin experiment   The changes can sometimes take a few minutes to show up on the website. Now, the next time you make changes to this repository, you can push them to your Github account - which is the 'origin' in the command above.  Add a new text file. Commit the changes. Push the changes to your account.   git clone  4.6. Imagine you are collaborating with one of your classmates. Your classmate is in charge of the project, and is keeping track of the 'official' folder of materials (eg, the repo). You wish to make some changes to the files in that repository. You can manage that collaboration via Github by making a copy, what Github calls a  fork .\n- Make sure you're logged into your Github account on the Github website. We're going to fork an example repository right now by going to  https://github.com/octocat/Spoon-Knife . Click the 'fork' button at top-right. Github now makes a copy of the repository in your own Github account!\n- To make a copy of that repository on your own machine, you will now clone it with the  git clone  command. (Remember: a 'fork' copies someone's Github repo into a repo in your OWN Github account; a 'clone' makes a copy on your own MACHINE). Type:  $ cd..\n$ pwd  We do that to make sure you're not  inside  any other repo you've made! Make sure you're not inside the repository we used in exercises 1 to 5, then proceed:  $ git clone https://github.com/YOUR-USERNAME/Spoon-Knife\n$ ls  You now have a folder called 'Spoon-Knife' on your machine! Any changes you make inside that folder can be tracked with commits. You can also  git push -u origin master  when you're inside it, and the changes will show up on your OWN copy (your fork) on Github.com.\nc. Make a fork of, and then clone, one of your classmates' repositories. Create a new branch. Add a new file to the repository on your machine, and then push it to your fork on Github. Remember, your new file will appear on the new branch you created, NOT the master branch.  pull request  4.7. Now, you let your collaborator know that you've made a change that you want her to  merge  into the original repository. You do this by issuing a  pull request . But first, we have to tell Git to keep an eye on that original repository, which we will call  upstream . You do this by adding that repository's location like so:   type (but change the address appropriately):   $ git remote add upstream THE-FULL-URL-TO-THEIR-REPO-ENDING-WITH-.git   You can keep your version of the remote up-to-date by fetching any new changes your classmate has done:   $ git fetch upstream   Now let's make a  pull  request (you might want to bookmark this  help document ). Go to your copy of your classmate's repository at your Github account. Make sure you've selected the correct branch you pushed your changes to, by selecting it from the Branches menu drop down list.  Click the 'new pull request' button.  The new page that appears can be confusing, but it is trying to double check with you which changes you want to make, and where.  'Base Branch'  is the branch where you want your changes to go, ie, your classmate's repository.  'head branch'  is the branch where you made  your  changes. Make sure these are set properly. Remember: the first one is the TO, the second one is the FROM: the place where you want your changes to go TO, FROM the place where you made the changes.  A pull request has to have a message attached to it, so that your classmate knows what kind of change you're proposing. Fill in the message fields appropriately, then hit the 'create pull request' button.   git merge again  4.8. Finally, the last bit of work to be done is to accept the pull request and  merge  the changes into the original repository.\n- Go to your repository on your Github account. Check to see if there are any 'pull requests' - these will be listed under the 'pull requests' tab. Click on that tab.\n- You can merge from the command line, but for now, you can simply click on the green 'merge pull request' button, and then the 'confirm merge' button. The changes your classmate has made have now been folded into your repository.\n- To get the updates on your local machine, go back to the command line and type  $ git pull origin master  Phew. You might want to do  $ history   recentcommands.md  just to remember what you've done. And then commit that to a repository.", 
            "title": "EXERCISE 4: a detailed look at using git on the command line"
        }, 
        {
            "location": "/module-1/Exercises/#conclusion", 
            "text": "In the modules to come, we will be using DHBox as we find data, fetch data, wrangle data, analyze data, and visualize data. It becomes very important that you note the kinds of commands you use or try, the thinking that you were doing at that point, and so on. You want to leave yourself (and anybody who comes after) breadcrumbs so that you understand what you were doing. Quick notes written in markdown, piping the history of what you've done to a file, and keeping those files in a repository alongside any other code or files that you may make will set you on the path to open access research and computational reproducibility. As Martha might say, 'and that's a good thing'.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/module-2/Finding Data/", 
            "text": "How do we find data, anyway? July 17 - 23\n\n\nConcepts\n\n\n\n\n'Something given'. That's a nice way of thinking about it. Of course, much of the data that we are 'given' wasn't really given \nwillingly\n. When we \ntopic model Martha Ballard's diary\n, did she \ngive\n this to us? Of course, she couldn't have imagined what we might try to do to it. Other kinds of data - census data, for instance - were compelled: folks had to answer the questions, on pain of punishment. This is all to suggest that there is a moral dimension to what we do with big data in history. Stop for a moment and read \n'the joys of big data'\n (if you haven't already) and then \n'the third wave of computational history'\n.\n\n\nDigitized data is not value-neutral; we need to think about, and talk about, what it means to collect, transform, analyze and visualize it. Who has the power here? (and you might also reflect on \n'the most profitable obsolete technology'\n ) Finally, you might also think about recent history - listen to Ian Milligan \ndiscuss how Yahoo's closure of Geocities represented a terrible blow to social history\n.\n\n\nAccepting that historical 'big' data is out there, that there's more material than one person can usefully digest and understand, and that a big-picture, macroscopic point of view is a useful perspective, means also thinking about the digital milieu that makes this possible. But see this piece by Tim Sherratt on \nSeams and edges: Dreams of aggregation, access \n discovery in a broken world\n. We interact with the data we find, and in the process, we alter both it and ourselves! As you do your projects and work through this workbook, think about the ethical, moral, and legal dimensions to what you are doing. \nAlways keep track of your thoughts in your notebook. Remember to put them up in your github repo\n.\n\n\nOk, so?\n\n\nSo how can we find big data? The exercises in this module will teach you how some historical materials get online, and the work involved in doing that. They will show you how to use wget on the command line to grab webpages; and they will introduce you to the concept of APIs and what you might achieve with them as a historian. Additional exercises show you how to use some existing free and commercial tools for webscraping; and we will also learn how to grab social media data as well.\n\n\nFor future references \nthis list of historical data sources\n may turn out to be handy. You should also perhaps dip into the \n'Data Fundamentals'\n part of \nData + Design\n; think of it as another excellent textbook to help you when you need another perspective on the materials in this course.\n\n\nAnd don't forget serendipity\n\n\nFollow researchers and institutions in your field of study. Once on Twitter I saw something that struck me as an excellent find. Penn Libraries tweeted, and I retweeted, \na link to a traveller's diary from the 19th century\n - a woman who sailed from the US to Europe and thence the Nile, which she ascended and explored.\n\nMy tweet\n led to a flurry of activity amongst scholars, and even now, the transcription has begun. Indeed, I made an android-only \ngame out of it\n.\n\n\nBut first... let's set a bit of framework.\n\n\nIf we're going to find data, we need to be able to access the power of our machines, to get them to do what we want. It's worth thinking about what Corey Doctorow has called \nthe war on general purpose computing\n as we begin...\n\n\n...and then thinking about what 'search' actually means. Check out Ted Underwood's piece on \n'Theorizing Research Practices We Forgot to Theorize Twenty Years Ago'\n.\n\n\nFinally, Cameron Blevins has some thoughts on the \n'perpetual sunrise of methodology'\n.\n\n\nWhat you need to do this week\n\n\n\n\nRespond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them \nhere\n\n\nDo the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Note that one of the exercises shows you how to get the data that you will need for your final project! Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your github account - for more on that, see the exercises!).\n\n\nSubmit your work \nhere\n\n\n\n\nReadings\n\n\nThis week, I want you to choose just \none\n of the articles linked to above to do your 'official' annotations on OR annotate one of these two articles regarding the 'Transcribing Bentham' project:\n\n\n\n\nCauser \n Wallace, \nBuilding A Volunteer Community: Results and Findings from Transcribe Bentham\n DHQ 6.2, 2012\n\n\nCauser, Tonra, \n Wallace \nTranscription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham\nLLC 27.2, 2012\n\n\n\n\nAgain, I also want you to respond to at least one substantive annotation made by your peers. Remember, with Hypothes.is you can annotate pdfs that you have opened in your browser from a website.\n\n\nReading questions\n: Have you ever sat down with one of the librarians to get help finding something? Consider the knowledge and labour involved not just with finding materials, but in making materials findable in the first place. Make an entry in your blog that reflects on these questions in the light of your annotations.", 
            "title": "How do we find data?"
        }, 
        {
            "location": "/module-2/Finding Data/#how-do-we-find-data-anyway-july-17-23", 
            "text": "", 
            "title": "How do we find data, anyway? July 17 - 23"
        }, 
        {
            "location": "/module-2/Finding Data/#concepts", 
            "text": "'Something given'. That's a nice way of thinking about it. Of course, much of the data that we are 'given' wasn't really given  willingly . When we  topic model Martha Ballard's diary , did she  give  this to us? Of course, she couldn't have imagined what we might try to do to it. Other kinds of data - census data, for instance - were compelled: folks had to answer the questions, on pain of punishment. This is all to suggest that there is a moral dimension to what we do with big data in history. Stop for a moment and read  'the joys of big data'  (if you haven't already) and then  'the third wave of computational history' .  Digitized data is not value-neutral; we need to think about, and talk about, what it means to collect, transform, analyze and visualize it. Who has the power here? (and you might also reflect on  'the most profitable obsolete technology'  ) Finally, you might also think about recent history - listen to Ian Milligan  discuss how Yahoo's closure of Geocities represented a terrible blow to social history .  Accepting that historical 'big' data is out there, that there's more material than one person can usefully digest and understand, and that a big-picture, macroscopic point of view is a useful perspective, means also thinking about the digital milieu that makes this possible. But see this piece by Tim Sherratt on  Seams and edges: Dreams of aggregation, access   discovery in a broken world . We interact with the data we find, and in the process, we alter both it and ourselves! As you do your projects and work through this workbook, think about the ethical, moral, and legal dimensions to what you are doing.  Always keep track of your thoughts in your notebook. Remember to put them up in your github repo .  Ok, so?  So how can we find big data? The exercises in this module will teach you how some historical materials get online, and the work involved in doing that. They will show you how to use wget on the command line to grab webpages; and they will introduce you to the concept of APIs and what you might achieve with them as a historian. Additional exercises show you how to use some existing free and commercial tools for webscraping; and we will also learn how to grab social media data as well.  For future references  this list of historical data sources  may turn out to be handy. You should also perhaps dip into the  'Data Fundamentals'  part of  Data + Design ; think of it as another excellent textbook to help you when you need another perspective on the materials in this course.  And don't forget serendipity  Follow researchers and institutions in your field of study. Once on Twitter I saw something that struck me as an excellent find. Penn Libraries tweeted, and I retweeted,  a link to a traveller's diary from the 19th century  - a woman who sailed from the US to Europe and thence the Nile, which she ascended and explored. My tweet  led to a flurry of activity amongst scholars, and even now, the transcription has begun. Indeed, I made an android-only  game out of it .  But first... let's set a bit of framework.  If we're going to find data, we need to be able to access the power of our machines, to get them to do what we want. It's worth thinking about what Corey Doctorow has called  the war on general purpose computing  as we begin...  ...and then thinking about what 'search' actually means. Check out Ted Underwood's piece on  'Theorizing Research Practices We Forgot to Theorize Twenty Years Ago' .  Finally, Cameron Blevins has some thoughts on the  'perpetual sunrise of methodology' .", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-2/Finding Data/#what-you-need-to-do-this-week", 
            "text": "Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them  here  Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. Note that one of the exercises shows you how to get the data that you will need for your final project! Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your github account - for more on that, see the exercises!).  Submit your work  here", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-2/Finding Data/#readings", 
            "text": "This week, I want you to choose just  one  of the articles linked to above to do your 'official' annotations on OR annotate one of these two articles regarding the 'Transcribing Bentham' project:   Causer   Wallace,  Building A Volunteer Community: Results and Findings from Transcribe Bentham  DHQ 6.2, 2012  Causer, Tonra,   Wallace  Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham LLC 27.2, 2012   Again, I also want you to respond to at least one substantive annotation made by your peers. Remember, with Hypothes.is you can annotate pdfs that you have opened in your browser from a website.  Reading questions : Have you ever sat down with one of the librarians to get help finding something? Consider the knowledge and labour involved not just with finding materials, but in making materials findable in the first place. Make an entry in your blog that reflects on these questions in the light of your annotations.", 
            "title": "Readings"
        }, 
        {
            "location": "/module-2/Exercises/", 
            "text": "Module 2: Exercises\n\n\nAll five exercises are on this page. Don't forget to scroll. If you have difficulties, or if the instructions need clarification, please click the 'issues' button and leave a note. Feel free to fork and improve these instructions, if you are so inclined. Remember, these exercises get progressively more difficult, and will require you to either download materials or read materials on other websites. Give yourself plenty of time. Try them all, and remember you can turn to your classmates for help. Work together!\n\n\nDO NOT suffer in silence as you try these exercises! Annotate, ask for help, set up an appointment, or find me in person.\n\n\nBackground\n\n\nWhere do we go to find data? Part of that problem is solved by knowing what question you are asking, and what \nkinds\n of data would help solve that question. Let's assume that you have a pretty good question you want an answer to - say, something concerning social and household history in early Ottawa, like what was the role of 'corner' stores (if such things exist?) in fostering a sense of neighbourhood - and begin thinking about how you'd find data to explore that question.\n\n\nThe exercises in this module cover:\n\n\n\n\nThe Dream Case\n\n\nWget\n\n\nWriting a program to extract data from a webpage\n\n\nCollecting data from Twitter\n\n\n\n\nThere is so much data available; with these methods, we can gather enormous amounts that will let us see large-scale macroscopic patterns. At the same time, it allows us to dive into the details with comparative ease. The thing is, not all digital data are created equally. Google has spent millions digitizing \neverything\n; newspapers have digitized their own collections. Genealogists and local historical societies upload yoinks of digitized photographs, wills, local tax records, \nyou-name-it\n, \nevery day\n. But, consider what Milligan has to say about \n'illusionary order'\n:\n\n\n\n\n[...] poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it\u2019s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I\u2019ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box.\n\n\n\n\nAsk yourself: what are some of the key dangers? Reflect: how have you used digitized resources uncritically in the past? Remember: 'To digitize' doesn't - or shouldn't - mean uploading a photograph of a document. There's a lot more going on that that. We'll get to that in a moment.\n\n\nEXERCISE 1: The Dream Case\n\n\nIn the dream case, your data are not just images, but are actually sorted and structured into some kind of pre-existing database. There are choices made in the \ncreation\n of the database, but a good database, a good project, will lay out for you their decision making, their corpora, and how they've dealt with ambiguity and so on. You search using a robust interface, and you get a well-formed spreadsheet of data in return. Two examples of 'dream case' data:\n\n\n\n\nEpigraphic Database Heidelberg\n\n\nCommwealth War Graves Commission, Find War Dead\n\n\n\n\nExplore both databases. Perform a search of interest to you. In the case of the epigraphic database, if you've done any Latin, try searching for terms related to occupations; or you could search '\nFiglin*\n'. In the CWGC database, search your own surname. Download your results. You now have data that you can explore! Using the nano text editor in your DHBox, make a record (or records) of what you searched, the URL for your search \n its results, and where you're keeping your data. Lodge a copy of this record in your repository.\n\n\nEXERCISE 2: Wget\n\n\nYou've already encountered wget in the introduction to this workbook, when you were setting up your DHBox to use Pandoc. In this exercise, I want you to do \nIan Milligan's wget tutorial at the Programming Historian\n to learn more about the power of this command, and how to wield that power properly. Skip ahead to step 2, since your DHBox already has wget installed. (If you want to continue to use wget after this course is over, you will have to install it on your own machine, obviously).\n\n\nOnce you've completed Milligan's tutorial, remember to put your history into a new markdown file, and to lodge a copy of it in your repository.\n\n\nNow that you're \nau fait\n with wget, I want you to use wget to download the Shawville Equity from the Quebec provincial archives in a responsible and respectful manner. Otherwise, you will look like a bot attacking their site. The data is in this location: \nhttp://collections.banq.qc.ca:8008/jrn03/equity/src/\n. Make a new directory: \n$ mkdir equity\n and then cd into it: \n$ cd equity\n. Make sure you're in the directory by typing \n$ pwd\n. Work out the command to download the Equity data; configure your command to retrieve only the txt files (as the pdfs are large and will take much time and bandwidth to acquire.) (Ok, I'm not heartless - scroll to the bottom of this page to see what the command should be).\n\n\nAdd your command to your history file, and lodge it in your repository.\n\n\nOpen some of the text files in Nano. How good, how careful was the 'object character recognition'? Part of the point of working with the Equity files is to show that even with horrible 'digitization', we can still extract useful insights. Digitization is more than simply throwing automatically generated files online. Good digitization requires scholarly work and effort! We will learn how to do this properly in the next exercise.\n\n\nEXERCISE 3: TEI\n\n\nDigitization requires human intervention. This can be as straightforward as correcting errors or adjusting the scanner settings when we do OCR, or it can be the rather more involved work of adding a layer of semantic information to the text. When we mark up a text with the semantic hooks and signs that explain we are talking about 'London, Ontario' rather than 'London, UK', we've made the text a whole lot more useful for other scholars or tools. In this exercise, you will do some basic marking up of a text using standards from the \nText Encoding Initiative\n. (Some of the earliest digital history work was along these lines). The TEI exercise requires carefully attention to detail. Read through it before you try it. In this exercise, you'll transcribe a page from an abolitionist's pamphlet. You'll also think about ways of transforming the resulting XML into other formats. Make notes in a file to upload to your repository, and upload your XML and your XSL file to your own repository as well. (As an added optional challenge, create a gh-pages branch and figure out the direct URL to your XML file, and email that URL to me).\n\n\nThe exercise may be found \nhere\n.\n\n\nI will note that a perfectly fine final project for HIST3814 might be to use this exercise as a model to markup a single issue of the Equity and suggest ways this material might be explored. Remember to make (and lodge in your repository) a file detailing the work you've done and any issues you've run into.\n\n\nEXERCISE 4: APIs\n\n\nSometimes, a website will have what is called an 'application programming interface'. In essence, this lets a program on your computer talk to the computer serving the website you're interested in, such that the website gives you the data that you're looking for.\n\n\nThat is, instead of \nyou\n punching in the search terms, and copying and pasting the results, you get the computer to do it. More or less. The thing is, the results come back to you in a machine-readable format - often, JSON, which is a kind of text format. It looks like this:\n\n\n\nThe 'Canadiana Discovery Portal' has tonnes of materials related to Canada's history, from a wide variety of sources. Its search page is at: http://search.canadiana.ca/\n\n\n\n\nGo there, and search \"ottawa\" and set the date range to 1800 to 1900. Hit enter. You are presented with a page of results -56 249 results! That's a lot of data. But do you notice the address bar of your browser? It'll say something like this:\n\n\n\n\nhttp://search.canadiana.ca/search?q=ottawa\nfield=\ndf=1900\ndt=1900\n\n\nYour search query has been put into the URL. You're looking at the API! Everything after /search is a command that you are sending to the Canadiana server.\n\n\nScroll through the results, and you'll see a number just before the ?\n\n\nhttp://search.canadiana.ca/search/2?df=1800\ndt=1900\nq=ottawa\nfield=\n\n\nhttp://search.canadiana.ca/search/3?df=1800\ndt=1900\nq=ottawa\nfield=\n\n\nhttp://search.canadiana.ca/search/4?df=1800\ndt=1900\nq=ottawa\nfield=\n\n\n....all the way up to 5625 (ie, 10 results per page, so 56249 / 10).\n\n\nIf you go to http://search.canadiana.ca/support/api you can see the full list of options. What we are particularly interested in now is the bit that looks like this:\n\n\nfmt=json\n\n\nAdd that to your query URL. How different the results now look! What's nice here is that the data is formatted in a way that makes sense to a machine - which we'll learn more about in due course.\n\n\nIf you look back at the full list of API options, you'll see at the bottom that one of the options is 'retrieving individual item records'; the key for that is a field called 'oocihm'. If you look at your page of json results, and scroll through them, you'll see that each individual record has its own oocihm number. If we could get a list of those, we'd be able to programmatically slot them into the commands for retrieving individual item records:\n\n\nhttp://search.canadiana.ca/view/oocihm.16278/?r=0\ns=1\nfmt=json\napi_text=1\n\n\nThe problem is: how to retrieve those oocihm numbers. The answer is, 'we write a program'. And the program that you want can be \nfound here\n. Study that program carefully. There are a number of useful things happening in there, notably 'curl', 'jq', 'sed', 'awk'. curl  is a program for downloading webpages, jq for dealing with json, and sed and awk for searching within and cleaning up text. If this all sounds greek to you, there is an excellent gentle introduction over at \nWilliam Turkel's blog\n.\n\n\nSo here's what we're going to do.\n\n\n\n\nWe need jq. We install it into our DHBox with \n$ sudo apt-get install jq -y\n\n\nWe need to create a program. Make a new directory for this exercise like so: \n$ mkdir m2e4\n. Then, change into that directory by typing \n$ cd m2e4\n. Make sure that's where you are by typing \n$ pwd\n. Now, make an empty file for our program with \n$ touch canadiana.sh\n. Touch makes an empty file; the .sh in the filename indicates that this is a shell script.\n\n\nOpen the empty file with \n$ nano canadiana.sh\n. Now, the program that Ian Milligan wrote makes calls to the API that \nused to live\n at eco.canadiana.ca. But note the error message \nhere\n. So we have to change Milligan's script so that it points to the API at search.canadiana.ca. Copy the script below into your empty canadiana.sh. If you want, adjust the search parameters (in the line starting with \npages\n) for material you're more interested in.\n\n\n\n\n#! /bin/bash\n\npages=$(curl 'http://search.canadiana.ca/search?q=ottawa*\nfield=\nso=score\ndf=1914\ndt=1918\nfmt=json' | jq '.pages')\n\n# this goes into the results and reads the value of 'pages' in each one of them.\n# it then tells us how many pages we're going to have.\n\necho \nPages:\n$pages\n\n# this says 'for each one of these pages, download the 'key' value on each page'\n\nfor i in $(seq 1 $pages)\ndo\n        curl 'http://search.canadiana.ca/search/'${i}'?q=montenegr*\nfield=\nso=score\ndf=1914\ndt=1918\nfmt=json' | jq '.docs[] | {key}' \n results.txt\ndone\n\n# the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks \n\\\n (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase \nkey:\n and delete it too.\n\nsed -e 's/\\\nkey\\\n: \\\n//g' results.txt | tr -d \n\\\n | tr -d \n{\n | tr -d \n}\n | tr -s \n \n | sed '/^\\s*$/d' | tr -d ' ' \n cleanlist.txt\n# this adds a prefix and a suffix.\n\nawk '$0=\nsearch.canadiana.ca/view/\n$0' cleanlist.txt| awk '{print $0 \n/1?r=0\ns=1\nfmt=json\napi_text=1\n}' \n urlstograb.txt\n\n# then if we want we can take those URLs and output them all to a big text file for analysis.\n\nwget -i urlstograb.txt -O output.txt\n\n\n\n\nHit ctrl+x to exit Nano, and save the changes.\n\n\nBefore we can run this program, we have to tell DHBox that it is alright to run it. We do that by changing the 'permissions' on the file, like so:\n\n\n$ chmod 755 canadiana.sh\n\n\nAnd now we can run the program:\n\n\n$ ./canadiana.sh\n\n\nTa da! You now have a pretty powerful tool now for grabbing data from one of the largest portals for Canadian history! Download your output.txt file to your computer via the file manager and have a look at it. Make sure to make a file noting what you've done, commands you've made, etc, and lodge it in your repository.\n\n\nEXERCISE 5: Mining Twitter\n\n\nEd Summers is part of a project called '\nDocumenting the Now\n' which is developing tools to collect and understand the historical materials being shared (and lost) on social media. One component of Documenting the Now is the Twitter Archiving Tool, '\nTwarc\n'. In this exercise, you are going to use Twarc to create an archive of Tweets relevant to a current trending news topic.\n\n\n\n\nFirst of all, you need to set up a Twitter account, if you haven't already got one. Do so, but make sure to minimize any personal information that is exposed. For instance, don't make your handle the same as your real name. Turn off geolocation. Do not give your actual location in the profile. View the settings, and make sure all of the privacy settings are dialed down. For the time being, you \ndo\n have to associate a cell phone number with your account. You can delete that once you've done the next step.\n\n\nGo to \nhttps://apps.twitter.com/\n and click on \u2018new app\u2019. Then, on the \u2018new application\u2019 page, just give your app a name like my-twarc\u2019 or similar, and website use the Crafting Digital History site url (although for our purposes any website will do). You don\u2019t need to fill in any of the rest of the fields. Continue on to the next page (tick off the box saying you\u2019ve read the developer code of behaviour). This next page shows you all the details about your new application.\n\n\nClick on the \u2018keys and access tokens\u2019 tab. Copy the consumer key, the consumer secret to a text file. Click on the \u2018create access tokens\u2019 at the bottom of the page. This generates an access token and an access secret. Copy those to your text file, save it. \ndo not put this file in your repo or leave it online anywhere\n \n\n\nIn your DHbox, at the command line, type \n$ sudo pip install twarc\n. Twarc is written in python, which is already installed in DHbox. 'Pip' is a package manager for installing new python modules and packages. If you forget the \nsudo\n, you will get an error to the effect that you don't have permission. So sudo!\n\n\nNow type \n$ twarc configure\n and give it the information it asks for (your consumer secret etc).\n\n\nYou're now ready to search. For instance, \n$ twarc search canada150 \n search.json\n will search Twitter for posts using the canada150 hashtag. Note that Twitter only gives access to the last two weeks or so via search. For grabbing the stream \nas an event happens\n you'd use the \ntwarc stream\n command - see the Twarc documentation for more.\n\n\nIt might take some time for the search to happen. You can always force-stop the search by hitting ctrl+c. If you do that though there could be an error in the formatting of the file which will throw an error when you get to step 10. You can still open the json in a text editor though, but you will have to go to the end of the file and fix the formatting.\n\n\nThe data being collected is in json format. That is, a list of 'keys' and 'values'. This is a handy format for computers, and some data visualization platforms require data in this format. For our purposes we might want to transform the json into a csv (comma separated) table - a spreadsheet.\n\n\nWe will install a command that can convert the json to csv format like so: \n$ sudo npm install json2csv --save -g\n. Full details about the command are \nhere\n.\n\n\nConvert your \nsearch.json\n to csv like so: \njson2csv -i search.json -o out.csv\n\n\nExamine your data either in a text editor or in a spreadsheet. Use twarc to create a file with a list of ids. Lodge this list and your history and notes in your repository.\n\n\n\n\nNB. Twitter forbids the sharing of the full metadata of a collection of tweets. You may however share a list of tweet IDs. See the Twarc documentation for the instructions on how to do that.\n\n\nWhat can you do with this data? Examine the Twarc repository, especially its utilities. You could extract the geolocated ones and map them. You could examine the difference between 'male' and 'female' tweeters (and how problematic might that be?). In your csv, save the text of the posts to a new file and upload it to something like \nVoyant\n to visualize trends over time. Google for analyzes of twitter data to get some ideas.\n\n\nEXERCISE 6: Using Tesseract to turn a pdf into text\n\n\nWe've all used pdfs - in academia, we often use a pdf to make sure that a page of text looks like an actual physical page of paper. PDFs always look the same on whatever machine they are displayed on, because they contain within themselves the complete description of what the 'page' should look like. If you've ever selected text within a pdf, you were only able to do this because there was within the pdf a text layer on top of the image layer. But when we digitize old newspapers, the pdf that results only contains the image layer, not the text. To turn that image into text, we have to do what's called 'object character recognition', or OCR. An OCR algorithm looks at the pattern of pixels in the image, and maps these against the shapes it 'knows' to be an A, or an a, or a B, or a \n, and so on. Cleaner, sharper printing gives better results as do hi resolution images free from noise. People who have a lot of material to OCR use some very powerful tools to identify blocks of text within the newspaper page, and then train the machine to identify these, a process beyond us just now (but see \nthis q \n a on stackoverflow\n if you're interested).\n\n\nIn this exercise, you'll:\n\n\n\n\ninstall the Tesseract OCR engine into your DHBox\n\n\ndownload an edition of the Equity\n\n\ninstall and use pdftk to burst the pdf into individual one-page files\n\n\ninstall and use imagemagick to convert the pdf into tiff image format\n\n\nuse Tesseract to OCR the resulting pages.\n\n\n\n\nBegin by making a new director for this exercise: \nmkdir ocr-test\n. Change directories into it: \ncd ocr-test\n\n\n\n\n$ sudo apt-get install tesseract-ocr\n will grab the latest version of tesseract and install it into your dhbox. Enter your password when the computer asks for it.\n\n\n$ sudo apt-get install imagemagick\n to install imagemagick\n\n\n$ sudo apt-get install pdftk\n to install pdftk.\n\n\nNow let's grab an edition of the Equity. Use wget to grab the pdf from July 4th 1957.\n\n\nLet's burst it into individual pages. The command is \npdtk \ninput file\n burst\n, so \n$ pdftk 83471_1957-07-04.pdf burst\n\n\nLet's convert the first file to tiff with imagemagick's convert command: \n$ convert -density 300 pg_0001.pdf -depth 8 -strip -background white -alpha off file.tiff\n You want a high density image, which is what the -density and the -depth flags do; the rest of the command formats the image in a way that Tesseract expects to encounter text. This command might take a while. Just wait, be patient.\n\n\nExtract text! \n$ tesseract file.tiff output.txt\n This might also take some time.\n\n\n\n\nDownload the output.txt file with the DHBox filemanager. Open the file with a text editor (there might be a lot of white space at the start of the file, fyi). Grab the txt file created by the Provincial Archives. Is yours better or worse than theirs? Look up the \nTesseract wiki\n. What other options could you use with the tesseract command to improve the results? For future reference, here are two guides to automating bulk OCR (multiple files) with tesseract: \nPeirson's\n, \nSchmidt's\n.\n\n\nwget command to grab The Equity\n\n\nHey - you've scrolled all the way down here. Here's the wget command to nicely download the Equity txt files:\n\nwget http://collections.banq.qc.ca:8008/jrn03/equity/src/ -A .txt -r --no-parent -nd \u2013w 2 --limit-rate=20k\n\n\nWhen you run this command, it will go through the entire file structure a couple of times, 'rejecting' index.html etc because we've asked it to just grab the .txt files. Be patient. How would you change it to grab just the pdfs?", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-2/Exercises/#module-2-exercises", 
            "text": "All five exercises are on this page. Don't forget to scroll. If you have difficulties, or if the instructions need clarification, please click the 'issues' button and leave a note. Feel free to fork and improve these instructions, if you are so inclined. Remember, these exercises get progressively more difficult, and will require you to either download materials or read materials on other websites. Give yourself plenty of time. Try them all, and remember you can turn to your classmates for help. Work together!  DO NOT suffer in silence as you try these exercises! Annotate, ask for help, set up an appointment, or find me in person.", 
            "title": "Module 2: Exercises"
        }, 
        {
            "location": "/module-2/Exercises/#background", 
            "text": "Where do we go to find data? Part of that problem is solved by knowing what question you are asking, and what  kinds  of data would help solve that question. Let's assume that you have a pretty good question you want an answer to - say, something concerning social and household history in early Ottawa, like what was the role of 'corner' stores (if such things exist?) in fostering a sense of neighbourhood - and begin thinking about how you'd find data to explore that question.  The exercises in this module cover:   The Dream Case  Wget  Writing a program to extract data from a webpage  Collecting data from Twitter   There is so much data available; with these methods, we can gather enormous amounts that will let us see large-scale macroscopic patterns. At the same time, it allows us to dive into the details with comparative ease. The thing is, not all digital data are created equally. Google has spent millions digitizing  everything ; newspapers have digitized their own collections. Genealogists and local historical societies upload yoinks of digitized photographs, wills, local tax records,  you-name-it ,  every day . But, consider what Milligan has to say about  'illusionary order' :   [...] poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it\u2019s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I\u2019ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box.   Ask yourself: what are some of the key dangers? Reflect: how have you used digitized resources uncritically in the past? Remember: 'To digitize' doesn't - or shouldn't - mean uploading a photograph of a document. There's a lot more going on that that. We'll get to that in a moment.", 
            "title": "Background"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-1-the-dream-case", 
            "text": "In the dream case, your data are not just images, but are actually sorted and structured into some kind of pre-existing database. There are choices made in the  creation  of the database, but a good database, a good project, will lay out for you their decision making, their corpora, and how they've dealt with ambiguity and so on. You search using a robust interface, and you get a well-formed spreadsheet of data in return. Two examples of 'dream case' data:   Epigraphic Database Heidelberg  Commwealth War Graves Commission, Find War Dead   Explore both databases. Perform a search of interest to you. In the case of the epigraphic database, if you've done any Latin, try searching for terms related to occupations; or you could search ' Figlin* '. In the CWGC database, search your own surname. Download your results. You now have data that you can explore! Using the nano text editor in your DHBox, make a record (or records) of what you searched, the URL for your search   its results, and where you're keeping your data. Lodge a copy of this record in your repository.", 
            "title": "EXERCISE 1: The Dream Case"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-2-wget", 
            "text": "You've already encountered wget in the introduction to this workbook, when you were setting up your DHBox to use Pandoc. In this exercise, I want you to do  Ian Milligan's wget tutorial at the Programming Historian  to learn more about the power of this command, and how to wield that power properly. Skip ahead to step 2, since your DHBox already has wget installed. (If you want to continue to use wget after this course is over, you will have to install it on your own machine, obviously).  Once you've completed Milligan's tutorial, remember to put your history into a new markdown file, and to lodge a copy of it in your repository.  Now that you're  au fait  with wget, I want you to use wget to download the Shawville Equity from the Quebec provincial archives in a responsible and respectful manner. Otherwise, you will look like a bot attacking their site. The data is in this location:  http://collections.banq.qc.ca:8008/jrn03/equity/src/ . Make a new directory:  $ mkdir equity  and then cd into it:  $ cd equity . Make sure you're in the directory by typing  $ pwd . Work out the command to download the Equity data; configure your command to retrieve only the txt files (as the pdfs are large and will take much time and bandwidth to acquire.) (Ok, I'm not heartless - scroll to the bottom of this page to see what the command should be).  Add your command to your history file, and lodge it in your repository.  Open some of the text files in Nano. How good, how careful was the 'object character recognition'? Part of the point of working with the Equity files is to show that even with horrible 'digitization', we can still extract useful insights. Digitization is more than simply throwing automatically generated files online. Good digitization requires scholarly work and effort! We will learn how to do this properly in the next exercise.", 
            "title": "EXERCISE 2: Wget"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-3-tei", 
            "text": "Digitization requires human intervention. This can be as straightforward as correcting errors or adjusting the scanner settings when we do OCR, or it can be the rather more involved work of adding a layer of semantic information to the text. When we mark up a text with the semantic hooks and signs that explain we are talking about 'London, Ontario' rather than 'London, UK', we've made the text a whole lot more useful for other scholars or tools. In this exercise, you will do some basic marking up of a text using standards from the  Text Encoding Initiative . (Some of the earliest digital history work was along these lines). The TEI exercise requires carefully attention to detail. Read through it before you try it. In this exercise, you'll transcribe a page from an abolitionist's pamphlet. You'll also think about ways of transforming the resulting XML into other formats. Make notes in a file to upload to your repository, and upload your XML and your XSL file to your own repository as well. (As an added optional challenge, create a gh-pages branch and figure out the direct URL to your XML file, and email that URL to me).  The exercise may be found  here .  I will note that a perfectly fine final project for HIST3814 might be to use this exercise as a model to markup a single issue of the Equity and suggest ways this material might be explored. Remember to make (and lodge in your repository) a file detailing the work you've done and any issues you've run into.", 
            "title": "EXERCISE 3: TEI"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-4-apis", 
            "text": "Sometimes, a website will have what is called an 'application programming interface'. In essence, this lets a program on your computer talk to the computer serving the website you're interested in, such that the website gives you the data that you're looking for.  That is, instead of  you  punching in the search terms, and copying and pasting the results, you get the computer to do it. More or less. The thing is, the results come back to you in a machine-readable format - often, JSON, which is a kind of text format. It looks like this:  The 'Canadiana Discovery Portal' has tonnes of materials related to Canada's history, from a wide variety of sources. Its search page is at: http://search.canadiana.ca/   Go there, and search \"ottawa\" and set the date range to 1800 to 1900. Hit enter. You are presented with a page of results -56 249 results! That's a lot of data. But do you notice the address bar of your browser? It'll say something like this:   http://search.canadiana.ca/search?q=ottawa field= df=1900 dt=1900  Your search query has been put into the URL. You're looking at the API! Everything after /search is a command that you are sending to the Canadiana server.  Scroll through the results, and you'll see a number just before the ?  http://search.canadiana.ca/search/2?df=1800 dt=1900 q=ottawa field=  http://search.canadiana.ca/search/3?df=1800 dt=1900 q=ottawa field=  http://search.canadiana.ca/search/4?df=1800 dt=1900 q=ottawa field=  ....all the way up to 5625 (ie, 10 results per page, so 56249 / 10).  If you go to http://search.canadiana.ca/support/api you can see the full list of options. What we are particularly interested in now is the bit that looks like this:  fmt=json  Add that to your query URL. How different the results now look! What's nice here is that the data is formatted in a way that makes sense to a machine - which we'll learn more about in due course.  If you look back at the full list of API options, you'll see at the bottom that one of the options is 'retrieving individual item records'; the key for that is a field called 'oocihm'. If you look at your page of json results, and scroll through them, you'll see that each individual record has its own oocihm number. If we could get a list of those, we'd be able to programmatically slot them into the commands for retrieving individual item records:  http://search.canadiana.ca/view/oocihm.16278/?r=0 s=1 fmt=json api_text=1  The problem is: how to retrieve those oocihm numbers. The answer is, 'we write a program'. And the program that you want can be  found here . Study that program carefully. There are a number of useful things happening in there, notably 'curl', 'jq', 'sed', 'awk'. curl  is a program for downloading webpages, jq for dealing with json, and sed and awk for searching within and cleaning up text. If this all sounds greek to you, there is an excellent gentle introduction over at  William Turkel's blog .  So here's what we're going to do.   We need jq. We install it into our DHBox with  $ sudo apt-get install jq -y  We need to create a program. Make a new directory for this exercise like so:  $ mkdir m2e4 . Then, change into that directory by typing  $ cd m2e4 . Make sure that's where you are by typing  $ pwd . Now, make an empty file for our program with  $ touch canadiana.sh . Touch makes an empty file; the .sh in the filename indicates that this is a shell script.  Open the empty file with  $ nano canadiana.sh . Now, the program that Ian Milligan wrote makes calls to the API that  used to live  at eco.canadiana.ca. But note the error message  here . So we have to change Milligan's script so that it points to the API at search.canadiana.ca. Copy the script below into your empty canadiana.sh. If you want, adjust the search parameters (in the line starting with  pages ) for material you're more interested in.   #! /bin/bash\n\npages=$(curl 'http://search.canadiana.ca/search?q=ottawa* field= so=score df=1914 dt=1918 fmt=json' | jq '.pages')\n\n# this goes into the results and reads the value of 'pages' in each one of them.\n# it then tells us how many pages we're going to have.\n\necho  Pages: $pages\n\n# this says 'for each one of these pages, download the 'key' value on each page'\n\nfor i in $(seq 1 $pages)\ndo\n        curl 'http://search.canadiana.ca/search/'${i}'?q=montenegr* field= so=score df=1914 dt=1918 fmt=json' | jq '.docs[] | {key}'   results.txt\ndone\n\n# the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks  \\  (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase  key:  and delete it too.\n\nsed -e 's/\\ key\\ : \\ //g' results.txt | tr -d  \\  | tr -d  {  | tr -d  }  | tr -s     | sed '/^\\s*$/d' | tr -d ' '   cleanlist.txt\n# this adds a prefix and a suffix.\n\nawk '$0= search.canadiana.ca/view/ $0' cleanlist.txt| awk '{print $0  /1?r=0 s=1 fmt=json api_text=1 }'   urlstograb.txt\n\n# then if we want we can take those URLs and output them all to a big text file for analysis.\n\nwget -i urlstograb.txt -O output.txt  Hit ctrl+x to exit Nano, and save the changes.  Before we can run this program, we have to tell DHBox that it is alright to run it. We do that by changing the 'permissions' on the file, like so:  $ chmod 755 canadiana.sh  And now we can run the program:  $ ./canadiana.sh  Ta da! You now have a pretty powerful tool now for grabbing data from one of the largest portals for Canadian history! Download your output.txt file to your computer via the file manager and have a look at it. Make sure to make a file noting what you've done, commands you've made, etc, and lodge it in your repository.", 
            "title": "EXERCISE 4: APIs"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-5-mining-twitter", 
            "text": "Ed Summers is part of a project called ' Documenting the Now ' which is developing tools to collect and understand the historical materials being shared (and lost) on social media. One component of Documenting the Now is the Twitter Archiving Tool, ' Twarc '. In this exercise, you are going to use Twarc to create an archive of Tweets relevant to a current trending news topic.   First of all, you need to set up a Twitter account, if you haven't already got one. Do so, but make sure to minimize any personal information that is exposed. For instance, don't make your handle the same as your real name. Turn off geolocation. Do not give your actual location in the profile. View the settings, and make sure all of the privacy settings are dialed down. For the time being, you  do  have to associate a cell phone number with your account. You can delete that once you've done the next step.  Go to  https://apps.twitter.com/  and click on \u2018new app\u2019. Then, on the \u2018new application\u2019 page, just give your app a name like my-twarc\u2019 or similar, and website use the Crafting Digital History site url (although for our purposes any website will do). You don\u2019t need to fill in any of the rest of the fields. Continue on to the next page (tick off the box saying you\u2019ve read the developer code of behaviour). This next page shows you all the details about your new application.  Click on the \u2018keys and access tokens\u2019 tab. Copy the consumer key, the consumer secret to a text file. Click on the \u2018create access tokens\u2019 at the bottom of the page. This generates an access token and an access secret. Copy those to your text file, save it.  do not put this file in your repo or leave it online anywhere    In your DHbox, at the command line, type  $ sudo pip install twarc . Twarc is written in python, which is already installed in DHbox. 'Pip' is a package manager for installing new python modules and packages. If you forget the  sudo , you will get an error to the effect that you don't have permission. So sudo!  Now type  $ twarc configure  and give it the information it asks for (your consumer secret etc).  You're now ready to search. For instance,  $ twarc search canada150   search.json  will search Twitter for posts using the canada150 hashtag. Note that Twitter only gives access to the last two weeks or so via search. For grabbing the stream  as an event happens  you'd use the  twarc stream  command - see the Twarc documentation for more.  It might take some time for the search to happen. You can always force-stop the search by hitting ctrl+c. If you do that though there could be an error in the formatting of the file which will throw an error when you get to step 10. You can still open the json in a text editor though, but you will have to go to the end of the file and fix the formatting.  The data being collected is in json format. That is, a list of 'keys' and 'values'. This is a handy format for computers, and some data visualization platforms require data in this format. For our purposes we might want to transform the json into a csv (comma separated) table - a spreadsheet.  We will install a command that can convert the json to csv format like so:  $ sudo npm install json2csv --save -g . Full details about the command are  here .  Convert your  search.json  to csv like so:  json2csv -i search.json -o out.csv  Examine your data either in a text editor or in a spreadsheet. Use twarc to create a file with a list of ids. Lodge this list and your history and notes in your repository.   NB. Twitter forbids the sharing of the full metadata of a collection of tweets. You may however share a list of tweet IDs. See the Twarc documentation for the instructions on how to do that.  What can you do with this data? Examine the Twarc repository, especially its utilities. You could extract the geolocated ones and map them. You could examine the difference between 'male' and 'female' tweeters (and how problematic might that be?). In your csv, save the text of the posts to a new file and upload it to something like  Voyant  to visualize trends over time. Google for analyzes of twitter data to get some ideas.", 
            "title": "EXERCISE 5: Mining Twitter"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-6-using-tesseract-to-turn-a-pdf-into-text", 
            "text": "We've all used pdfs - in academia, we often use a pdf to make sure that a page of text looks like an actual physical page of paper. PDFs always look the same on whatever machine they are displayed on, because they contain within themselves the complete description of what the 'page' should look like. If you've ever selected text within a pdf, you were only able to do this because there was within the pdf a text layer on top of the image layer. But when we digitize old newspapers, the pdf that results only contains the image layer, not the text. To turn that image into text, we have to do what's called 'object character recognition', or OCR. An OCR algorithm looks at the pattern of pixels in the image, and maps these against the shapes it 'knows' to be an A, or an a, or a B, or a  , and so on. Cleaner, sharper printing gives better results as do hi resolution images free from noise. People who have a lot of material to OCR use some very powerful tools to identify blocks of text within the newspaper page, and then train the machine to identify these, a process beyond us just now (but see  this q   a on stackoverflow  if you're interested).  In this exercise, you'll:   install the Tesseract OCR engine into your DHBox  download an edition of the Equity  install and use pdftk to burst the pdf into individual one-page files  install and use imagemagick to convert the pdf into tiff image format  use Tesseract to OCR the resulting pages.   Begin by making a new director for this exercise:  mkdir ocr-test . Change directories into it:  cd ocr-test   $ sudo apt-get install tesseract-ocr  will grab the latest version of tesseract and install it into your dhbox. Enter your password when the computer asks for it.  $ sudo apt-get install imagemagick  to install imagemagick  $ sudo apt-get install pdftk  to install pdftk.  Now let's grab an edition of the Equity. Use wget to grab the pdf from July 4th 1957.  Let's burst it into individual pages. The command is  pdtk  input file  burst , so  $ pdftk 83471_1957-07-04.pdf burst  Let's convert the first file to tiff with imagemagick's convert command:  $ convert -density 300 pg_0001.pdf -depth 8 -strip -background white -alpha off file.tiff  You want a high density image, which is what the -density and the -depth flags do; the rest of the command formats the image in a way that Tesseract expects to encounter text. This command might take a while. Just wait, be patient.  Extract text!  $ tesseract file.tiff output.txt  This might also take some time.   Download the output.txt file with the DHBox filemanager. Open the file with a text editor (there might be a lot of white space at the start of the file, fyi). Grab the txt file created by the Provincial Archives. Is yours better or worse than theirs? Look up the  Tesseract wiki . What other options could you use with the tesseract command to improve the results? For future reference, here are two guides to automating bulk OCR (multiple files) with tesseract:  Peirson's ,  Schmidt's .  wget command to grab The Equity  Hey - you've scrolled all the way down here. Here's the wget command to nicely download the Equity txt files: wget http://collections.banq.qc.ca:8008/jrn03/equity/src/ -A .txt -r --no-parent -nd \u2013w 2 --limit-rate=20k  When you run this command, it will go through the entire file structure a couple of times, 'rejecting' index.html etc because we've asked it to just grab the .txt files. Be patient. How would you change it to grab just the pdfs?", 
            "title": "EXERCISE 6: Using Tesseract to turn a pdf into text"
        }, 
        {
            "location": "/module-3/Wrangling Data/", 
            "text": "Wrangling Data July 24 - 30\n\n\nConcepts\n\n\nIn the previous module, we successfully grabbed \nlots\n of data from various online repositories. Some of it was already in well-structured tables; much of it was not. All of it was text though. Initially, it (or most of it) was just scanned images of documents. At some point, \nobject character recognition\n was used to identify the black dots from the white dots in those images, to recognize the patterns that make up letters, numbers, and punctuation. There are commercial products that can do this (and we have some installed in the Underhill Research Room that you can use), and there are \nfree products that you can install\n on your computer to do it yourself.\n\n\nIt all looks so neat and tidy. Ian Milligan discusses this \n'illusionary order'\n and its implications for historians:\n\n\n\n\nIn this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely \u2013 if ever \u2013 made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources.\n\n\n\n\nJust as we saw with Ted Underwood's article on \ntheorizing search\n, these 'simple' steps in the research process are anything but. They are also profoundly theoretical in how they change what it is we can know. In archaeology, every step of the method, every stage in the process, has a profound impact on the stories we eventually tell about the past. Decisions we make destroy data, and create new data. Historians aren't used to thinking about these kinds of issues!\n\n\nThere are also \nmanual\n ways of doing the same thing as OCR does - we call these things 'humans', and we organize their work through 'crowdsourcing'. We break the process up into wee manageable steps, and make these available over the net. Sometimes we gamify these steps, to make them more 'fun'. If several people all work on the same piece of text, the thinking is that errors will cancel each other out: a proper transcription will emerge from the work of the crowd.  While transcriptions might've provided the earliest examples of crowdsourcing research (but see also \nThe HeritageCrowd Project\n and the subsequent \n'How I Lost the Crowd'\n), other tasks are now finding their way into the crowdsourced world - see the archaeological applications within the \nMicroPasts\n platform. These include things like 'masking' artefact photographs in order to develop 3d photogrammetric models.\n\n\nBut often, we don't have a whole crowd. We're just one person, alone, with a computer, at the archive. Or working with someone else's \ndigitized image that we found online\n. How do we wrangle that data? Let's start with \nM. H. Beal's account of how she 'xml'd her way to data management'\n and then consider a few more of the nuts and bolts of her work in \nOA TEI-XML DH on the WWW; or, My Guide to Acronymic Success\n.\n\n\nThis kind of work is extraordinarily important!\n You already had a taste of it in the TEI exercise in the last module. (Now, if we had a \nseriously\n big project where we were transcribing lots of text, we'd invest in a dedicated XML editor like \nOxygen\n - there are \nplugins available and frameworks for doing historical transcription\n on this platform. There is a 30 day free trial license if you want to give it a try. But for now, Notepad++, Textwrangler, Komodo Edit, Sublime text, or any of a number of good text editors will do all that we need to do). Also, check out the \nTEI\n. Take 15 minutes and read throuhg \nWhat is XML and Why Should Humanists Care?\n by David Birnbaum. Annotate!\n\n\nIn this module we're going to do some other kinds of wrangling.\n\n\nExercises\n\n\nIn the exercises for this week we are going to focus on some bare-bones wrangling of data. First, we are going to do some activities and exercises to get in the right frame of mind. Then, we'll switch gears and we'll use \nregular expressions\n to search and extract information from the Diplomatic Correspondence of the Republic of Texas, which you'll find at the Internet Archive. If you're on a PC, download \nNotepad++\n - this is a souped-up version of the simple notepad application, and allows us to do very useful things indeed. If you're on a Mac, TextWrangler is probably already installed and is all you need. If you're working in Linux, you can use whatever text editor you're familiar with.\n\n\nWe'll conclude by using 'Open Refine' to tidy up the information we extracted from the Texan correspondence.\n\n\nThings you will learn in this module:\n\n\n\n\nthe power of regular expressions. 'Search' and 'Replace' in Word just won't cut it any more for you! (Another reason why you should write in Markdown in the first place and then convert to Word for the final typesetting.\n\n\nOpen Refine as a powerful engine for tidying up the messiness that is ocr'd text.\n\n\n\n\nWhat you need to do this week\n\n\n\n\nRespond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them \nhere\n\n\nDo the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there.  Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your github account - for more on that, see the exercises!).\n\n\nSubmit your work \nhere\n\n\n\n\nReadings\n\n\nSelect one of the articles behind the links above OR select one of the articles below to annotate.\n\n\nBlevins, \nMining and Mapping the Production of Space A View of the World from Houston\n\n\nBlevins, \nSpace, Nation, and the Triumph of Region: A View of the World from Houston\n\n\nIan Milligan on Imageplot\n and \nhere\n\n\nRyan Cordell, Qitjb-the-raven\n\n\nReading questions\n: On your blog, reflect on any data cleaning you've had to do in other classes. Why don't historians discuss this kind of work? What gets hidden, what gets lost, how is the ultimate argument \nweaker\n as a result? Or does it matter? Make reference (or link to) key annotations, whether by your or one of your peers, to support your points.", 
            "title": "Data is messy"
        }, 
        {
            "location": "/module-3/Wrangling Data/#wrangling-data-july-24-30", 
            "text": "", 
            "title": "Wrangling Data July 24 - 30"
        }, 
        {
            "location": "/module-3/Wrangling Data/#concepts", 
            "text": "In the previous module, we successfully grabbed  lots  of data from various online repositories. Some of it was already in well-structured tables; much of it was not. All of it was text though. Initially, it (or most of it) was just scanned images of documents. At some point,  object character recognition  was used to identify the black dots from the white dots in those images, to recognize the patterns that make up letters, numbers, and punctuation. There are commercial products that can do this (and we have some installed in the Underhill Research Room that you can use), and there are  free products that you can install  on your computer to do it yourself.  It all looks so neat and tidy. Ian Milligan discusses this  'illusionary order'  and its implications for historians:   In this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely \u2013 if ever \u2013 made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources.   Just as we saw with Ted Underwood's article on  theorizing search , these 'simple' steps in the research process are anything but. They are also profoundly theoretical in how they change what it is we can know. In archaeology, every step of the method, every stage in the process, has a profound impact on the stories we eventually tell about the past. Decisions we make destroy data, and create new data. Historians aren't used to thinking about these kinds of issues!  There are also  manual  ways of doing the same thing as OCR does - we call these things 'humans', and we organize their work through 'crowdsourcing'. We break the process up into wee manageable steps, and make these available over the net. Sometimes we gamify these steps, to make them more 'fun'. If several people all work on the same piece of text, the thinking is that errors will cancel each other out: a proper transcription will emerge from the work of the crowd.  While transcriptions might've provided the earliest examples of crowdsourcing research (but see also  The HeritageCrowd Project  and the subsequent  'How I Lost the Crowd' ), other tasks are now finding their way into the crowdsourced world - see the archaeological applications within the  MicroPasts  platform. These include things like 'masking' artefact photographs in order to develop 3d photogrammetric models.  But often, we don't have a whole crowd. We're just one person, alone, with a computer, at the archive. Or working with someone else's  digitized image that we found online . How do we wrangle that data? Let's start with  M. H. Beal's account of how she 'xml'd her way to data management'  and then consider a few more of the nuts and bolts of her work in  OA TEI-XML DH on the WWW; or, My Guide to Acronymic Success .  This kind of work is extraordinarily important!  You already had a taste of it in the TEI exercise in the last module. (Now, if we had a  seriously  big project where we were transcribing lots of text, we'd invest in a dedicated XML editor like  Oxygen  - there are  plugins available and frameworks for doing historical transcription  on this platform. There is a 30 day free trial license if you want to give it a try. But for now, Notepad++, Textwrangler, Komodo Edit, Sublime text, or any of a number of good text editors will do all that we need to do). Also, check out the  TEI . Take 15 minutes and read throuhg  What is XML and Why Should Humanists Care?  by David Birnbaum. Annotate!  In this module we're going to do some other kinds of wrangling.  Exercises  In the exercises for this week we are going to focus on some bare-bones wrangling of data. First, we are going to do some activities and exercises to get in the right frame of mind. Then, we'll switch gears and we'll use  regular expressions  to search and extract information from the Diplomatic Correspondence of the Republic of Texas, which you'll find at the Internet Archive. If you're on a PC, download  Notepad++  - this is a souped-up version of the simple notepad application, and allows us to do very useful things indeed. If you're on a Mac, TextWrangler is probably already installed and is all you need. If you're working in Linux, you can use whatever text editor you're familiar with.  We'll conclude by using 'Open Refine' to tidy up the information we extracted from the Texan correspondence.  Things you will learn in this module:   the power of regular expressions. 'Search' and 'Replace' in Word just won't cut it any more for you! (Another reason why you should write in Markdown in the first place and then convert to Word for the final typesetting.  Open Refine as a powerful engine for tidying up the messiness that is ocr'd text.", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-3/Wrangling Data/#what-you-need-to-do-this-week", 
            "text": "Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them  here  Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there.  Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your github account - for more on that, see the exercises!).  Submit your work  here", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-3/Wrangling Data/#readings", 
            "text": "Select one of the articles behind the links above OR select one of the articles below to annotate.  Blevins,  Mining and Mapping the Production of Space A View of the World from Houston  Blevins,  Space, Nation, and the Triumph of Region: A View of the World from Houston  Ian Milligan on Imageplot  and  here  Ryan Cordell, Qitjb-the-raven  Reading questions : On your blog, reflect on any data cleaning you've had to do in other classes. Why don't historians discuss this kind of work? What gets hidden, what gets lost, how is the ultimate argument  weaker  as a result? Or does it matter? Make reference (or link to) key annotations, whether by your or one of your peers, to support your points.", 
            "title": "Readings"
        }, 
        {
            "location": "/module-3/Exercises/", 
            "text": "Module 3: Exercises\n\n\nExercise 1\n\n\nWhen we have text that has been marked up, we can do interesting things with it. In the previous module, you saw that XSL can be used to add styles to your XML (which a browser can interpret and display). You can see how having those tags makes for easier searching for information as well, right? Sometimes things are not well marked up. Sometimes, it's all a real mess. In this exercise, we'll explore 'regular expressions', (aka 'Regex') or ways of asking the computer to search for \npatterns\n rather than matching exact text.\n\n\nThis exercise follows a tutorial written for \nThe Macroscope\n.(Another good reference is \nhttp://www.regular-expressions.info/\n).\n\n\nYou should have \nRegeXr\n, an interactive tutorial that shows us what various regular expressions can do, open in a browser window somewhere so you can test your regular expressions out \nfirst\n before applying them to your data!  We will use the power of regular expressions to search for particular patterns in a published volume of the correspondence of the Republic of Texas. We'll use regex to massage the information into a format that we can then use to generate a social network of letter writers over time. (You might want to give this tutorial a try, too: \nUnderstanding Regular Expressions\n).\n\n\nWhat if you had \nlots\n of documents that you needed to clean up? One way of doing it would be to write a python program that applied your regex automatically, across all the files in a folder. \nOptional advanced exercise\n: \nCleaning OCR'd Text with Regular Expressions\n\n\n\n\nStart with \na gentle introduction to regex\n and then begin the \nregex exercise\n.\n\n\n\n\nremember to copy your history file and any other information/observations/thoughts to your Github repo\n\n\nExercise 2\n\n\nOpen Refine\n is the final tool we'll explore in this module. This engine allows us to clean up our messy data. We will feed it the results from Exercise 2 in order to consolidate individuals (ie, 'Shawn' and 'S4awn' are probably the same person, so Open Refine will consolidate that information for us). This exercise also follows a tutorial originally written for \nThe Macroscope\n.\n\n\nOpen Refine does not run in DHBox, so use the filemanager in DHBox to move your \ncleaned-correspondence\n file to somewhere safe on your computer.\n\n\nHere are the instructions\n.\n\n\nFor more advanced usage of Open Refine, as an optional exercise you can also try \nThe Programming Historian's Tutorial on Open Refine\n.\n\n\nremember to copy your notes and any other information/observations/thoughts to your Github repo\n\n\nYour final project\n\n\nYou can use what you've been learning here to do some clean-up on the Shawville Equity files (or a subset of them, of course, as suits your interests - you have explored them, haven't you?). Google for 'sed patterns' and see if you can combine what you find with what you've learned in this module in order to clean up the text. For instance, a common error in OCR is to confuse the letter i with the letter l and the numeral 1. Shawville becomes Shawvllle. You could use grep to see if that error is present, and then sed to correct it. The file names are long, and there are several hundred; you might give this kind of thing a try too:\n\n\nfind . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\;\n\n\nThis command finds all files in a folder and creates a backup for each one in turn before searching for 'foo' and replacing it with 'bar'.\n\n\nThis command: \ngrep \"[b-df-hj-np-tv-xz]\\{5\\}\" filename\n will find all instances of strings of five consonants or more, which can be useful to give you and idea of what kinds of sed patterns to write.\n\n\nMaybe, if you inspect the pdfs and the txt together, you can figure out patterns that set off interesting things in say the classified ads or the editorials - and then write some grep and sed to create new files with just that information. Then you could use Open Refine to further clean things up. Maybe the messiness of the data is \nexactly the point\n (and \nhere, too\n) you want to explore. Nevertheless:\n\n\ncleaning data is 80% of the work in digital history\n.\n\n\nremember to copy your notes and any other information/observations/thoughts to your Github repo\n\n\nGoing Further\n\n\n\n\n\n\nSee some of the suggestions at the end of the \nOpen Refine exercise\n\n\n\n\n\n\nCounting and Mining Data with Unix\n\n\n\n\n\n\nThe \nStanford Named Entity Recognizer\n is a program that enables you to automatically tag words in your corpus according to whether or not they are place names, individuals, and so on. The output can then be subsequently extracted and visualized, which you'll learn in Module 4. \nWorking with the Stanford NER\n.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-3/Exercises/#module-3-exercises", 
            "text": "", 
            "title": "Module 3: Exercises"
        }, 
        {
            "location": "/module-3/Exercises/#exercise-1", 
            "text": "When we have text that has been marked up, we can do interesting things with it. In the previous module, you saw that XSL can be used to add styles to your XML (which a browser can interpret and display). You can see how having those tags makes for easier searching for information as well, right? Sometimes things are not well marked up. Sometimes, it's all a real mess. In this exercise, we'll explore 'regular expressions', (aka 'Regex') or ways of asking the computer to search for  patterns  rather than matching exact text.  This exercise follows a tutorial written for  The Macroscope .(Another good reference is  http://www.regular-expressions.info/ ).  You should have  RegeXr , an interactive tutorial that shows us what various regular expressions can do, open in a browser window somewhere so you can test your regular expressions out  first  before applying them to your data!  We will use the power of regular expressions to search for particular patterns in a published volume of the correspondence of the Republic of Texas. We'll use regex to massage the information into a format that we can then use to generate a social network of letter writers over time. (You might want to give this tutorial a try, too:  Understanding Regular Expressions ).  What if you had  lots  of documents that you needed to clean up? One way of doing it would be to write a python program that applied your regex automatically, across all the files in a folder.  Optional advanced exercise :  Cleaning OCR'd Text with Regular Expressions   Start with  a gentle introduction to regex  and then begin the  regex exercise .   remember to copy your history file and any other information/observations/thoughts to your Github repo", 
            "title": "Exercise 1"
        }, 
        {
            "location": "/module-3/Exercises/#exercise-2", 
            "text": "Open Refine  is the final tool we'll explore in this module. This engine allows us to clean up our messy data. We will feed it the results from Exercise 2 in order to consolidate individuals (ie, 'Shawn' and 'S4awn' are probably the same person, so Open Refine will consolidate that information for us). This exercise also follows a tutorial originally written for  The Macroscope .  Open Refine does not run in DHBox, so use the filemanager in DHBox to move your  cleaned-correspondence  file to somewhere safe on your computer.  Here are the instructions .  For more advanced usage of Open Refine, as an optional exercise you can also try  The Programming Historian's Tutorial on Open Refine .  remember to copy your notes and any other information/observations/thoughts to your Github repo", 
            "title": "Exercise 2"
        }, 
        {
            "location": "/module-3/Exercises/#your-final-project", 
            "text": "You can use what you've been learning here to do some clean-up on the Shawville Equity files (or a subset of them, of course, as suits your interests - you have explored them, haven't you?). Google for 'sed patterns' and see if you can combine what you find with what you've learned in this module in order to clean up the text. For instance, a common error in OCR is to confuse the letter i with the letter l and the numeral 1. Shawville becomes Shawvllle. You could use grep to see if that error is present, and then sed to correct it. The file names are long, and there are several hundred; you might give this kind of thing a try too:  find . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\;  This command finds all files in a folder and creates a backup for each one in turn before searching for 'foo' and replacing it with 'bar'.  This command:  grep \"[b-df-hj-np-tv-xz]\\{5\\}\" filename  will find all instances of strings of five consonants or more, which can be useful to give you and idea of what kinds of sed patterns to write.  Maybe, if you inspect the pdfs and the txt together, you can figure out patterns that set off interesting things in say the classified ads or the editorials - and then write some grep and sed to create new files with just that information. Then you could use Open Refine to further clean things up. Maybe the messiness of the data is  exactly the point  (and  here, too ) you want to explore. Nevertheless:  cleaning data is 80% of the work in digital history .  remember to copy your notes and any other information/observations/thoughts to your Github repo", 
            "title": "Your final project"
        }, 
        {
            "location": "/module-3/Exercises/#going-further", 
            "text": "See some of the suggestions at the end of the  Open Refine exercise    Counting and Mining Data with Unix    The  Stanford Named Entity Recognizer  is a program that enables you to automatically tag words in your corpus according to whether or not they are place names, individuals, and so on. The output can then be subsequently extracted and visualized, which you'll learn in Module 4.  Working with the Stanford NER .", 
            "title": "Going Further"
        }, 
        {
            "location": "/module-4/Seeing Patterns/", 
            "text": "Seeing Patterns July 31 - Aug 6\n\n\nConcepts\n\n\nIn the previous module, we collected and cleaned vast amounts of historical data. Let's see what patterns emerge. Part exploration, part reflection, and part argument, these two stages are not linear but feed into one another. Exploration and preliminary visualizations are often about finding the holes. Sometimes, a quick visualization helps you understand what is missing, or what is a glitch in your method, or what is just plain mistaken in your assumptions. I once spent quite a lot of time staring at a network graph before I realized that I'd neglected to import the entire file - I was trying to interpret just a subset of it! Back to my wrangling I went, and in the process of cleaning that data, I realized there were patterns that would be better visualized as simple line plots. I visualized these - and found other errors. Back to wrangling. In the end, a combination of network graph and simple line plots allowed me to identify a story about influence and control of landholding (I was working with archaeological data).\n\n\nIn this module, let us begin by looking at a couple of pieces that explore this cyclical dynamic between data wrangling and exploration. In our Slack space for this module or in your notebooks, I want you to explicitly discuss the work of the following scholars. Make explicit connections with things you've read/explored/studied/developed in other history classes!\n\n\nIn essence: what would your last essay last term have looked like, if you'd been thinking along these lines?\n\n\nBegin by taking a look at the \nMapping Texts\n project. Look at the work of Michelle Moravec who is using \ncorpus linguistics tools to understand the history of women's suffrage\n (and also \nhere\n). Listen to Micki Kaufmann on \nQuantifying Kissinger\n. (Her \nmethods are detailed here\n.) Talk about \nnetwork analysis\n. Keep an \neye on Paul Revere\n. Talk about \ntopic modeling\n (and also \nhere\n). Talk about \nprinciples of visualization\n. You should probably talk about \nEdward Tufte\n, too.\n\n\nTalk. What could history be like if more of our materials went through these mills?\n\n\nThings you will learn in this module\n\n\n\n\nimporting, querying, and visualizing networks with \nGephi\n \n- igraph, R.\n\n\ntopic modeling with the GUI topic modeling tool, and MALLET at the command line, and in R\n\n\nsimple maps with CartoDB (which may include georectifying and displaying historical maps as base layers)\n\n\ncorpus linguistics with AntConc\n\n\nTF-IDF with Overview\n\n\nquick visualizations using \nRAW\n\n\n\n\nWe will be busy in this module. \nDo not be afraid\n to ask myself, your peers, or the wider DH community for help and advice! It's the only way we grow.\n\n\nAnd finally...\n\n\nJust because there is a package, or a routine, or an approach to doing \nx\n with your data \ndoes not mean\n that you park your critical apparatus at the door. Consider: Matthew Jockers has done some amazing work putting together \na package for the R statistical language that helps one analyze plot arcs in thousands of novels at once\n. He describes how it works on his \nblog\n. Annie Swafford \nexplores this package\n in a comprehensive blog post. She highlights important issues in the way the package deals with the complexity of the world, and how that might have an impact on results and conclusions drawn from using the package. The interplay of the work that Jockers has done, and the dialogue that Swafford has started with Jockers, is an important feature (and perhaps, a defining feature) of digital humanities scholarship. Jocker builds; and Swafford approaches the code from a humanistic perspective; and in the dialectic of their exchange, the code and our ability to understand the code's limitations and potentials will improve. And so we understand the world better.\n\n\nWhen you choose your exploratory method, you need to consider that the method has its own agenda!\n\n\nWhat you need to do this week\n\n\n\n\nRespond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them \nhere\n\n\nDo the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. The \nexercises\n in this module will pick up where we left off, with the Texan Correspondence. You'll represent the exchange of letters as a network. We'll try to extract place names from it. We'll topic model the letters themselves. We'll explore various ways of visualizing those results. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your github account - for more on that, see the exercises!).\n\n\nSubmit your work \nhere\n\n\n\n\nReadings\n\n\nSelect one of the articles behind the links above (and/or in the exercises) to annotate, asking yourself, 'who benefits from this? who is hurt from this?'. Make an entry in your blog on this theme.", 
            "title": "Seeing Patterns"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#seeing-patterns-july-31-aug-6", 
            "text": "", 
            "title": "Seeing Patterns July 31 - Aug 6"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#concepts", 
            "text": "In the previous module, we collected and cleaned vast amounts of historical data. Let's see what patterns emerge. Part exploration, part reflection, and part argument, these two stages are not linear but feed into one another. Exploration and preliminary visualizations are often about finding the holes. Sometimes, a quick visualization helps you understand what is missing, or what is a glitch in your method, or what is just plain mistaken in your assumptions. I once spent quite a lot of time staring at a network graph before I realized that I'd neglected to import the entire file - I was trying to interpret just a subset of it! Back to my wrangling I went, and in the process of cleaning that data, I realized there were patterns that would be better visualized as simple line plots. I visualized these - and found other errors. Back to wrangling. In the end, a combination of network graph and simple line plots allowed me to identify a story about influence and control of landholding (I was working with archaeological data).  In this module, let us begin by looking at a couple of pieces that explore this cyclical dynamic between data wrangling and exploration. In our Slack space for this module or in your notebooks, I want you to explicitly discuss the work of the following scholars. Make explicit connections with things you've read/explored/studied/developed in other history classes!  In essence: what would your last essay last term have looked like, if you'd been thinking along these lines?  Begin by taking a look at the  Mapping Texts  project. Look at the work of Michelle Moravec who is using  corpus linguistics tools to understand the history of women's suffrage  (and also  here ). Listen to Micki Kaufmann on  Quantifying Kissinger . (Her  methods are detailed here .) Talk about  network analysis . Keep an  eye on Paul Revere . Talk about  topic modeling  (and also  here ). Talk about  principles of visualization . You should probably talk about  Edward Tufte , too.  Talk. What could history be like if more of our materials went through these mills?  Things you will learn in this module   importing, querying, and visualizing networks with  Gephi   - igraph, R.  topic modeling with the GUI topic modeling tool, and MALLET at the command line, and in R  simple maps with CartoDB (which may include georectifying and displaying historical maps as base layers)  corpus linguistics with AntConc  TF-IDF with Overview  quick visualizations using  RAW   We will be busy in this module.  Do not be afraid  to ask myself, your peers, or the wider DH community for help and advice! It's the only way we grow.  And finally...  Just because there is a package, or a routine, or an approach to doing  x  with your data  does not mean  that you park your critical apparatus at the door. Consider: Matthew Jockers has done some amazing work putting together  a package for the R statistical language that helps one analyze plot arcs in thousands of novels at once . He describes how it works on his  blog . Annie Swafford  explores this package  in a comprehensive blog post. She highlights important issues in the way the package deals with the complexity of the world, and how that might have an impact on results and conclusions drawn from using the package. The interplay of the work that Jockers has done, and the dialogue that Swafford has started with Jockers, is an important feature (and perhaps, a defining feature) of digital humanities scholarship. Jocker builds; and Swafford approaches the code from a humanistic perspective; and in the dialectic of their exchange, the code and our ability to understand the code's limitations and potentials will improve. And so we understand the world better.  When you choose your exploratory method, you need to consider that the method has its own agenda!", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#what-you-need-to-do-this-week", 
            "text": "Respond to the readings and the reading questions through annotation (taking care to respond to others' annotations as well) - see the instructions below.  Remember to tag your annotations with 'hist3814o' so that we can find them  here  Do the exercises for this module, pushing yourself as far as you can. Annotate the instructions where they might be unclear or confusing; see if others have annotated them as well, and respond to them with help if you can. Keep an eye on our Slack channel - you can always offer help or seek out help there. The  exercises  in this module will pick up where we left off, with the Texan Correspondence. You'll represent the exchange of letters as a network. We'll try to extract place names from it. We'll topic model the letters themselves. We'll explore various ways of visualizing those results. Write a blog post describing what happened as you went through the exercises (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes you upload to your github account - for more on that, see the exercises!).  Submit your work  here", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#readings", 
            "text": "Select one of the articles behind the links above (and/or in the exercises) to annotate, asking yourself, 'who benefits from this? who is hurt from this?'. Make an entry in your blog on this theme.", 
            "title": "Readings"
        }, 
        {
            "location": "/module-4/Exercises/", 
            "text": "Module 4 Exercises\n\n\nThere are \nmany\n different tools and approaches\n you could use to visualize your data, both as a preliminary pass to spot the holes and also for more formal analysis. In which case, for this module, I would like you to select \ntwo\n of these exercises which seem most germane for your final project.\n\n\nYou are welcome to work through more of them, of course, but I want the exercises to move your own research forward. Some of these I wrote; some are adapted from \nThe Macroscope\n; others are adapted or used holus-bolus from scholars like \nMiriam Posner\n, \nFred Gibbs\n, and \nHeather Froehlich\n (and I'm grateful that they shared their materials!). Finally, you are welcome to explore the lessons and tutorials at \nThe Programming Historian\n if they seem appropriate to what you want to do for your project.\n\n\nBut what if I haven't any idea of what to do for the final project?\n Then read through the various tutorials for inspiration. Find something that strikes you as interesting, and then talk to me about how you might employ the ideas or concepts with regard to the Equity data.\n\n\nThings to install?\n\n\nMany of these involve having to install more software on your own machine. In those exercises that involve using R and RStudio, you are welcome to install RStudio on your own machine OR to use it in DHBox. Please read \nthis quick introduction to R and Rstudio carefully\n. If you decide to install R and RStudio on your own machine, I would suggest you read the introductory bits from Lincoln Mullen's book-in-progress, \nComputational Historical Thinking\n, especially the 'setup' part under 'getting started' (pay attention to the bit on installing packages and dependencies). If you spot any exercises in Mullen's book that seem relevant to your project, you may do those as an alternative to the ones here. \nAlternatively\n, go to \nSwirl\n and \nlearn the basics of R within R\n. \nDHNow\n links to a new \nBasic Text Mining in R\n tutorial which is worth checking out as well. \nnb\n the instructions for DHBox versions of the R exercises are different because the version of R included in DHBox is a bit older than what you can install on your own machine. It is always very important to record in your own notebooks what version of R you used for your analysis, what version of any R packages you installed and used, and so on.\n\n\nIn the table below I've gathered the exercises together under the headings of 'text', 'networks', 'maps', and 'charts'. I've also added some entries that I am categorizing under 'art' The first is a series of exercises on \nthe sonification of data\n and the second is a guide \nto making twitterbots\n; the third is about glitching digital imagery. These approaches can provide surprising and novel insights into history, as they move from representing history digitally to \nperforming\n it. See for instance the final project in an undergraduate digital history class at the University of Saskatchewan by \nDaniel Ruten\n where he translated simple wordclouds of a WWI diary into a profound auditory performance. I would be very interested indeed to see if any final projects in HIST3814o gave sonification or twitterbots or glitch a try.\n\n\n\n\n\n\n\n\nTexts\n\n\nNetworks\n\n\nMaps\n\n\nCharts\n\n\nArt\n\n\n\n\n\n\n\n\n\n\nTopic Modeling Tool\n\n\nNetwork analysis in Gephi\n\n\nSimple mapping \n georectifying\n\n\nQuick charts using RAW\n\n\nSonification\n\n\n\n\n\n\nTopic Modeling in R\n\n\nConverting 2-mode to 1-mode\n\n\nQGIS (tutorials by Fred Gibbs)\n\n\n\n\nTwitterbots\n\n\n\n\n\n\nText Analysis with OverviewProject\n\n\nNetwork Analysis in R\n\n\nGeoparsing with Python\n\n\n\n\nGlitching Photos\n\n\n\n\n\n\nCorpus Linguistics with AntConc\n\n\nNetwork Analysis in Cytoscape\n\n\nPalladio with Posner\n\n\n\n\n\n\n\n\n\n\nText Analysis with Voyant\n\n\nChoose your own adventure\n\n\nLeaflet.js Maps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\nNetwork Visualization\n\n\nThis exercise uses the open source programme \nGephi\n which you install on your own computer. If you'd rather not install anything, please see \nNetwork Analysis in R\n instead.\n\n\nRecall that the index of the collected letters of the Republic of Texas was just a list of letters from so-and-so to so-and-so. We haven't looked at the content of those letters, but the shape of network - the meta data of that correspondence - can be revealing (remember \nPaul Revere!\n) When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. I would recommend that you also take a long look at Scott Weingart's series, \nNetworks Demystified\n. Finally, \nheed our warning\n.\n\n\nIn this exercise, you will transform your Texan Correspondence data into a network, which you will then visualize with the open source programme \nGephi\n The detailed instructions are \nhere\n.\n\n\n\n\nExercise 2\n\n\nTopic Modeling Tool\n\n\nIn exercise 2, you will use the 'Topic Modeling Tool' to create a simple topic model and a webpage that allows you to browse the results.\n\n\n\n\nDownload the \ntool\n.\n\n\nMake sure you have some content on your own machine; the Colonial Newspaper Database is a handy corpus. (Created by Melodee Beals, it's a series of late 18th, early 19th century cleanly transcribed newspaper articles from Scotland and Northern England; You can grab my copy from \nhere\n). Or perhaps you might move your copy of the Shawville equity out of DHBox onto your computer. At the command prompt in DHBox, type \n$ ls\n to make sure you can see your Equity folder (ie, you can't zip a folder from the command line if you are \nin\n that folder, so \ncd\n out of it if necessary). Assuming your files are in \nequityfolder\n, zip the folder up with this command, \n$ zip -r equityfiles.zip equityfolder\n. Then use the filemanager to download the zip file. Unzip the folder on your machine.\n\n\nDouble-click on the file you downloaded in step 1. This will open a java-based graphical user interface with one of the most common topic-modeling approaches, 'Latent Dirichlet Allocation'.\n\n\nSet the input to be the Colonial Newspaper Database OR the Shawville Equity folder.\n\n\nSet the output to be somewhere neat and tidy on your computer.\n\n\nSet the number of topics you'd like to model.\n\n\nClick 'train topics' to run the algorithm.\n\n\nWhen it finishes, go to the folder you selected for output, and find the file 'all_topics.html' in the 'output_html' folder. Click on that, and you now have a browser-based way of navigating your topics and documents. In the output_csv folder created, you will find the same information as csv, which you could then input into a spreadsheet for other kinds of visualizations (which we'll talk about in class.)\n\n\n\n\nMake a note in your open notebook about your process and your observations. How does reading this material in this way change/challenge/or focus your understanding of the material?\n\n\n\n\nexercise 3\n\n\nTopic Modeling in R\n\n\nExercise 2 was quite a simple way to do topic modeling. In this exercise, we are going to use a package for the R statistical language called 'Mallet' to do our topic modeling. One way isn't necessarily better than the other, although doing our analysis within R allows the potential for extending the analysis or combining it with other data. First, read \nthis introduction to R\n so what follows isn't a complete shock!\n\n\n\n\nguidance for doing this in RStudio \nin the DHBox\n\n\nguidance for doing this in RStudio \ninstalled on your own computer\n\n\n\n\n\n\nexercise 4\n\n\nText Analysis with Overview\n\n\nIn exercise 4, we're going to look at the Colonial Newspaper Database again, but this time using a tool called 'Overview'. Overview uses a different approach that the topic models we've been discussing. In essence, it looks at word frequencies and their distributions within a document, and within a corpus, to organize the documents into folders of progressively similar word use.\n\n\nYou can download Overview to run on your own machine, but for our purposes, the hosted version at \nhttps://www.overviewdocs.com/\n is sufficient. Go to that page, watch the video, create an account, and then log in. (More help about how Overview works \nmay be found on their blog\n, including helpful videos.)\n\n\nOnce you're inside, click 'import from a CSV file', and upload the CND.csv (which you can download and save to your own machine from \nhere\n \n- right-click and save as. On the 'UPLOAD A CSV FILE' page in Overview click 'browse' and select the CND.csv. It will give you a preview. There are a number of options here - you can tell Overview which words to ignore, and which words to give added importance to. What words will you select? Make a note in your notebook. Then hit 'upload'.\n\n\nA new page appears, called 'YOUR DOCUMENT SETS'. Click on the one you just uploaded. A file folder tree showing documents of progressively greater similarity will open; on the right hand side will be the list of documents within each box (the box in question will be greyed out when you click on it, so you know where you are). You can search for words in your document, and Overview will tell you where they are; you can tag documents that you find interesting. The Overview system allows you to jump between a distant, macroscopic view and a close, document level view. Jump back and forth, see what you can find. For suggestions about how to use Overview effectively, try \ntheir blog\n. Make notes about what you observe in your notebook. Also, you can export your tagged document set from Overview, so that you could visualize the patterns of tagging in a spreadsheet (for instance).\n\n\nGoing further\n Do you see how you could upload your documents that you collected during Module 2?\n\n\n\n\nexercise 5\n\n\nCorpus Linguistics with AntConc\n\n\nHeather Froelich has put together an excellent step-by-step with using AntConc for exploring textual patterns within, and across, corpora of texts. Work your way through her \ntutorial\n\n\nCan you get our example materials (from the Colonial Newspaper Database) into AntConc? \nThis might help you\n to split the csv into individual txt files. Alternatively, do you have any materials of your own, already collected? Feed them into AntConc. What patterns do you see? What if you compare your materials against other corpora of texts?\n\n\nFYI, \nhere is a collection of corpora that you can explore\n\n\n\n\nexercise 6\n\n\nText Analysis with Voyant\n\n\nIn module 2 if you recall, we worked through how to transform XML using stylesheets. Melodee Beals used a \nstylesheet \n to transform her database into a series of individual txt files. In the exercises above, a transformer was used to make the database into a single CSV file. In this exercise, we are going to use \nVoyant Tools\n to visualize patterns in word use in the database. Voyant can read either a CSV \nor\n text files. The advantage of uploading a folder of text files is that, if the files are in chronological order, Voyant's default visualizations will also be arranged in chronological order and thus we can see change over time.\n\n\nGo to \nhttp://voyant-tools.org\n. Paste the URL to the csv of the CND database: \nhttps://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv\n .\n\n\nNow, open a new browser window, and go here \nhttp://voyant-tools.org/?corpus=colonial-newspapers\nstopList=stop.en.taporware.txt\n\n\nDo you see the difference? In the latter window, the individual articles have been uploaded individually, and thus are treated as individual documents in chronological order.\n\n\nExplore the corpus, comparing terms over time, looking at keywords in context, and using the RezoViz tool to create a graph where people, places, and organizations that appear in the same documents (and across documents) are connected (you can find 'rezoviz' under the cogwheel icon at the top right of the panel). Google these terms and tools for what they mean and how others have used them. You can embed any of the tools in your blogs by using the 'save' icon and getting the iframe or embed code.  You can apply 'stopwords' by clicking on the cogwheel in any of the different tools, and selecting stopwords. Apply the stopwords globally, and you'll only have to do this once! What patterns do you see? What do different tools highlight? Which ones are useful? What patterns do you see that strike you as interesting? Note this all down.\n\n\nGoing further\n Upload materials you collected in module 2 and explore them.\n\n\n\n\nexercise 7\n\n\nQuick Charts Using RAW\n\n\nA quick chart can be a handy thing to have. Google spreadsheets, Microsoft Excel, and a host of other programs can make excellent charts quickly with their wizard functions. Never hesitate to turn to these. However, they are not always good with non-numeric data. In module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like \nthis\n. Can we do a quick visualization of this information? One useful tool is \nRAW\n. Open that in a new window. Copy the table of data of places mentioned in the Texan correspondence, and paste it into the data input box at the top of the RAW screen.\n\n\noh noes an error!\n\n\nA quick data munge\n\n\nYou should get an error message, to the effect that you need to check 'line 2'. What's gone wrong? RAW has checked the number of values you have in that row, and compared it to the number of columns in row 1 (which contains all the column names). It sees that the two don't match. What we need to do is add a default null value in those cells. So, go to \nGoogle Sheets\n, click the 'go to google sheets' button, and then click on the big green plus sign to start a new sheet. Paste the following into the top-left cell (cell A1):\n\n\n=IMPORTDATA(\"https://raw.githubusercontent.com/hist3907b-winter2015/module4-holes/master/texas.csv\")\n\n\nPretty neat, eh? Now, here's the thing: even though your sheet \nlooks\n like it is filled with information, it's not (at least, as far as the script we are about to run is concerned). That is to say, the sheet itself only has one cell of data, and that one cell is grabbing info from elsewhere on the web and dynamically filling the sheet. The script we're going to run works only on static values (more or less).\n\n\nSo, place your cursor in cell B1. On a Mac, hit \nshift+cmnd+downarrow\n. On a Windows machine, hit \nshift+ctrl+downarrow\n. Then on Mac \nshit+cmnd+rightarrow\n, on Windows \nshitf+crtl+rightarrow\n. Then copy all of that data (\ncmnd+c\n or \nctrl+c\n). Then, under 'Edit' select 'paste special' -\n 'paste VALUES only'.\n\n\nThe formula you put in cell A1 now says \n#REF!\n. You can delete this now. This mucking about is necessary so that the add on script we are about to run will work.\n\n\nWe now need to fill those empty values. In the tool bar, click \nadd ons\n -\n \nget add ons\n. Search for \nblanks\n. You want to add \nBlank Detector\n.\n\n\nNow, click somewhere in your data. On Mac, hit \ncmnd+a\n. On Windows, hit \nctrl+a\n. This highlights all of your data. Click \nAdd ons\n -\n \nblank detector\n -\n \ndetect cells\n. A dialogue panel will open on the right hand side of your screen. Click the button beside \nset value\n and type in \nnull\n. Hit \nrun\n. All of the blank cells will fill with the word \nnull\n. Delete column A (which formerly had record numbers, but is now just filled with the word \nnull\n. We don't need it). \nIf you get the error, 'run exceeded maximum time'\n just hit the run button again. This script might take a few minutes.\n\n\nYou can now copy and paste your table of data into the data input box in RAW, and you should get the green thumbs up saying x records have been successfully parsed!\n\n\nPlaying with RAW\n\n\nRAW takes your data, and depending on your choices, passes it into chart templates built on the d3.js code library. D3.js is a powerful library for making all sorts of charts (including interactive ones). If this sort of thing interests you, you can follow the tutorials in \nElijah Meeks' excellent new book\n.\n\n\nWith your data pasted in, you can now experiment with a number of different visualizations that are all built on the d3.js code library.  Try the \u2018alluvial\u2019 diagram.  Pick place1 and place2 as your dimensions - you click and drag the green boxes under 'map your data' into the 'steps' box. Leave the 'size' box empty. Under 'customize your visualization' you can click inside the 'width' box to make the diagram wider and more legible.\n\n\nDoes anything jump out? Try place3 and place 4. Try place1, place2, place3, and place4 in a single alluvial diagram. When we look at the original letters, we see that the writer often identified the town in which he was writing, and the town of the addressee. Why choose the third and fourth places? Perhaps it makes sense, for a given research question, to assume that with the pleasantries out of the way the writers will discuss the places important to their message. Experiment! This is one of the joys of working with data, experimenting to see how you can deform your materials to see them in a new light.\n\n\nYou can export your visualization under the 'download' box at the bottom of the RAW page - your choices are as a simple raster image (png), a vector image (svg) or a data representation (json).\n\n\n\n\nexercise 8\n\n\nSimple Mapping and Georectifying\n\n\nIn this exercise, you will find a historical map online, upload a copy to a mapwarper service, georectify it, and then display the map online, via a hosted service like CartoDB, and also through a map you will build yourself using leaflet.js. Finally, we will also convert csv to geojson using http://togeojson.com/, and we'll map that as a github gist. We'll also grab a geojson file hosted on github gist and import it into cartodb.\n\n\nGeorectifying\n\n\nGeorectifying is the process of taking an image (whether it is of a historical map, chart, airphoto, or whatever) and manipulating its geometry so that it matches a geographic projection. Think of it like this: you take your handdrawn map, and use pushpins to pin down known locations on your map to a globe. As you pin, your image stretches and warps. Traditionally, this has not been an easy thing to do, if you are new to GIS. In recent years, the curve has flattened significantly. In this exercise, we'll grab an image, upload it to the Harvard Library MapWarper service, and then export it as a tileset which can be used in other mapping programs.\n\n\n\n\nGet a historical map. I like the Fire Insurance plans from the \nGatineau Valley Historical Society\n; I'm sure you can find others to suit your interests.\n\n\nRight-click, save as.... grab a copy. Save it somewhere handy.\n\n\nGo to \nHarvard World MapWarp\n and sign up for an account. Then login.\n\n\nGo to the upload screen: \n \n\n\nFill in as much of the metadata as you can. Then select your map from your computer, and upload it.\n\n\nOn the next page, click 'rectify'. \n \n\n\nPan and zoom both maps until you're sure you're looking at the same area in both. Double click in a map, select the pencil icon, and click on a point (location) you are sure you can match in the other window. Then click on the other map window, select the pencil, and then click on the same point. The 'add control point' button below and between both maps will light up. Click on this to confirm that this is a control point you want. Do this at least three times; the more times you can do it, the better the map warp.\n\n\nHaving selected your control points, click on 'warp image'.\n\n\nYou can now click on the 'export' panel, and get the URL for your georectified image in a few different formats. If you clicked on the KML option, a google map window will open \nlike so\n. For many webmapping applications, the Tiles (Google/OSM scheme): Tiles Based URL is what you want. You'll get a URL like this: \nhttp://warp.worldmap.harvard.edu/maps/tile/4152/z/x/y.png\n   Save that info. You'll need it later.\n\n\n\n\nYou have now georectified a map. Let's use that map as a base layer in \nPalladio\n\n\nWe need some place data for Palladio. Here's what I'm using \n \n \n Note how I've formatted this data. I'll be copying and pasting it into Palladio. (For more on how to input geographic data into Palladio, see \nthis tutorial\n). Basically, you want something like this:\n\n\n\n\n\n\n\n\n\n\nPlace\n\n\nCoordinates\n\n\n\n\n\n\n\n\n\n\n\n\nMexico\n\n\n23.634501,-102.552784\n\n\n\n\n\n\n\n\nCalifornia\n\n\n36.778261,-119.4179324\n\n\n\n\n\n\n\n\nBrazos\n\n\n32.661389,-98.121667\n\n\n\n\n\n\n\n\netc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude.\n\n\n\n\nGo to \nPalladio\n. Hit 'start' then 'upload spreadsheet or csv'. In the box, paste in your data. \nYou can progress to the next step without having any real data: just paste or type something in - see the video below.\n Obviously, you won't have any points on your map, but if you were having trouble with that step, this allows you to bypass it to continue on with this tutorial.\n\n\nClick on 'map'. Under 'places', select 'coordinates'. Then, click 'add new layer'. In the popup, beside 'Choose one of Palladio default layers or create a new one.', select 'custom'. This is where you're going to paste it that tiles based URL from the map warper. Paste it in, but \nreplace\n the \n/z/x/y\n part with \n{z}/{x}/{y}\n. Click add.\n\n\n\n\nHere is a video walk through; places where you might have got into trouble include getting past the initial data entry box on Palladio, and finding where exactly to past in your georectified map url.\n\n\n\n\n\nCongratulations! You've georectified a map, and used it as a base layer for a visualization of some point data. Here are some \nnotes on using a georectified map with the CartoDB service\n.\n\n\n\n\n\n\nexercise 9\n\n\nNetwork Analysis in R\n\n\nEarlier, we took the index from the Texan Correspondence, a list of letters from so-and-so to so-and-so. When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place.Before you go any further, make sure you also take a long look at Scott Weingart's series, \nNetworks Demystified\n. Finally, \nheed our warning\n.\n\n\nThis exercise uses the R language to do our analysis, which in DHBox we access via R Studio, a programming environment. Please read \nthis introduction to R\n and then progress to the \nexercise\n.\n\n\nexercise 10 QGIS\n\n\nQGIS\n\n\nThere are many excellent tutorials around concerning how to get started with GIS. Our own library, in the \nMADGIC centre\n has tremendous resources and I would encourage you to speak with the map librarians before embarking on any \nserious\n mapping projects. In the short term, the historian \nFred Gibbs\n has an excellent series on using the open source GIS platform \nQGIS\n to make and map historical data.\n\n\nFor this exercise, I would recommend you try Gibbs' first tutorial,\n\n\n'Making a map with QGIS'\n\n\n...and then, try georectifying a historical map and adding it to your GIS:\n\n\n'Using Historical maps with qgis'\n\n\nGoing Further\n\n\nThere are many tutorials at \nThe Programming Historian\n that are appropriate here. Try some under the 'data manipulation' or 'distant reading' headings.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-4/Exercises/#module-4-exercises", 
            "text": "There are  many  different tools and approaches  you could use to visualize your data, both as a preliminary pass to spot the holes and also for more formal analysis. In which case, for this module, I would like you to select  two  of these exercises which seem most germane for your final project.  You are welcome to work through more of them, of course, but I want the exercises to move your own research forward. Some of these I wrote; some are adapted from  The Macroscope ; others are adapted or used holus-bolus from scholars like  Miriam Posner ,  Fred Gibbs , and  Heather Froehlich  (and I'm grateful that they shared their materials!). Finally, you are welcome to explore the lessons and tutorials at  The Programming Historian  if they seem appropriate to what you want to do for your project.  But what if I haven't any idea of what to do for the final project?  Then read through the various tutorials for inspiration. Find something that strikes you as interesting, and then talk to me about how you might employ the ideas or concepts with regard to the Equity data.", 
            "title": "Module 4 Exercises"
        }, 
        {
            "location": "/module-4/Exercises/#things-to-install", 
            "text": "Many of these involve having to install more software on your own machine. In those exercises that involve using R and RStudio, you are welcome to install RStudio on your own machine OR to use it in DHBox. Please read  this quick introduction to R and Rstudio carefully . If you decide to install R and RStudio on your own machine, I would suggest you read the introductory bits from Lincoln Mullen's book-in-progress,  Computational Historical Thinking , especially the 'setup' part under 'getting started' (pay attention to the bit on installing packages and dependencies). If you spot any exercises in Mullen's book that seem relevant to your project, you may do those as an alternative to the ones here.  Alternatively , go to  Swirl  and  learn the basics of R within R .  DHNow  links to a new  Basic Text Mining in R  tutorial which is worth checking out as well.  nb  the instructions for DHBox versions of the R exercises are different because the version of R included in DHBox is a bit older than what you can install on your own machine. It is always very important to record in your own notebooks what version of R you used for your analysis, what version of any R packages you installed and used, and so on.  In the table below I've gathered the exercises together under the headings of 'text', 'networks', 'maps', and 'charts'. I've also added some entries that I am categorizing under 'art' The first is a series of exercises on  the sonification of data  and the second is a guide  to making twitterbots ; the third is about glitching digital imagery. These approaches can provide surprising and novel insights into history, as they move from representing history digitally to  performing  it. See for instance the final project in an undergraduate digital history class at the University of Saskatchewan by  Daniel Ruten  where he translated simple wordclouds of a WWI diary into a profound auditory performance. I would be very interested indeed to see if any final projects in HIST3814o gave sonification or twitterbots or glitch a try.     Texts  Networks  Maps  Charts  Art      Topic Modeling Tool  Network analysis in Gephi  Simple mapping   georectifying  Quick charts using RAW  Sonification    Topic Modeling in R  Converting 2-mode to 1-mode  QGIS (tutorials by Fred Gibbs)   Twitterbots    Text Analysis with OverviewProject  Network Analysis in R  Geoparsing with Python   Glitching Photos    Corpus Linguistics with AntConc  Network Analysis in Cytoscape  Palladio with Posner      Text Analysis with Voyant  Choose your own adventure  Leaflet.js Maps", 
            "title": "Things to install?"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-1", 
            "text": "Network Visualization  This exercise uses the open source programme  Gephi  which you install on your own computer. If you'd rather not install anything, please see  Network Analysis in R  instead.  Recall that the index of the collected letters of the Republic of Texas was just a list of letters from so-and-so to so-and-so. We haven't looked at the content of those letters, but the shape of network - the meta data of that correspondence - can be revealing (remember  Paul Revere! ) When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place. I would recommend that you also take a long look at Scott Weingart's series,  Networks Demystified . Finally,  heed our warning .  In this exercise, you will transform your Texan Correspondence data into a network, which you will then visualize with the open source programme  Gephi  The detailed instructions are  here .", 
            "title": "Exercise 1"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-2", 
            "text": "Topic Modeling Tool  In exercise 2, you will use the 'Topic Modeling Tool' to create a simple topic model and a webpage that allows you to browse the results.   Download the  tool .  Make sure you have some content on your own machine; the Colonial Newspaper Database is a handy corpus. (Created by Melodee Beals, it's a series of late 18th, early 19th century cleanly transcribed newspaper articles from Scotland and Northern England; You can grab my copy from  here ). Or perhaps you might move your copy of the Shawville equity out of DHBox onto your computer. At the command prompt in DHBox, type  $ ls  to make sure you can see your Equity folder (ie, you can't zip a folder from the command line if you are  in  that folder, so  cd  out of it if necessary). Assuming your files are in  equityfolder , zip the folder up with this command,  $ zip -r equityfiles.zip equityfolder . Then use the filemanager to download the zip file. Unzip the folder on your machine.  Double-click on the file you downloaded in step 1. This will open a java-based graphical user interface with one of the most common topic-modeling approaches, 'Latent Dirichlet Allocation'.  Set the input to be the Colonial Newspaper Database OR the Shawville Equity folder.  Set the output to be somewhere neat and tidy on your computer.  Set the number of topics you'd like to model.  Click 'train topics' to run the algorithm.  When it finishes, go to the folder you selected for output, and find the file 'all_topics.html' in the 'output_html' folder. Click on that, and you now have a browser-based way of navigating your topics and documents. In the output_csv folder created, you will find the same information as csv, which you could then input into a spreadsheet for other kinds of visualizations (which we'll talk about in class.)   Make a note in your open notebook about your process and your observations. How does reading this material in this way change/challenge/or focus your understanding of the material?", 
            "title": "Exercise 2"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-3", 
            "text": "Topic Modeling in R  Exercise 2 was quite a simple way to do topic modeling. In this exercise, we are going to use a package for the R statistical language called 'Mallet' to do our topic modeling. One way isn't necessarily better than the other, although doing our analysis within R allows the potential for extending the analysis or combining it with other data. First, read  this introduction to R  so what follows isn't a complete shock!   guidance for doing this in RStudio  in the DHBox  guidance for doing this in RStudio  installed on your own computer", 
            "title": "exercise 3"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-4", 
            "text": "Text Analysis with Overview  In exercise 4, we're going to look at the Colonial Newspaper Database again, but this time using a tool called 'Overview'. Overview uses a different approach that the topic models we've been discussing. In essence, it looks at word frequencies and their distributions within a document, and within a corpus, to organize the documents into folders of progressively similar word use.  You can download Overview to run on your own machine, but for our purposes, the hosted version at  https://www.overviewdocs.com/  is sufficient. Go to that page, watch the video, create an account, and then log in. (More help about how Overview works  may be found on their blog , including helpful videos.)  Once you're inside, click 'import from a CSV file', and upload the CND.csv (which you can download and save to your own machine from  here   - right-click and save as. On the 'UPLOAD A CSV FILE' page in Overview click 'browse' and select the CND.csv. It will give you a preview. There are a number of options here - you can tell Overview which words to ignore, and which words to give added importance to. What words will you select? Make a note in your notebook. Then hit 'upload'.  A new page appears, called 'YOUR DOCUMENT SETS'. Click on the one you just uploaded. A file folder tree showing documents of progressively greater similarity will open; on the right hand side will be the list of documents within each box (the box in question will be greyed out when you click on it, so you know where you are). You can search for words in your document, and Overview will tell you where they are; you can tag documents that you find interesting. The Overview system allows you to jump between a distant, macroscopic view and a close, document level view. Jump back and forth, see what you can find. For suggestions about how to use Overview effectively, try  their blog . Make notes about what you observe in your notebook. Also, you can export your tagged document set from Overview, so that you could visualize the patterns of tagging in a spreadsheet (for instance).  Going further  Do you see how you could upload your documents that you collected during Module 2?", 
            "title": "exercise 4"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-5", 
            "text": "Corpus Linguistics with AntConc  Heather Froelich has put together an excellent step-by-step with using AntConc for exploring textual patterns within, and across, corpora of texts. Work your way through her  tutorial  Can you get our example materials (from the Colonial Newspaper Database) into AntConc?  This might help you  to split the csv into individual txt files. Alternatively, do you have any materials of your own, already collected? Feed them into AntConc. What patterns do you see? What if you compare your materials against other corpora of texts?  FYI,  here is a collection of corpora that you can explore", 
            "title": "exercise 5"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-6", 
            "text": "Text Analysis with Voyant  In module 2 if you recall, we worked through how to transform XML using stylesheets. Melodee Beals used a  stylesheet   to transform her database into a series of individual txt files. In the exercises above, a transformer was used to make the database into a single CSV file. In this exercise, we are going to use  Voyant Tools  to visualize patterns in word use in the database. Voyant can read either a CSV  or  text files. The advantage of uploading a folder of text files is that, if the files are in chronological order, Voyant's default visualizations will also be arranged in chronological order and thus we can see change over time.  Go to  http://voyant-tools.org . Paste the URL to the csv of the CND database:  https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv  .  Now, open a new browser window, and go here  http://voyant-tools.org/?corpus=colonial-newspapers stopList=stop.en.taporware.txt  Do you see the difference? In the latter window, the individual articles have been uploaded individually, and thus are treated as individual documents in chronological order.  Explore the corpus, comparing terms over time, looking at keywords in context, and using the RezoViz tool to create a graph where people, places, and organizations that appear in the same documents (and across documents) are connected (you can find 'rezoviz' under the cogwheel icon at the top right of the panel). Google these terms and tools for what they mean and how others have used them. You can embed any of the tools in your blogs by using the 'save' icon and getting the iframe or embed code.  You can apply 'stopwords' by clicking on the cogwheel in any of the different tools, and selecting stopwords. Apply the stopwords globally, and you'll only have to do this once! What patterns do you see? What do different tools highlight? Which ones are useful? What patterns do you see that strike you as interesting? Note this all down.  Going further  Upload materials you collected in module 2 and explore them.", 
            "title": "exercise 6"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-7", 
            "text": "Quick Charts Using RAW  A quick chart can be a handy thing to have. Google spreadsheets, Microsoft Excel, and a host of other programs can make excellent charts quickly with their wizard functions. Never hesitate to turn to these. However, they are not always good with non-numeric data. In module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like  this . Can we do a quick visualization of this information? One useful tool is  RAW . Open that in a new window. Copy the table of data of places mentioned in the Texan correspondence, and paste it into the data input box at the top of the RAW screen.  oh noes an error!  A quick data munge  You should get an error message, to the effect that you need to check 'line 2'. What's gone wrong? RAW has checked the number of values you have in that row, and compared it to the number of columns in row 1 (which contains all the column names). It sees that the two don't match. What we need to do is add a default null value in those cells. So, go to  Google Sheets , click the 'go to google sheets' button, and then click on the big green plus sign to start a new sheet. Paste the following into the top-left cell (cell A1):  =IMPORTDATA(\"https://raw.githubusercontent.com/hist3907b-winter2015/module4-holes/master/texas.csv\")  Pretty neat, eh? Now, here's the thing: even though your sheet  looks  like it is filled with information, it's not (at least, as far as the script we are about to run is concerned). That is to say, the sheet itself only has one cell of data, and that one cell is grabbing info from elsewhere on the web and dynamically filling the sheet. The script we're going to run works only on static values (more or less).  So, place your cursor in cell B1. On a Mac, hit  shift+cmnd+downarrow . On a Windows machine, hit  shift+ctrl+downarrow . Then on Mac  shit+cmnd+rightarrow , on Windows  shitf+crtl+rightarrow . Then copy all of that data ( cmnd+c  or  ctrl+c ). Then, under 'Edit' select 'paste special' -  'paste VALUES only'.  The formula you put in cell A1 now says  #REF! . You can delete this now. This mucking about is necessary so that the add on script we are about to run will work.  We now need to fill those empty values. In the tool bar, click  add ons  -   get add ons . Search for  blanks . You want to add  Blank Detector .  Now, click somewhere in your data. On Mac, hit  cmnd+a . On Windows, hit  ctrl+a . This highlights all of your data. Click  Add ons  -   blank detector  -   detect cells . A dialogue panel will open on the right hand side of your screen. Click the button beside  set value  and type in  null . Hit  run . All of the blank cells will fill with the word  null . Delete column A (which formerly had record numbers, but is now just filled with the word  null . We don't need it).  If you get the error, 'run exceeded maximum time'  just hit the run button again. This script might take a few minutes.  You can now copy and paste your table of data into the data input box in RAW, and you should get the green thumbs up saying x records have been successfully parsed!  Playing with RAW  RAW takes your data, and depending on your choices, passes it into chart templates built on the d3.js code library. D3.js is a powerful library for making all sorts of charts (including interactive ones). If this sort of thing interests you, you can follow the tutorials in  Elijah Meeks' excellent new book .  With your data pasted in, you can now experiment with a number of different visualizations that are all built on the d3.js code library.  Try the \u2018alluvial\u2019 diagram.  Pick place1 and place2 as your dimensions - you click and drag the green boxes under 'map your data' into the 'steps' box. Leave the 'size' box empty. Under 'customize your visualization' you can click inside the 'width' box to make the diagram wider and more legible.  Does anything jump out? Try place3 and place 4. Try place1, place2, place3, and place4 in a single alluvial diagram. When we look at the original letters, we see that the writer often identified the town in which he was writing, and the town of the addressee. Why choose the third and fourth places? Perhaps it makes sense, for a given research question, to assume that with the pleasantries out of the way the writers will discuss the places important to their message. Experiment! This is one of the joys of working with data, experimenting to see how you can deform your materials to see them in a new light.  You can export your visualization under the 'download' box at the bottom of the RAW page - your choices are as a simple raster image (png), a vector image (svg) or a data representation (json).", 
            "title": "exercise 7"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-8", 
            "text": "Simple Mapping and Georectifying  In this exercise, you will find a historical map online, upload a copy to a mapwarper service, georectify it, and then display the map online, via a hosted service like CartoDB, and also through a map you will build yourself using leaflet.js. Finally, we will also convert csv to geojson using http://togeojson.com/, and we'll map that as a github gist. We'll also grab a geojson file hosted on github gist and import it into cartodb.  Georectifying  Georectifying is the process of taking an image (whether it is of a historical map, chart, airphoto, or whatever) and manipulating its geometry so that it matches a geographic projection. Think of it like this: you take your handdrawn map, and use pushpins to pin down known locations on your map to a globe. As you pin, your image stretches and warps. Traditionally, this has not been an easy thing to do, if you are new to GIS. In recent years, the curve has flattened significantly. In this exercise, we'll grab an image, upload it to the Harvard Library MapWarper service, and then export it as a tileset which can be used in other mapping programs.   Get a historical map. I like the Fire Insurance plans from the  Gatineau Valley Historical Society ; I'm sure you can find others to suit your interests.  Right-click, save as.... grab a copy. Save it somewhere handy.  Go to  Harvard World MapWarp  and sign up for an account. Then login.  Go to the upload screen:     Fill in as much of the metadata as you can. Then select your map from your computer, and upload it.  On the next page, click 'rectify'.     Pan and zoom both maps until you're sure you're looking at the same area in both. Double click in a map, select the pencil icon, and click on a point (location) you are sure you can match in the other window. Then click on the other map window, select the pencil, and then click on the same point. The 'add control point' button below and between both maps will light up. Click on this to confirm that this is a control point you want. Do this at least three times; the more times you can do it, the better the map warp.  Having selected your control points, click on 'warp image'.  You can now click on the 'export' panel, and get the URL for your georectified image in a few different formats. If you clicked on the KML option, a google map window will open  like so . For many webmapping applications, the Tiles (Google/OSM scheme): Tiles Based URL is what you want. You'll get a URL like this:  http://warp.worldmap.harvard.edu/maps/tile/4152/z/x/y.png    Save that info. You'll need it later.   You have now georectified a map. Let's use that map as a base layer in  Palladio  We need some place data for Palladio. Here's what I'm using       Note how I've formatted this data. I'll be copying and pasting it into Palladio. (For more on how to input geographic data into Palladio, see  this tutorial ). Basically, you want something like this:      Place  Coordinates       Mexico  23.634501,-102.552784     California  36.778261,-119.4179324     Brazos  32.661389,-98.121667     etc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude.   Go to  Palladio . Hit 'start' then 'upload spreadsheet or csv'. In the box, paste in your data.  You can progress to the next step without having any real data: just paste or type something in - see the video below.  Obviously, you won't have any points on your map, but if you were having trouble with that step, this allows you to bypass it to continue on with this tutorial.  Click on 'map'. Under 'places', select 'coordinates'. Then, click 'add new layer'. In the popup, beside 'Choose one of Palladio default layers or create a new one.', select 'custom'. This is where you're going to paste it that tiles based URL from the map warper. Paste it in, but  replace  the  /z/x/y  part with  {z}/{x}/{y} . Click add.   Here is a video walk through; places where you might have got into trouble include getting past the initial data entry box on Palladio, and finding where exactly to past in your georectified map url.   Congratulations! You've georectified a map, and used it as a base layer for a visualization of some point data. Here are some  notes on using a georectified map with the CartoDB service .", 
            "title": "exercise 8"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-9", 
            "text": "Network Analysis in R  Earlier, we took the index from the Texan Correspondence, a list of letters from so-and-so to so-and-so. When we stitch that together into a network of people connected because they exchanged letters, we end up with a shard of their social network. Networks can be queried for things like power, position, and role, and so used judiciously, we can begin to suss something of the social structures in which their history took place.Before you go any further, make sure you also take a long look at Scott Weingart's series,  Networks Demystified . Finally,  heed our warning .  This exercise uses the R language to do our analysis, which in DHBox we access via R Studio, a programming environment. Please read  this introduction to R  and then progress to the  exercise .", 
            "title": "exercise 9"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-10-qgis", 
            "text": "QGIS  There are many excellent tutorials around concerning how to get started with GIS. Our own library, in the  MADGIC centre  has tremendous resources and I would encourage you to speak with the map librarians before embarking on any  serious  mapping projects. In the short term, the historian  Fred Gibbs  has an excellent series on using the open source GIS platform  QGIS  to make and map historical data.  For this exercise, I would recommend you try Gibbs' first tutorial,  'Making a map with QGIS'  ...and then, try georectifying a historical map and adding it to your GIS:  'Using Historical maps with qgis'", 
            "title": "exercise 10 QGIS"
        }, 
        {
            "location": "/module-4/Exercises/#going-further", 
            "text": "There are many tutorials at  The Programming Historian  that are appropriate here. Try some under the 'data manipulation' or 'distant reading' headings.", 
            "title": "Going Further"
        }, 
        {
            "location": "/module-5/Humanities Visualization/", 
            "text": "Humanities Visualization Aug 7 - Aug 13\n\n\nIn this week, I want you to focus on making your final project. You do not need to read or annotate any pieces this week, nor to work through the exercises that are included in module 5. They are there for you to dip into as you finish your project. YOU DO have to write a blog post explaining what you've been up to this week.\n\n\nConcepts\n\n\nIn this module, we will be exploring the nuts and bolts of visualization. However, we will also be thinking about what it means to visualize 'data' from a \nhumanities\n perspective. Following Drucker, we're going to imagine what it means to think about our 'data' not as things received (ie, empirically observed) but rather as 'capta', as things taken, transformed.\n\n\nIt means visualizing and dealing with the intepretive process that got us to this point. What's more, we need to be aware of 'screen essentialism' and how it might be blinkering us to the possibilities of what humanities visualization could be. Finally, we need to be aware of the ways our digital 'templates' that we use reproduce ways of thinking and being that are antithetical to humanities' perspectives.\n\n\nThe following are worth reading on these issues:\n\n\n\n\nDrucker, J. \"Humanities approaches to graphical display\". \nDHQ\n 2011.5 \nhttp://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html\n\n\nWilliams, G. \"Disability, Universal Design, and the Digital Humanities\" \nDebates in the Digital Humanities\n 2012. \nhttp://dhdebates.gc.cuny.edu/debates/text/44\n\n\nOwens, T. \"Discovery and Justification Are Different: Notes on Science-ing the Humanities\" \nhttp://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/\n\n\nOwens, T. \"Defining Data for Humanists: Text, Artifact, Information, or Evidence?\" \nJournal of Digital Humanities\n 2011 1.1. \nhttp://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/\n\n\nWatters, Audrey. \n\"Men (Still) Explain Technology to Me: Gender and Education Technology\" \nHackeducation\n\n\n\n\nI also have a \nnumber of pieces of my own archaeological work\n that I think provide examples of how humanistic visualization can be a driver of interpretation and understanding. For instance, one thing I am currently working on is the possibility for \nsound\n \nto be a better representation of humanistic data\n. Oh, and by the way: maps are great, but sometimes, maps of ideas are even better; check out this \nlandscape of Last.fm Folksonomy\n. (If you have any facility with Python, you might like \nthis library that allows you to generate similar self-organizing maps\n). Since Python 3 is installed in your DHBox, you're all set!. (BTW, I find \nthis piece\n very helpful anytime I set out to do any Python on my own computer.)\n\n\nWhat you need to do this week\n\n\n\n\nWork on your project. You have until midnight August 16th to submit it. See the \nFinal Project\n requirements. Remember that all supporting files need to be in their own github repository (it is not necessary to share the Equity files, unless you have created some sort of dataset from them), while the final project itself has to be mounted on your own domain.\n\n\nTalk to me, talk to each other in Slack. Feel free to collaborate, but keep a record of who does what and how much.\n\n\nIf you missed completing a module, now might also be a good time to finish it (see \n2.5 of the course manual\n)\n\n\nUse the materials in this module to help make your project.\n\n\nWrite a blog post describing what you've been up to THIS WEEK re your project (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes on your process that you upload to your github. If you did any of the exercises, mention that).\n\n\n\n\nReadings\n\n\nNo formal readings are assigned this week.", 
            "title": "Communicating your Findings"
        }, 
        {
            "location": "/module-5/Humanities Visualization/#humanities-visualization-aug-7-aug-13", 
            "text": "In this week, I want you to focus on making your final project. You do not need to read or annotate any pieces this week, nor to work through the exercises that are included in module 5. They are there for you to dip into as you finish your project. YOU DO have to write a blog post explaining what you've been up to this week.", 
            "title": "Humanities Visualization Aug 7 - Aug 13"
        }, 
        {
            "location": "/module-5/Humanities Visualization/#concepts", 
            "text": "In this module, we will be exploring the nuts and bolts of visualization. However, we will also be thinking about what it means to visualize 'data' from a  humanities  perspective. Following Drucker, we're going to imagine what it means to think about our 'data' not as things received (ie, empirically observed) but rather as 'capta', as things taken, transformed.  It means visualizing and dealing with the intepretive process that got us to this point. What's more, we need to be aware of 'screen essentialism' and how it might be blinkering us to the possibilities of what humanities visualization could be. Finally, we need to be aware of the ways our digital 'templates' that we use reproduce ways of thinking and being that are antithetical to humanities' perspectives.  The following are worth reading on these issues:   Drucker, J. \"Humanities approaches to graphical display\".  DHQ  2011.5  http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html  Williams, G. \"Disability, Universal Design, and the Digital Humanities\"  Debates in the Digital Humanities  2012.  http://dhdebates.gc.cuny.edu/debates/text/44  Owens, T. \"Discovery and Justification Are Different: Notes on Science-ing the Humanities\"  http://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/  Owens, T. \"Defining Data for Humanists: Text, Artifact, Information, or Evidence?\"  Journal of Digital Humanities  2011 1.1.  http://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/  Watters, Audrey.  \"Men (Still) Explain Technology to Me: Gender and Education Technology\"  Hackeducation   I also have a  number of pieces of my own archaeological work  that I think provide examples of how humanistic visualization can be a driver of interpretation and understanding. For instance, one thing I am currently working on is the possibility for  sound   to be a better representation of humanistic data . Oh, and by the way: maps are great, but sometimes, maps of ideas are even better; check out this  landscape of Last.fm Folksonomy . (If you have any facility with Python, you might like  this library that allows you to generate similar self-organizing maps ). Since Python 3 is installed in your DHBox, you're all set!. (BTW, I find  this piece  very helpful anytime I set out to do any Python on my own computer.)", 
            "title": "Concepts"
        }, 
        {
            "location": "/module-5/Humanities Visualization/#what-you-need-to-do-this-week", 
            "text": "Work on your project. You have until midnight August 16th to submit it. See the  Final Project  requirements. Remember that all supporting files need to be in their own github repository (it is not necessary to share the Equity files, unless you have created some sort of dataset from them), while the final project itself has to be mounted on your own domain.  Talk to me, talk to each other in Slack. Feel free to collaborate, but keep a record of who does what and how much.  If you missed completing a module, now might also be a good time to finish it (see  2.5 of the course manual )  Use the materials in this module to help make your project.  Write a blog post describing what you've been up to THIS WEEK re your project (your successes, your failures, the help you may have found/received), and link to your 'faillog' (ie, the notes on your process that you upload to your github. If you did any of the exercises, mention that).", 
            "title": "What you need to do this week"
        }, 
        {
            "location": "/module-5/Humanities Visualization/#readings", 
            "text": "No formal readings are assigned this week.", 
            "title": "Readings"
        }, 
        {
            "location": "/module-5/Exercises/", 
            "text": "Exercises\n\n\nThe exercises in this module are about colour, layout, and manipulating graphics - but not necessarily in that order. You might find 'Sprucing up a PDF in Inkscape' exercise useful, as well as the 'Typography', 'Colour', and 'Layout' exercises.\n\n\nFinally: while we haven't explored the possibilities here, I do not want to leave you thinking that all digital history work is necessarily text based. To that end, if you are looking for a challenge or for exposure to something rather different, I suggest you at least bookmar my \nseries of tutorials on augmented reality, games, and 3d models for history and archaeology\n.\n\n\nSprucing up a PDF in Inkscape\n | \nTypography\n | \nColour\n | \nLayout\n | \nMore\n\n\n\n\nUkraine fist fight\n\n\nBy the way: Raster v Vector\n\n\nThe first thing to know is that graphic images come in two distinct flavours - raster, and vector.\n\n\nRaster images are composed of pixels, such that if you zoom into them, they become a pointilist blur (if you remember \nFerris Beuller's Day Off\n, there's a scene where Cameron stares and stares at a painting, falling into its individual points of colour...). Vector images on the other hand are described by mathematical functions, of lines and arcs and areas. No matter how deep you delve into these kinds of images, the image is always sharp - because the zoom is just another function of the mathematics describing the image.\n\n\nRasters: blocks of colours\n\n\nVectors: descriptions of points and arcs\n\n\nSprucing up a pdf in Inkscape\n\n\nSome of the tools that we used in Module 4 give visual output as raster images, others as vectors. Sometimes, we would like to tweak these outputs to make them more visually appealling, or clearer, or more useful. A program like MS Paint is only useful for dealing with raster images (and then, only in certain kinds of cases). We need a program that can deal with both, and also, lets us edit the image by treating each edit we do as a mostly-transparent layer on top of the image. That way, we can add, rearrange, hide or reveal, our edits to create a composite image. A free program that is immensely useful in this regard is \nInkscape\n. Inkscape is also quite useful in that we can open a pdf file in it, break the visual elements of the pdf into individual layers, and then rearrange/touch up/fix them up to make them more esthetically appealing.\n\n\nIn this first exercise, we will take the plot we generated in Module 4's exercise on topic modeling in R where we made a bar chart showing the number of articles by year. In R we exported that plot as a PDF. In Inkscape, we can import that pdf and 'explode' it so that we can manipulate its parts individually. We are going to take the simple bar chart and make it more legible, more visually appealing, for incorporation on a webpage.\n\n\n\n\nDownload and install \nInkscape\n \nnb Mac\n the installation instructions are a bit more complicated for Mac. Pay attention and follow closely!\n\n\nDownload the pdf we generated in R \npublication-year.pdf\n\n\nOpen that pdf. It's a pretty plain graphic. Right away there are at least two things we could do to make it more visually appealling. We could change the orientation of the characters in the y-axis to make them more legible. We can highlight bars of interest. And we could apply a colour scheme more generally that would make our graphic legible to folks with colour-blindness (see the 'going further' section at bottom).\n\n\nStart Inkscape. Click File \n Import \n and then navigate to where you save the 'publication-year.pdf'. Click Ok when you've got it selected. In the next pop-up, just accept the default settings and click 'ok'. Your Inkscape window should now look like this:\n\n\nThe pdf is now a layer in the new image you are creating in Inkscape. You can save this drawing, \nwith its information about the layers and what is in each one\n by clicking File \n Save As. (\nhere's my version\n). 'SVG' stands for 'scalable vector graphic'. (SVG is a kind of text file that describes the complete geometry of your illustration).\n\n\nDo you see the bounding box around the plot? If you grab any of those handles (the double-arrow things), you can make it bigger or smaller on your sheet. We can't edit any of the other elements yet - we can't change the colour of the bars, or the fonts of the text. We have to tell Inkscape to 'explode' these elements into their own 'objects'. In the menu bar at top, got to Object \n Ungroup. There are now a series of overlapping bounding boxes around each object.\n\n\nZoom in (by pressing the + sign on your keyboard) so that you're looking at the numbers of the y-axis. We're going to rotate these by 90 degrees to make them more legible. Select the arrow icon from the toolbar on the left side of the screen.\n\n\nClick on the '50'. You'll get a bounding box around it. Click Object \n Rotate 90 CW. The 50 is now rotated! Do the same for the other numbers. Save. (If you double-click on the number, you might trigger the 'text edit' function. If you do that, no problem - you can change the font, change the number... although if you did that, it'd be a bit dishonest, right? Click on the arrow pointer icon in the toolbar again to get out of the text-editing function).\n\n\nLet's imagine, for whatever reason, that you wanted to change one of the bars to a different colour, to highlight its importance to your argument. With the arrow icon, click on one of the bars so that you get the bounding box around it. Then, click on one of the colours from the palette at the bottom. Boom! You've got a newly colourized bar. Save.\n\n\nAdd a legend. Write it so that the important message you want your viewer to get is immediately clear. Choose a font from Inkscape's included fonts that \nsupports\n your message.\n\n\nTo export your image so that you can use it in a website or paper, click Edit \n Select All in All Layers. Every element of your image will now have a bounding box around it.\n\n\nGo to File \n Export Bitmap. Never mind the options in the popup; just hit 'Export'. Inkscape will automatically assign your drawing a name with .png; here's \nmine\n. Remember, if you want to edit this image again later, hit the 'save' button to save it as an svg. The svg will preserve all your layer information, while the png file is the visual representation (the png is in fact a raster graphic). Most browsers can handle svg files, so you could use that in your website; programs like Word seem to be able to handle raster graphics better than they do svg. You might want to experiment. In any event, every journal has different requirements for image formats. You would export your image to whatever those specifications are.\n\n\n\n\nGoing further\n\n\nIn this \ntutorial, you will learn how to load a custom colour palette\n. Why might you want to do that? You should be designing your work so that it is as universally accessible as possible. Many folks are colour-blind. Use \nColor-brewer\n to generate a colour-blind safe palette. Then look for the 'GIMP and Inkscape - GIMP color palette for this scheme.' Click on that link, and you'll get a text file with the scheme you generated. Use that scheme to alter the colours on your plot.\n\n\n\n\nTypography\n\n\nTypographic plays an important role in visual communication. It can do everything from reduce eyestrain (do a search for 'easiest fonts to read on screen') to communicate power and authority. Is it any wonder that strong bold capitals come to be known as 'Roman'? But first: \nare you a comic sans criminal?\n\n\nIn this exercise,\n\n\n\n\n\n\nI want you to read and understand the section on \nfont choices from the Owl at Purdue\n.\n\n\n\n\n\n\nThen, play some rounds of \nTypeconnection\n. Pay attention to why - or why not - various pairings work.\n\n\n\n\n\n\nThen, I want to consider the document you will be preparing for me that accounts for your learning in this course - the document where you choose your best exercises from the modules and explain how your approach to history, data, the digital, etc, has changed over this course. What typographic pair would work best for you?\n\n\n\n\n\n\nFinally, you'll make a webpage that uses those fonts and explains why they work.\n\n\n\n\n\n\nThe first part of this exercise then is to find a pair and to understand why they work best for you - go read the materials above, and once you're done with the Typeconnection site, go to \nGoogle Fonts\n and search for a pair that are \nsuitable for your purpose\n. When you find a font you like, click the 'add to collection' button. Then, at the bottom of the screen, you'll see a link for 'use'. Click on this - google will ask you if you want to use any of the variants of the font. Tick off accordingly. Then, do you see the embed code that google provides, and the code to integrate the font into your CSS (stylesheet)? Leave this window open - we're going to use it in a moment.\n\n\n\n\nMake a new repository for this exercise. In your repository, click the button beside 'branch'. In the search box that opens, type in \ngh-pages\n. This will create a version of your repository that can be served as a website.\n\n\nYou're now in the gh-pages branch. Click on the + sign beside the repository name. This will create a new file in your gh-branch of your repository. Call it \nmyfontchoice.html\n \n- the .html is important to specify; otherwise your browser will not know how to render your page.\n\n\nYou now need to put some html in there. I've written a simple webpage that will use two fonts from Google Fonts, and then applies the font to my text depending on whether or not it is a header, which you specify like this: \nh1\n this is a header in between header tags \n/h1\n or a paragraph, which you specify like this: \np\nblah blah blah a paragraph of text blah blah blah\n/p\n. My webpage is \nhere\n; right-click the link and open in a new tab. Copy the html into your new html document. Commit your changes (ie, save!).\n\n\nLet's see what we've got. To see the website version of your gh-pages branch, you go to \nyourusername\n.github.io/\nreponame\n/myfontchoice.html\n \n- ie, the final part of the url is the name of the document in your repo. Do that now. You should see a simple webpage, with two very distinctive fonts.\n\n\nNow: let's slide your font choices into the html. Go to your html page in your gh-pages repo (ie, not the \ngithub.io\n version, but the \ngithub.com/\nyourusername\n/\nrepo\n/myfontchoice.html\n version. Hit the edit button. Look closely at line 6. Do you see my two fonts? Do you see the pipe character between them? That tells google you want \nboth\n fonts. Go look at the google fonts page again to grab the exact name for your fonts (ie, uppercase letters make a difference!) and paste them into line 5 appropriately.\n\n\nLines 8 and 14 specify which font to use for headers, and which font to use for body text. Change appropriately.\n\n\nChange my silly text for a paragraph that explains what the fonts are, and why they work for this purpose. Commit your changes.\n\n\nGo to the \ngithub.io\n version of your repository (if you forget the address, you can find it under the 'settings' button on your normal repository page when you're in the gh-pages branch). Reload the page a couple of times to clear the older version you've already looked at and to replace it with your version. Ta da! Not only have you thought carefully about typography and fonts, you've also learned how to serve a webpage from a Github repository.\n\n\n\n\nHint\n You could use this as the basics of your submission for assessment, for your exercises. Build a webpage, link to your evidence, embed your images... For basic html \nhere's a really good guide to keep around\n.\n\n\n\n\nColour\n\n\nThere's a lot of bumpf out there on the 'pyschology of colour'. Google it to see what I mean (\nhere's an example\n). A lot of what you see here isn't so much psychology as it is associations (and indeed, western associations, at that). Associations are important of course; you don't want to undermine your message by making some unfortunate connections.\n\n\n\n\nIn this exercise, I want you to take the webpage you developed in the previous exercise, and make two versions of it: one, where the colours support the broader message of the page (a prototype for your exercises assessment piece, remember?), and the other where the colours \nundermine\n that broader message. Explain, in both cases, how your colour choices enhance/detract.\n\n\n\n\n\n\nalternatively, you can make a one page slide in powerpoint doing the same thing.\n\n\n\n\nResources\n\n\nHere's a graphic \n a movie to help with the theoretical side of things:\n\n\n\n\nUnderstanding the rules of color, Lynda.com\n\n\nTo learn how to style your webpage appropriately, you can \nfollow this tutorial on CSS from codeacademy.com\n.\n\n\nColours in Cultures:\n\n\n\n\n\n\nLayout\n\n\n'Layout' can mean different things in different contexts. A general overview on issues in layout is covered by \n'What makes design great, Lynda.com\n and \n'Exploring principles of layout and composition'\n. This \nslideshare\n gives you a sense of things to watch out for as well.\n\n\nFor academic posters in particular, consider \nthese suggestions from the APA\n.\n\n\nIn essence, good layout makes your argument legible, intuitive, and reinforces the rhetorical points you are trying to make. You should take into account 'principles of universal design' - consider these issues with \npowerpoint\n and with \nwebsites\n (although some of the technical solutions proposed in those two documents are a bit out of date, the general principles hold!)\n\n\nIn this exercise, you will design a new poster OR modify an existing poster. You can use either Inkscape or Powerpoint.\n\n\n\n\n\n\nInkscape: download one of the scientific poster templates from \nUgo Sangiorgi\n (These are developed from \nthis blog post\n; note his discussion on design). Open it in Inkscape. Make notes in your open notebook: \nfrom the point of view of layout\n, what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercise (\nremember, details here\n)?  \n Here's \nhelp with the basics of Inkscape\n. Modify the poster, and upload the svg or pdf or png version to your repository.\n\n\n\n\n\n\nPPT: there's a lot of information out there on making posters with powerpoint. \nRead parts 1,2, and 3 here\n and then consider \nthe advice here\n. Once you've read and digested, pick a poster from \nPimp my poster\n that strikes use. Make notes in your open notebook: \nfrom the point of view of layout\n what elements of the design work? What aren't working? How would you repurpose this posert to fit the requirements ot he assessment exercies (\nremember, details here\n)? \n Grab a template from \nhere\n, and use it to prototype a poster that works. Upload the poster as a pdf to your repository.\n\n\n\n\n\n\nIf you want to explore layout in the context of webpage creation, I would point you to the the roster of lessons at \nCodeacademy\n. Same instructions: find an existing site that you will consider from the point of view of what works, what doesn't, and use that reflection to guide the construction of your own.\n\n\n\n\n\n\nMore\n\n\nMore resources, tutorials, and things to explore.\n\n\nAccessibility and Design\n\n\nWhile most of this applies to websites, think also about how the general principles apply to other ways \n means of telling our data stories.\n\n\n\n\nHow People with Disabilities Use the Web\n\n\nWeb Content Accessibility and Mobile Web\n\n\nAccessibility in HTML5\n\n\nWeb Accessibility Evaluation Tool\n\n\nConsidering the User Perspective\n\n\nConstructing a POUR Website\n Percievable, Operable, Understandable, Robust. Applies much wider than design of websites.\n\n\nChecking Microsoft Office-generated documents for accessibility\n\n\n\n\nInfographics / Storytelling\n\n\nInfographics\n\n\n\n\nThe Difference between Infographics and Visualizations\n\n\nDesign hints for infographics\n\n\nPiktochart\n\n\nInfogr.am\n\n\nInfoactive.co\n (has a free option, but might not allow exports. You'd have to check).\n\n\nEasel.ly\n\n\n\n\nStorytelling\n\n\n\n\nCreatavist\n\n\nMedium\n\n\nCowbird\n\n\nExposure\n\n\nTimeline.js\n\n\nStorymap\n\n+\n\n\n\n\nManyLines\n\n\nManylines\n is an application that allows you to create narratives from network graphs. In essence, you upload a network file in .gexf format (which you can export from Gephi) and it renders it on the screen. There are some layout options to make the graph more intelligible. Then, you take a series of snapshots zoomed in on the graph in different places, and add text to describe what it is that's important about these networks. The app puts a Prezi-like wrapper around your snapshots, and the whole can then be embedded in a website or be used as a standalone website. \nHere's my first attempt.\n \n\n\nYou can also embed nearly anything in the narrative panels - youtube videos, \ntimeline.js\n, as long as you know how to correctly format an \niframe\n.  \n\n\nTo give this a try, why not use the Texan Correspondence network we generated in earlier modules? Export it in .gexf format from gephi, import to ManyLines, and go! The interface is fairly straightforward. Just follow the prompts.\n\n\nCaveat Utilitor\n I don't know how long anything made with ManyLines will live on their website. But, knowing what you know about wget and other tools, do you see how you could archive a copy on your own machine? ManyLines is available on \ngithub\n so you can certainly use it locally.\n\n\nLeaflet\n\n\nMaps can be created through a variety of services (\ntilemill\n, \ncartodb\n, \nmapbox\n, for instance). These can then be embedded in your webpages or documents. Often, that's enough. But sometimes, you'd like to take control, and keep all the data, all the map, under your own name, in your own webspace. Here is a gentle introduction to using \nleaflet.js\n to make, style, and serve your maps. \nHere is a template\n for mapping with leaflet, drawing all of your point data and ancillary information from a csv file.\n\n\nOh, and here's a list of background maps you can use: \nhttp://leaflet-extras.github.io/leaflet-providers/preview/index.html\n.\n\n\nDesigning Maps with Processing and Illustrator\n\n\nNicolas Felton is a renowned designer; this 90 minute workshop is worth watching and working through. \nSkillshare\n\n\nCaveat: I do not use processing or illustrator, but processing is free and Inkscape can do many of the things that Illustrator does.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-5/Exercises/#exercises", 
            "text": "The exercises in this module are about colour, layout, and manipulating graphics - but not necessarily in that order. You might find 'Sprucing up a PDF in Inkscape' exercise useful, as well as the 'Typography', 'Colour', and 'Layout' exercises.  Finally: while we haven't explored the possibilities here, I do not want to leave you thinking that all digital history work is necessarily text based. To that end, if you are looking for a challenge or for exposure to something rather different, I suggest you at least bookmar my  series of tutorials on augmented reality, games, and 3d models for history and archaeology .  Sprucing up a PDF in Inkscape  |  Typography  |  Colour  |  Layout  |  More   Ukraine fist fight  By the way: Raster v Vector  The first thing to know is that graphic images come in two distinct flavours - raster, and vector.  Raster images are composed of pixels, such that if you zoom into them, they become a pointilist blur (if you remember  Ferris Beuller's Day Off , there's a scene where Cameron stares and stares at a painting, falling into its individual points of colour...). Vector images on the other hand are described by mathematical functions, of lines and arcs and areas. No matter how deep you delve into these kinds of images, the image is always sharp - because the zoom is just another function of the mathematics describing the image.  Rasters: blocks of colours  Vectors: descriptions of points and arcs", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-5/Exercises/#sprucing-up-a-pdf-in-inkscape", 
            "text": "Some of the tools that we used in Module 4 give visual output as raster images, others as vectors. Sometimes, we would like to tweak these outputs to make them more visually appealling, or clearer, or more useful. A program like MS Paint is only useful for dealing with raster images (and then, only in certain kinds of cases). We need a program that can deal with both, and also, lets us edit the image by treating each edit we do as a mostly-transparent layer on top of the image. That way, we can add, rearrange, hide or reveal, our edits to create a composite image. A free program that is immensely useful in this regard is  Inkscape . Inkscape is also quite useful in that we can open a pdf file in it, break the visual elements of the pdf into individual layers, and then rearrange/touch up/fix them up to make them more esthetically appealing.  In this first exercise, we will take the plot we generated in Module 4's exercise on topic modeling in R where we made a bar chart showing the number of articles by year. In R we exported that plot as a PDF. In Inkscape, we can import that pdf and 'explode' it so that we can manipulate its parts individually. We are going to take the simple bar chart and make it more legible, more visually appealing, for incorporation on a webpage.   Download and install  Inkscape   nb Mac  the installation instructions are a bit more complicated for Mac. Pay attention and follow closely!  Download the pdf we generated in R  publication-year.pdf  Open that pdf. It's a pretty plain graphic. Right away there are at least two things we could do to make it more visually appealling. We could change the orientation of the characters in the y-axis to make them more legible. We can highlight bars of interest. And we could apply a colour scheme more generally that would make our graphic legible to folks with colour-blindness (see the 'going further' section at bottom).  Start Inkscape. Click File   Import   and then navigate to where you save the 'publication-year.pdf'. Click Ok when you've got it selected. In the next pop-up, just accept the default settings and click 'ok'. Your Inkscape window should now look like this:  The pdf is now a layer in the new image you are creating in Inkscape. You can save this drawing,  with its information about the layers and what is in each one  by clicking File   Save As. ( here's my version ). 'SVG' stands for 'scalable vector graphic'. (SVG is a kind of text file that describes the complete geometry of your illustration).  Do you see the bounding box around the plot? If you grab any of those handles (the double-arrow things), you can make it bigger or smaller on your sheet. We can't edit any of the other elements yet - we can't change the colour of the bars, or the fonts of the text. We have to tell Inkscape to 'explode' these elements into their own 'objects'. In the menu bar at top, got to Object   Ungroup. There are now a series of overlapping bounding boxes around each object.  Zoom in (by pressing the + sign on your keyboard) so that you're looking at the numbers of the y-axis. We're going to rotate these by 90 degrees to make them more legible. Select the arrow icon from the toolbar on the left side of the screen.  Click on the '50'. You'll get a bounding box around it. Click Object   Rotate 90 CW. The 50 is now rotated! Do the same for the other numbers. Save. (If you double-click on the number, you might trigger the 'text edit' function. If you do that, no problem - you can change the font, change the number... although if you did that, it'd be a bit dishonest, right? Click on the arrow pointer icon in the toolbar again to get out of the text-editing function).  Let's imagine, for whatever reason, that you wanted to change one of the bars to a different colour, to highlight its importance to your argument. With the arrow icon, click on one of the bars so that you get the bounding box around it. Then, click on one of the colours from the palette at the bottom. Boom! You've got a newly colourized bar. Save.  Add a legend. Write it so that the important message you want your viewer to get is immediately clear. Choose a font from Inkscape's included fonts that  supports  your message.  To export your image so that you can use it in a website or paper, click Edit   Select All in All Layers. Every element of your image will now have a bounding box around it.  Go to File   Export Bitmap. Never mind the options in the popup; just hit 'Export'. Inkscape will automatically assign your drawing a name with .png; here's  mine . Remember, if you want to edit this image again later, hit the 'save' button to save it as an svg. The svg will preserve all your layer information, while the png file is the visual representation (the png is in fact a raster graphic). Most browsers can handle svg files, so you could use that in your website; programs like Word seem to be able to handle raster graphics better than they do svg. You might want to experiment. In any event, every journal has different requirements for image formats. You would export your image to whatever those specifications are.   Going further  In this  tutorial, you will learn how to load a custom colour palette . Why might you want to do that? You should be designing your work so that it is as universally accessible as possible. Many folks are colour-blind. Use  Color-brewer  to generate a colour-blind safe palette. Then look for the 'GIMP and Inkscape - GIMP color palette for this scheme.' Click on that link, and you'll get a text file with the scheme you generated. Use that scheme to alter the colours on your plot.", 
            "title": "Sprucing up a pdf in Inkscape"
        }, 
        {
            "location": "/module-5/Exercises/#typography", 
            "text": "Typographic plays an important role in visual communication. It can do everything from reduce eyestrain (do a search for 'easiest fonts to read on screen') to communicate power and authority. Is it any wonder that strong bold capitals come to be known as 'Roman'? But first:  are you a comic sans criminal?  In this exercise,    I want you to read and understand the section on  font choices from the Owl at Purdue .    Then, play some rounds of  Typeconnection . Pay attention to why - or why not - various pairings work.    Then, I want to consider the document you will be preparing for me that accounts for your learning in this course - the document where you choose your best exercises from the modules and explain how your approach to history, data, the digital, etc, has changed over this course. What typographic pair would work best for you?    Finally, you'll make a webpage that uses those fonts and explains why they work.    The first part of this exercise then is to find a pair and to understand why they work best for you - go read the materials above, and once you're done with the Typeconnection site, go to  Google Fonts  and search for a pair that are  suitable for your purpose . When you find a font you like, click the 'add to collection' button. Then, at the bottom of the screen, you'll see a link for 'use'. Click on this - google will ask you if you want to use any of the variants of the font. Tick off accordingly. Then, do you see the embed code that google provides, and the code to integrate the font into your CSS (stylesheet)? Leave this window open - we're going to use it in a moment.   Make a new repository for this exercise. In your repository, click the button beside 'branch'. In the search box that opens, type in  gh-pages . This will create a version of your repository that can be served as a website.  You're now in the gh-pages branch. Click on the + sign beside the repository name. This will create a new file in your gh-branch of your repository. Call it  myfontchoice.html   - the .html is important to specify; otherwise your browser will not know how to render your page.  You now need to put some html in there. I've written a simple webpage that will use two fonts from Google Fonts, and then applies the font to my text depending on whether or not it is a header, which you specify like this:  h1  this is a header in between header tags  /h1  or a paragraph, which you specify like this:  p blah blah blah a paragraph of text blah blah blah /p . My webpage is  here ; right-click the link and open in a new tab. Copy the html into your new html document. Commit your changes (ie, save!).  Let's see what we've got. To see the website version of your gh-pages branch, you go to  yourusername .github.io/ reponame /myfontchoice.html   - ie, the final part of the url is the name of the document in your repo. Do that now. You should see a simple webpage, with two very distinctive fonts.  Now: let's slide your font choices into the html. Go to your html page in your gh-pages repo (ie, not the  github.io  version, but the  github.com/ yourusername / repo /myfontchoice.html  version. Hit the edit button. Look closely at line 6. Do you see my two fonts? Do you see the pipe character between them? That tells google you want  both  fonts. Go look at the google fonts page again to grab the exact name for your fonts (ie, uppercase letters make a difference!) and paste them into line 5 appropriately.  Lines 8 and 14 specify which font to use for headers, and which font to use for body text. Change appropriately.  Change my silly text for a paragraph that explains what the fonts are, and why they work for this purpose. Commit your changes.  Go to the  github.io  version of your repository (if you forget the address, you can find it under the 'settings' button on your normal repository page when you're in the gh-pages branch). Reload the page a couple of times to clear the older version you've already looked at and to replace it with your version. Ta da! Not only have you thought carefully about typography and fonts, you've also learned how to serve a webpage from a Github repository.   Hint  You could use this as the basics of your submission for assessment, for your exercises. Build a webpage, link to your evidence, embed your images... For basic html  here's a really good guide to keep around .", 
            "title": "Typography"
        }, 
        {
            "location": "/module-5/Exercises/#colour", 
            "text": "There's a lot of bumpf out there on the 'pyschology of colour'. Google it to see what I mean ( here's an example ). A lot of what you see here isn't so much psychology as it is associations (and indeed, western associations, at that). Associations are important of course; you don't want to undermine your message by making some unfortunate connections.   In this exercise, I want you to take the webpage you developed in the previous exercise, and make two versions of it: one, where the colours support the broader message of the page (a prototype for your exercises assessment piece, remember?), and the other where the colours  undermine  that broader message. Explain, in both cases, how your colour choices enhance/detract.    alternatively, you can make a one page slide in powerpoint doing the same thing.   Resources  Here's a graphic   a movie to help with the theoretical side of things:   Understanding the rules of color, Lynda.com  To learn how to style your webpage appropriately, you can  follow this tutorial on CSS from codeacademy.com .  Colours in Cultures:", 
            "title": "Colour"
        }, 
        {
            "location": "/module-5/Exercises/#layout", 
            "text": "'Layout' can mean different things in different contexts. A general overview on issues in layout is covered by  'What makes design great, Lynda.com  and  'Exploring principles of layout and composition' . This  slideshare  gives you a sense of things to watch out for as well.  For academic posters in particular, consider  these suggestions from the APA .  In essence, good layout makes your argument legible, intuitive, and reinforces the rhetorical points you are trying to make. You should take into account 'principles of universal design' - consider these issues with  powerpoint  and with  websites  (although some of the technical solutions proposed in those two documents are a bit out of date, the general principles hold!)  In this exercise, you will design a new poster OR modify an existing poster. You can use either Inkscape or Powerpoint.    Inkscape: download one of the scientific poster templates from  Ugo Sangiorgi  (These are developed from  this blog post ; note his discussion on design). Open it in Inkscape. Make notes in your open notebook:  from the point of view of layout , what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercise ( remember, details here )?    Here's  help with the basics of Inkscape . Modify the poster, and upload the svg or pdf or png version to your repository.    PPT: there's a lot of information out there on making posters with powerpoint.  Read parts 1,2, and 3 here  and then consider  the advice here . Once you've read and digested, pick a poster from  Pimp my poster  that strikes use. Make notes in your open notebook:  from the point of view of layout  what elements of the design work? What aren't working? How would you repurpose this posert to fit the requirements ot he assessment exercies ( remember, details here )?   Grab a template from  here , and use it to prototype a poster that works. Upload the poster as a pdf to your repository.    If you want to explore layout in the context of webpage creation, I would point you to the the roster of lessons at  Codeacademy . Same instructions: find an existing site that you will consider from the point of view of what works, what doesn't, and use that reflection to guide the construction of your own.", 
            "title": "Layout"
        }, 
        {
            "location": "/module-5/Exercises/#more", 
            "text": "More resources, tutorials, and things to explore.", 
            "title": "More"
        }, 
        {
            "location": "/module-5/Exercises/#accessibility-and-design", 
            "text": "While most of this applies to websites, think also about how the general principles apply to other ways   means of telling our data stories.   How People with Disabilities Use the Web  Web Content Accessibility and Mobile Web  Accessibility in HTML5  Web Accessibility Evaluation Tool  Considering the User Perspective  Constructing a POUR Website  Percievable, Operable, Understandable, Robust. Applies much wider than design of websites.  Checking Microsoft Office-generated documents for accessibility", 
            "title": "Accessibility and Design"
        }, 
        {
            "location": "/module-5/Exercises/#infographics-storytelling", 
            "text": "Infographics   The Difference between Infographics and Visualizations  Design hints for infographics  Piktochart  Infogr.am  Infoactive.co  (has a free option, but might not allow exports. You'd have to check).  Easel.ly   Storytelling   Creatavist  Medium  Cowbird  Exposure  Timeline.js  Storymap \n+", 
            "title": "Infographics / Storytelling"
        }, 
        {
            "location": "/module-5/Exercises/#manylines", 
            "text": "Manylines  is an application that allows you to create narratives from network graphs. In essence, you upload a network file in .gexf format (which you can export from Gephi) and it renders it on the screen. There are some layout options to make the graph more intelligible. Then, you take a series of snapshots zoomed in on the graph in different places, and add text to describe what it is that's important about these networks. The app puts a Prezi-like wrapper around your snapshots, and the whole can then be embedded in a website or be used as a standalone website.  Here's my first attempt.    You can also embed nearly anything in the narrative panels - youtube videos,  timeline.js , as long as you know how to correctly format an  iframe .    To give this a try, why not use the Texan Correspondence network we generated in earlier modules? Export it in .gexf format from gephi, import to ManyLines, and go! The interface is fairly straightforward. Just follow the prompts.  Caveat Utilitor  I don't know how long anything made with ManyLines will live on their website. But, knowing what you know about wget and other tools, do you see how you could archive a copy on your own machine? ManyLines is available on  github  so you can certainly use it locally.", 
            "title": "ManyLines"
        }, 
        {
            "location": "/module-5/Exercises/#leaflet", 
            "text": "Maps can be created through a variety of services ( tilemill ,  cartodb ,  mapbox , for instance). These can then be embedded in your webpages or documents. Often, that's enough. But sometimes, you'd like to take control, and keep all the data, all the map, under your own name, in your own webspace. Here is a gentle introduction to using  leaflet.js  to make, style, and serve your maps.  Here is a template  for mapping with leaflet, drawing all of your point data and ancillary information from a csv file.  Oh, and here's a list of background maps you can use:  http://leaflet-extras.github.io/leaflet-providers/preview/index.html .", 
            "title": "Leaflet"
        }, 
        {
            "location": "/module-5/Exercises/#designing-maps-with-processing-and-illustrator", 
            "text": "Nicolas Felton is a renowned designer; this 90 minute workshop is worth watching and working through.  Skillshare  Caveat: I do not use processing or illustrator, but processing is free and Inkscape can do many of the things that Illustrator does.", 
            "title": "Designing Maps with Processing and Illustrator"
        }, 
        {
            "location": "/conclusion/conclusion/", 
            "text": "Conclusion\n\n\nThere will be a profound and witty concluding paragraph here, some day. But if you've noticed anything at all, it's that the process of crafting digital history never comes to an end. \nAdam Crymble\n published a piece recently that throws this into high relief - pay attention to his second paragraph in his main diagram!\n\n\nIn the meantime, I do want you to know that you too can craft excellent digital history. There were some great moments in the first iteration of this course, and you will have great moments too: like when Matt forked one of my tutorials and rewrote it for the better, or built a virtual machine. Or when Patrick finally slayed Github! Or when Allison got the Canadiana API to work. Or when Phoebe finally persuaded Inkscape to play nice. Or when Matt conquered TWARC. Or when TEI blew Ryan's mind. Or when Christina forked an Anthropology class project at MSU to repurpose for her project. Or... or... or. We covered a lot of ground.\n\n\nYou will too. \n\n\nA word to the wise\n\n\nIf, in your other courses, you decide to use some of the methods here, I will be most gratified. However - in course work as in life, \nknow\n your audience. Will your prof appreciate this work? Is your prof familiar with the underlying issues - will she know what to look for, the hidden gotchas, the places where things might get, erm, fudged? It is \nyour\n responsibility to make the argument, in your work, why a particular methodological choice is appropriate. It is \nyour\n responsibility to show how these choices are theoretically informed and meaningful. As in all history, you have to make the argument. Never fall into the trap of thinking the method speaks for itself. This is why I compelled you to create the paradata document for your project. As that master historian Yoda once said,\n\n\n\n\nDo or do not. There is no try.\n\n\n\n\nAlright, not exactly appropriate. But you get the gist: digital history requires explicit paradata. Do. There is no try.", 
            "title": "6. Final Thoughts"
        }, 
        {
            "location": "/conclusion/conclusion/#conclusion", 
            "text": "There will be a profound and witty concluding paragraph here, some day. But if you've noticed anything at all, it's that the process of crafting digital history never comes to an end.  Adam Crymble  published a piece recently that throws this into high relief - pay attention to his second paragraph in his main diagram!  In the meantime, I do want you to know that you too can craft excellent digital history. There were some great moments in the first iteration of this course, and you will have great moments too: like when Matt forked one of my tutorials and rewrote it for the better, or built a virtual machine. Or when Patrick finally slayed Github! Or when Allison got the Canadiana API to work. Or when Phoebe finally persuaded Inkscape to play nice. Or when Matt conquered TWARC. Or when TEI blew Ryan's mind. Or when Christina forked an Anthropology class project at MSU to repurpose for her project. Or... or... or. We covered a lot of ground.  You will too.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/conclusion/conclusion/#a-word-to-the-wise", 
            "text": "If, in your other courses, you decide to use some of the methods here, I will be most gratified. However - in course work as in life,  know  your audience. Will your prof appreciate this work? Is your prof familiar with the underlying issues - will she know what to look for, the hidden gotchas, the places where things might get, erm, fudged? It is  your  responsibility to make the argument, in your work, why a particular methodological choice is appropriate. It is  your  responsibility to show how these choices are theoretically informed and meaningful. As in all history, you have to make the argument. Never fall into the trap of thinking the method speaks for itself. This is why I compelled you to create the paradata document for your project. As that master historian Yoda once said,   Do or do not. There is no try.   Alright, not exactly appropriate. But you get the gist: digital history requires explicit paradata. Do. There is no try.", 
            "title": "A word to the wise"
        }, 
        {
            "location": "/supporting materials/tei/", 
            "text": "Close Reading with TEI\n\n\nThis worksheet, and all related files, are released CC-BY.\n\n\nBy M. H. Beals\n; Adapted for HIST3814o by S Graham\n\n\nYou will need the files in \nthis subfolder\n so download that repository as a zip file and unzip it somewhere handy on your machine.\n\n\nThis exercise will explore a historical text and help you create a digital record of your analysis\n\n\nVetting a Website\n\n\nVisit the Recovered Histories Website at \nhttp://www.recoveredhistories.org\n\n\nExamine the site's layout and read its introduction\n\n\n\n\nWhat makes you believe this site is a trustworthy provider of historical texts?\n\n\nWhat makes you believe this site is NOT a trustworthy provider of historical texts?\n\n\n\n\nFinding a Source\n\n\nVisit the site's collections via the 'Browse' function\n\n\nLocate the pamphlet \nNegro Slavery\n by Zachary Macaulay and open it\n\n\nThis is an abolitionist pamphlet regarding the Atlantic slave trade, presenting and examine evidence of how it is run.  When you approach a primary source like this, it is tempting to read through it from beginning to end, to get an overview of its contents, and then 'mine' or 'cherry-pick' good quotations to include in your assessments.  However, we are going to focus on examining a very small part of the text in a very high level of detail.  \n\n\nSetting Up Your Workspace\n\n\nYou will use your own machine rather than DHBox for this work. Arrange your workspace so that you have the scanned text of the pamphlet easily visible on one side of your screen. Open the 'blanktemplate.txt' file in \nSublime Text\n, \nAtom\n, \nTextwrangler\n or \nNotepadd++\n (or any text editor that understands encoding) and have that on the other side of the screen.\n\n\nThe last lines will be\n\n/body\n/text\n/TEI\n/teiCorpus\n. Everything you write today should be just above \n/body\n.\n\n\nTranscribing Your Page\n\n\nThe first thing you will need is go to the tag\n\n\nbiblScope\n1\n/biblScope\n\n\nand replace the number one (1) with the page number you are transcribing. Which page should you transcribe? Select a page in the document that you find interesting.\n\n\nNext, you will need to \nvery carefully\n transcribe your page of text from the image into your document.  Make sure you do not make any changes to the text, even if you think they author has used poor grammar or misspelled a word.  You do not need to worry about line breaks but should start every new paragraph (or heading) with a \np\n and end every paragraph (or heading) with a \n/p\n.\n\n\nOnce you have completed your transcription, look away from your computer for 30-45 seconds.  Staring into the distance every 10-20 minutes will keep your eyes from straining.  Also, shake out your hands at the wrists, to prevent repetitive stress injuries to your fingers.  \n\n\nEncoding Your Transcription\n\n\nYou are now going to \nencode\n or \nmark-up\n your text.  \n\n\nRe-read your page and highlight / colour the following things:\n\n\n\n\nAny persons mentioned (including any he/she if they refer to a specific person)\n\n\nAny places mentioned\n\n\nAny claims, assertions or arguments made\nNow that you have highlighted these, you are going to put proper code around them.\n\n\n\n\nFor persons, surround your text with these\n\n\npersName key=\"Last, First\" from=\"YYYY\" to=\"YYYY\" role=\"Occupation\" ref=\"http://www.website.com/webpage.html\"\n \n/persName\n\n\n\n\nInside the speech marks for \nkey\n, include the real full name of the person mentioned\n\n\nIn \nfrom\n and \nto\n, include their birth and death years, using ? for unknown years\n\n\nIn \nrole\n, put the occupation, role or 'claim to fame' for this individual.  \n\n\nIn \nref\n, put the URL (link) to the Dictionary of National Biography, Wikipedia or other biography website where you found this information. If there is a \n in your link, you will need to replace this with \namp;\n\n\n\n\nFor places, surround your text with these\n\n\nplaceName key=\"Sheffield, United Kingdom\" ref=\"http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield\nparams=53_23_01_N_1_28_01_W_type:city_region:GB\"\"\n \n/placeName\n\n\n\n\nIn \nkey\n, put the city and country with best information you can find for the modern names for this location\n\n\nIn \nref\n, put a link to the relevant coordinates on Wikipedia GeoHack website (http://tools.wmflabs.org/geohack/)\n\nTo obtain this, go to the Wikipedia page for this city and click on the latitude/longitude coordinates for the location. For large areas, such as entire countries or continents, just use the Wikipedia page URL.\n\n\n\n\nFor claims or arguments, surround your text with these\n\n\ninterp key=\"reason\" n=\"citation\" cert=\"high\" ref=\"http://www.website.com/webpage.html\"\n \n/interp\n\n\n\n\nIn \nkey\n, explain why you believe this claim is true or not\n\n\nIn \nn\n, put a full citation to the relevant source\n\n\nIn \ncert\n (short for certainty), put: high, medium, low or unknown\n\n\nIn \nref\n, put the link to the website where you got the information to assess this claim.\n\n\n\n\nWhen you are happy with your work, hit save your work, give it a useful name, make sure it has .xml as the extension, and save it \nand the .xsl file\n to your repository.\n\n\n\n\nAlex Gill has made \nThe Short and Sweet TEI Handout\n which you might want to explore as well. When you embark on encoding documents for your own research, \nhere are some questions to think about\n to help you decide what kinds of tagging you'll need; these \ntemplates from HisTEI\n might be useful (open the whole project with OxygenXML for full functionality, but you can copy those templates in any editor).\n\n\n\n\nViewing Your Encoded Text\n\n\nTo see your encoded text, make sure your .xml and .xsl file are in the same folder. \nOpen either Internet Explorer or Firefox\n. The following will not work in Chrome because it has different security settings.\n\n\nMaking sure both your (page number).xml file and your 000style.xsl file are in the same folder (or both on your desktop), drag the icon for (page number).xml into your Internet Explorer Browser window.\n\n\nIf you now see a colour-coded version of your text, Congratulations! If you hover over the coloured sections, you should see a pop-up with the additional information you entered.\n\n\nIf your text comes up only in black, with no paragraph divisions or headings, or doesn't come up at all, something has gone wrong. Re-open your .xml file and check that you have:\n\n\nPlaced \np\n at the start of every paragraph, including the start of the page\n\n\nPlaced \n/p\n at the end of every paragraph, including the end of the page\n\n\nMade sure all your \npersName\n, \nplaceName\n and \ninterp\n tags are properly enclosed in \ns\n\n\nMade sure you have both an open \n and close \n\\\n tag for each tag you use\n\n\nMade sure you attribute values are fully enclosed in \n\"\"\n.\n\n\nMade sure you have a space between the \n\"\n of one attribute and the start of the next\n\n\nMade sure you do NOT have a space after the \n=\n of an attribute\n\n\nIf your text still does not appear formatted, you may need to remove the text one paragraph at a time, refreshing your browser window, until it appears. This will help you identify which paragraph (or sentence) has the error within it).\n\n\nIf you still don't see your text\n\n\nIf you do not see the colour-coded version of your text, this might not necessarily mean that you've done something wrong\n\n\n\n\nSome browsers will not perform the transformation, for security reasons.\n\n\n\n\nIn which case, here's what we can do. If you are on a Windows machine using Notepad++, go to 'Plugins' \n Plugin Tools. (If you are on Windows but aren't using Notepad++, Sublime and Atom probably have a similar functionality, but you will have to search to figure it out.) Select 'XML Tools' from the list, and install it. You'll probably have to restart the program to complete the plugin installation. Open up the \n1.xml\n file in Notepad ++. Then, under 'plugins'\n'xml tools\" select 'XSL Transformation settings'. In the popup, click on the elipses: \n...\n to open up the file finder, and select the \n000style.xsl\n stylesheet. Click 'transform'. A new tab will open in Notepad++ \nwith a fully-formed html file displaying your data according to the stylesheet.\n Save this, and then open it in a browser!\n\n\n\n\n\n\nYou can also check 'validate' from the XML Tools menu in Notepad++, which will identify errors in your XML. If you're still having errors, a likely culprit might be the way your geographic URLs are encoded. Compare what you've got with what's in the \n1.xml\n reference document.\n\n\n\n\n\n\nAdvanced\n: If you install a \nWAMP\n or \nMAMP\n server, and put your xml and xsl files in the WWW folder, you \nshould\n be able to see the transformation no problem at \nlocalhost\\myxml.xml\n (for example). (You can also use \nPython's built in webserver if you have Python on your machine\n - all Mac users for instance do.)\n\n\n\n\n\n\n\n\nSo here's the CND.xml, transformed into a csv: \nhttp://shawngraham.github.io/exercise/cnd.xml\n . If you 'view page source', you'll see the original XML again! Save-as the page as whatever-you-want.csv and you can do some data mining on it.\n\n\n\n\nMore on transformations\n\n\nI made a file I've called \nSG_transformer.xsl\n. Open that file in your text editor. What tags would it be looking for in the xml file? What might it do to your markup? What line would you change in your XML file to get it to point to this stylesheet? Write all this down in your open notebook. It is a good habit to get into to keep track of your thoughts when looking at ancillary files like this.\n\n\nIf the nature of your project will involve a lot of transcription, you would be well advised to use an XML editor like \nOxygenXML\n, which has a free 1 month trial. The editor makes it easy to maintain \nconsistency\n in your markup, and also, to quickly create stylesheets for whatever purpose you need. There are also a number of utility programs freely available that will convert XML to CSV or other formats. One such may be \nfound here\n. But the best way to transform these XML files is with XSL.", 
            "title": "Text Encoding"
        }, 
        {
            "location": "/supporting materials/tei/#close-reading-with-tei", 
            "text": "This worksheet, and all related files, are released CC-BY.  By M. H. Beals ; Adapted for HIST3814o by S Graham  You will need the files in  this subfolder  so download that repository as a zip file and unzip it somewhere handy on your machine.  This exercise will explore a historical text and help you create a digital record of your analysis  Vetting a Website  Visit the Recovered Histories Website at  http://www.recoveredhistories.org  Examine the site's layout and read its introduction   What makes you believe this site is a trustworthy provider of historical texts?  What makes you believe this site is NOT a trustworthy provider of historical texts?   Finding a Source  Visit the site's collections via the 'Browse' function  Locate the pamphlet  Negro Slavery  by Zachary Macaulay and open it  This is an abolitionist pamphlet regarding the Atlantic slave trade, presenting and examine evidence of how it is run.  When you approach a primary source like this, it is tempting to read through it from beginning to end, to get an overview of its contents, and then 'mine' or 'cherry-pick' good quotations to include in your assessments.  However, we are going to focus on examining a very small part of the text in a very high level of detail.    Setting Up Your Workspace  You will use your own machine rather than DHBox for this work. Arrange your workspace so that you have the scanned text of the pamphlet easily visible on one side of your screen. Open the 'blanktemplate.txt' file in  Sublime Text ,  Atom ,  Textwrangler  or  Notepadd++  (or any text editor that understands encoding) and have that on the other side of the screen.  The last lines will be /body /text /TEI /teiCorpus . Everything you write today should be just above  /body .  Transcribing Your Page  The first thing you will need is go to the tag  biblScope 1 /biblScope  and replace the number one (1) with the page number you are transcribing. Which page should you transcribe? Select a page in the document that you find interesting.  Next, you will need to  very carefully  transcribe your page of text from the image into your document.  Make sure you do not make any changes to the text, even if you think they author has used poor grammar or misspelled a word.  You do not need to worry about line breaks but should start every new paragraph (or heading) with a  p  and end every paragraph (or heading) with a  /p .  Once you have completed your transcription, look away from your computer for 30-45 seconds.  Staring into the distance every 10-20 minutes will keep your eyes from straining.  Also, shake out your hands at the wrists, to prevent repetitive stress injuries to your fingers.    Encoding Your Transcription  You are now going to  encode  or  mark-up  your text.    Re-read your page and highlight / colour the following things:   Any persons mentioned (including any he/she if they refer to a specific person)  Any places mentioned  Any claims, assertions or arguments made\nNow that you have highlighted these, you are going to put proper code around them.   For persons, surround your text with these  persName key=\"Last, First\" from=\"YYYY\" to=\"YYYY\" role=\"Occupation\" ref=\"http://www.website.com/webpage.html\"   /persName   Inside the speech marks for  key , include the real full name of the person mentioned  In  from  and  to , include their birth and death years, using ? for unknown years  In  role , put the occupation, role or 'claim to fame' for this individual.    In  ref , put the URL (link) to the Dictionary of National Biography, Wikipedia or other biography website where you found this information. If there is a   in your link, you will need to replace this with  amp;   For places, surround your text with these  placeName key=\"Sheffield, United Kingdom\" ref=\"http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield params=53_23_01_N_1_28_01_W_type:city_region:GB\"\"   /placeName   In  key , put the city and country with best information you can find for the modern names for this location  In  ref , put a link to the relevant coordinates on Wikipedia GeoHack website (http://tools.wmflabs.org/geohack/) To obtain this, go to the Wikipedia page for this city and click on the latitude/longitude coordinates for the location. For large areas, such as entire countries or continents, just use the Wikipedia page URL.   For claims or arguments, surround your text with these  interp key=\"reason\" n=\"citation\" cert=\"high\" ref=\"http://www.website.com/webpage.html\"   /interp   In  key , explain why you believe this claim is true or not  In  n , put a full citation to the relevant source  In  cert  (short for certainty), put: high, medium, low or unknown  In  ref , put the link to the website where you got the information to assess this claim.   When you are happy with your work, hit save your work, give it a useful name, make sure it has .xml as the extension, and save it  and the .xsl file  to your repository.   Alex Gill has made  The Short and Sweet TEI Handout  which you might want to explore as well. When you embark on encoding documents for your own research,  here are some questions to think about  to help you decide what kinds of tagging you'll need; these  templates from HisTEI  might be useful (open the whole project with OxygenXML for full functionality, but you can copy those templates in any editor).   Viewing Your Encoded Text  To see your encoded text, make sure your .xml and .xsl file are in the same folder.  Open either Internet Explorer or Firefox . The following will not work in Chrome because it has different security settings.  Making sure both your (page number).xml file and your 000style.xsl file are in the same folder (or both on your desktop), drag the icon for (page number).xml into your Internet Explorer Browser window.  If you now see a colour-coded version of your text, Congratulations! If you hover over the coloured sections, you should see a pop-up with the additional information you entered.  If your text comes up only in black, with no paragraph divisions or headings, or doesn't come up at all, something has gone wrong. Re-open your .xml file and check that you have:  Placed  p  at the start of every paragraph, including the start of the page  Placed  /p  at the end of every paragraph, including the end of the page  Made sure all your  persName ,  placeName  and  interp  tags are properly enclosed in  s  Made sure you have both an open   and close  \\  tag for each tag you use  Made sure you attribute values are fully enclosed in  \"\" .  Made sure you have a space between the  \"  of one attribute and the start of the next  Made sure you do NOT have a space after the  =  of an attribute  If your text still does not appear formatted, you may need to remove the text one paragraph at a time, refreshing your browser window, until it appears. This will help you identify which paragraph (or sentence) has the error within it).  If you still don't see your text  If you do not see the colour-coded version of your text, this might not necessarily mean that you've done something wrong   Some browsers will not perform the transformation, for security reasons.   In which case, here's what we can do. If you are on a Windows machine using Notepad++, go to 'Plugins'   Plugin Tools. (If you are on Windows but aren't using Notepad++, Sublime and Atom probably have a similar functionality, but you will have to search to figure it out.) Select 'XML Tools' from the list, and install it. You'll probably have to restart the program to complete the plugin installation. Open up the  1.xml  file in Notepad ++. Then, under 'plugins' 'xml tools\" select 'XSL Transformation settings'. In the popup, click on the elipses:  ...  to open up the file finder, and select the  000style.xsl  stylesheet. Click 'transform'. A new tab will open in Notepad++  with a fully-formed html file displaying your data according to the stylesheet.  Save this, and then open it in a browser!    You can also check 'validate' from the XML Tools menu in Notepad++, which will identify errors in your XML. If you're still having errors, a likely culprit might be the way your geographic URLs are encoded. Compare what you've got with what's in the  1.xml  reference document.    Advanced : If you install a  WAMP  or  MAMP  server, and put your xml and xsl files in the WWW folder, you  should  be able to see the transformation no problem at  localhost\\myxml.xml  (for example). (You can also use  Python's built in webserver if you have Python on your machine  - all Mac users for instance do.)     So here's the CND.xml, transformed into a csv:  http://shawngraham.github.io/exercise/cnd.xml  . If you 'view page source', you'll see the original XML again! Save-as the page as whatever-you-want.csv and you can do some data mining on it.   More on transformations  I made a file I've called  SG_transformer.xsl . Open that file in your text editor. What tags would it be looking for in the xml file? What might it do to your markup? What line would you change in your XML file to get it to point to this stylesheet? Write all this down in your open notebook. It is a good habit to get into to keep track of your thoughts when looking at ancillary files like this.  If the nature of your project will involve a lot of transcription, you would be well advised to use an XML editor like  OxygenXML , which has a free 1 month trial. The editor makes it easy to maintain  consistency  in your markup, and also, to quickly create stylesheets for whatever purpose you need. There are also a number of utility programs freely available that will convert XML to CSV or other formats. One such may be  found here . But the best way to transform these XML files is with XSL.", 
            "title": "Close Reading with TEI"
        }, 
        {
            "location": "/supporting materials/ner/", 
            "text": "Using the Stanford NER to tag a corpus\n\n\nIn our regular expressions example, we were able to extract some of the metadata from the document because it was more or less already formatted in such a way that we could write a pattern to find it. Sometimes however clear-cut patterns are not quite as easy to apply. For instance, what if we were interested in the place names that appear in the documents? What if we suspected that the focus of diplomatic activity shifted over time? This is where \u2018named entity recognition\u2019 can be useful. Named entity recognition covers a broad range of techniques, based on machine learning and statistical models of language to laboriously trained classifiers using dictionaries. One of the easiest to use out-of-the-box is the Stanford Named Entity Recognizer.  In essence, we tell it \u2018here is a block of text \u2013 classify!\u2019 It will then process through the text, looking at the structure of your text and matching it against its statistical models of word use to identify person, organization, and locations. One can also expand that classification to extract time, money, percent, and date. \n\n\nGrab the Stanford NER\n\n\nLet us use the NER to extract person, organization, and locations.  First, download the Stanford NER from \nhttp://nlp.stanford.edu/software/CRF-NER.shtml\n and extract it to your machine. Open the location where you extracted the files. \n+ On a Mac, double-click on the one called \u2018ner-gui.command\u2019. (Mac Users: there is also this excellent tutorial from Michelle Moravec \nyou may wish to consult\n.\n+ On PC, double-click on ner-gui.bat. This opens up a new window (using Java) with \u2018Stanford Named Entity Recognizer\u2019 and also a terminal window. \n\n\nDon\u2019t touch the terminal window for now. (\nPC users, hang on a moment\n \u2013 there is a bit more that you need to know before you can use this tool successfully. You will have to use the command line in order to get the output out).\n\n\nRunning the NER via its GUI\n\n\nIn the \u2018Stanford Named Entity Recognizer\u2019 window there is some default text. Click inside this window and delete the text. Then, click on \u2018File,\u2019 then \u2018Open,\u2019 and select your text for the diplomatic correspondence of the Republic of Texas (that you should still have from earlier modules in this course). Since this text file contains a lot of extraneous information in it \u2013 information which we are not currently interested in, including the publishing information and the index table of letters \u2013 you should open the file in a text editor first and delete that information. We want just the letters for this exercise. Save with a new name and then open it using \u2018File \n open\u2019 in the Stanford NER. The file will open within the window. In the Stanford NER window, click on \u2018classifier\u2019 then \u2018load CRF from file\u2019. Navigate to where you unzipped the Stanford NER folder. Click on the \u2018classifier\u2019 folder. There are a number of files here; the ones that you are interested in end with .gz:\n\n\nenglish.all.3class.distsim.crf.ser.gz\n\n\nenglish.all.4class.distsim.crf.ser.gz\n\n\neglish.muc.7class.distsim.crf.ser.gz\n\n\nThese files correspond to these entities to extract:\n\n\n3class:    Location, Person, Organization\n\n\n4class:    Location, Person, Organization, Misc\n\n\n7class:    Time, Location, Organization, Person, Money, Percent, Date\n\n\nSelect the location, person, and organization classifier (ie, 3class) and then press \u2018Run NER.\u2019 At this point, the program will appear to \u2018hang\u2019 \u2013 nothing much will seem to be happening. However, in the background, the program has started to process your text. Depending on the size of your text, this could take anywhere from a few minutes to a few hours. Be patient! Watch the terminal window \u2013 once the program has results for you, these will start to scroll by in the terminal window. In the main program window, once the entire text has processed, the text will appear with colour-coded highlighting showing which words are location words, which ones are persons, which ones are organizations. You have now classified a text. Note: sometimes your computer may run out of memory \u2013 in that case, you\u2019ll see an error referring to \u201cOut of Heap Space\u201d in your terminal window. That\u2019s OK \u2013 just copy and paste a smaller bit of the document, say the first 10,000 lines or so. Then try again.\n\n\nManipulating that data\n\n\nMac users can grab the data and paste it elsewhere; PC users will have to run the NER from the command line to get usable output.\n\n\nMac Users\n\n\nOn a Mac, you can copy and paste the output from the terminal window into your text editor of choice. It will look something like: \n\n\nLOCATION: Texas\n\nPERSON: Moore\n\nORGANIZATION: Suprema\n\n\nAnd so forth. Once you've pasted it into Textwrangler (or whatever text editor you use), you can now use regular expressions to manipulate the text further. More in a moment.\n\n\nPC Users\n\n\nOn a PC, things are not so simple because the command window only shows a small fraction of the complete output \u2013 you cannot copy and paste it all! What we have to do instead is type in a command at the command prompt, rather than using the graphical interface, and then redirect the output into a new text file. \n\n\n\n\nOpen a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).\n\n\nType the following as a single line:\n\n\n\n\njava -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \n \u201cmy-ner-output.txt\u201d\n\n\n\n\nThe first bit, \njava \u2013mx500m\n says how much memory to use. If you have 1gb of memory available, you can type \njava \u2013mx 1g\n (or 2g, or 3g, etc). The next part of the command calls the NER programme itself.  You can set which classifier to use after the \n\u2013loadClassifier classifiers/\n by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). At \n\u2013textFile\n you give it the name of your input file (on our machine, called \ntexas-letters.txt\n, and then specify the outputFormat.  The \n character sends the output to a new text file, here called \nmy-ner-output.txt\n.  Hit enter, and a few moments later the programme will tell you something along the lines of\n\n\nCRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second\n\n\nOpen the text file in Notepad++, and you\u2019ll see output like this:\n\n\nIn the name of the \nLOCATION\nRepublic of Texas\n/LOCATION\n, Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I \nPERSON\nSam Houston\n/PERSON\n President thereof send Greeting\n\n\nCongratulations \u2013 you\u2019ve successfully tagged a document using a named entity recognizer!\n\n\nNow you need to do a bit more data-munging before you can do anything useful. \nImagine you wanted to eventually visualize this as a network\n. You will need your regex skills again...", 
            "title": "NER"
        }, 
        {
            "location": "/supporting materials/ner/#using-the-stanford-ner-to-tag-a-corpus", 
            "text": "In our regular expressions example, we were able to extract some of the metadata from the document because it was more or less already formatted in such a way that we could write a pattern to find it. Sometimes however clear-cut patterns are not quite as easy to apply. For instance, what if we were interested in the place names that appear in the documents? What if we suspected that the focus of diplomatic activity shifted over time? This is where \u2018named entity recognition\u2019 can be useful. Named entity recognition covers a broad range of techniques, based on machine learning and statistical models of language to laboriously trained classifiers using dictionaries. One of the easiest to use out-of-the-box is the Stanford Named Entity Recognizer.  In essence, we tell it \u2018here is a block of text \u2013 classify!\u2019 It will then process through the text, looking at the structure of your text and matching it against its statistical models of word use to identify person, organization, and locations. One can also expand that classification to extract time, money, percent, and date.", 
            "title": "Using the Stanford NER to tag a corpus"
        }, 
        {
            "location": "/supporting materials/ner/#grab-the-stanford-ner", 
            "text": "Let us use the NER to extract person, organization, and locations.  First, download the Stanford NER from  http://nlp.stanford.edu/software/CRF-NER.shtml  and extract it to your machine. Open the location where you extracted the files. \n+ On a Mac, double-click on the one called \u2018ner-gui.command\u2019. (Mac Users: there is also this excellent tutorial from Michelle Moravec  you may wish to consult .\n+ On PC, double-click on ner-gui.bat. This opens up a new window (using Java) with \u2018Stanford Named Entity Recognizer\u2019 and also a terminal window.   Don\u2019t touch the terminal window for now. ( PC users, hang on a moment  \u2013 there is a bit more that you need to know before you can use this tool successfully. You will have to use the command line in order to get the output out).", 
            "title": "Grab the Stanford NER"
        }, 
        {
            "location": "/supporting materials/ner/#running-the-ner-via-its-gui", 
            "text": "In the \u2018Stanford Named Entity Recognizer\u2019 window there is some default text. Click inside this window and delete the text. Then, click on \u2018File,\u2019 then \u2018Open,\u2019 and select your text for the diplomatic correspondence of the Republic of Texas (that you should still have from earlier modules in this course). Since this text file contains a lot of extraneous information in it \u2013 information which we are not currently interested in, including the publishing information and the index table of letters \u2013 you should open the file in a text editor first and delete that information. We want just the letters for this exercise. Save with a new name and then open it using \u2018File   open\u2019 in the Stanford NER. The file will open within the window. In the Stanford NER window, click on \u2018classifier\u2019 then \u2018load CRF from file\u2019. Navigate to where you unzipped the Stanford NER folder. Click on the \u2018classifier\u2019 folder. There are a number of files here; the ones that you are interested in end with .gz:  english.all.3class.distsim.crf.ser.gz  english.all.4class.distsim.crf.ser.gz  eglish.muc.7class.distsim.crf.ser.gz  These files correspond to these entities to extract:  3class:    Location, Person, Organization  4class:    Location, Person, Organization, Misc  7class:    Time, Location, Organization, Person, Money, Percent, Date  Select the location, person, and organization classifier (ie, 3class) and then press \u2018Run NER.\u2019 At this point, the program will appear to \u2018hang\u2019 \u2013 nothing much will seem to be happening. However, in the background, the program has started to process your text. Depending on the size of your text, this could take anywhere from a few minutes to a few hours. Be patient! Watch the terminal window \u2013 once the program has results for you, these will start to scroll by in the terminal window. In the main program window, once the entire text has processed, the text will appear with colour-coded highlighting showing which words are location words, which ones are persons, which ones are organizations. You have now classified a text. Note: sometimes your computer may run out of memory \u2013 in that case, you\u2019ll see an error referring to \u201cOut of Heap Space\u201d in your terminal window. That\u2019s OK \u2013 just copy and paste a smaller bit of the document, say the first 10,000 lines or so. Then try again.", 
            "title": "Running the NER via its GUI"
        }, 
        {
            "location": "/supporting materials/ner/#manipulating-that-data", 
            "text": "Mac users can grab the data and paste it elsewhere; PC users will have to run the NER from the command line to get usable output.  Mac Users  On a Mac, you can copy and paste the output from the terminal window into your text editor of choice. It will look something like:   LOCATION: Texas \nPERSON: Moore \nORGANIZATION: Suprema  And so forth. Once you've pasted it into Textwrangler (or whatever text editor you use), you can now use regular expressions to manipulate the text further. More in a moment.  PC Users  On a PC, things are not so simple because the command window only shows a small fraction of the complete output \u2013 you cannot copy and paste it all! What we have to do instead is type in a command at the command prompt, rather than using the graphical interface, and then redirect the output into a new text file.    Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).  Type the following as a single line:   java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML   \u201cmy-ner-output.txt\u201d  The first bit,  java \u2013mx500m  says how much memory to use. If you have 1gb of memory available, you can type  java \u2013mx 1g  (or 2g, or 3g, etc). The next part of the command calls the NER programme itself.  You can set which classifier to use after the  \u2013loadClassifier classifiers/  by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). At  \u2013textFile  you give it the name of your input file (on our machine, called  texas-letters.txt , and then specify the outputFormat.  The   character sends the output to a new text file, here called  my-ner-output.txt .  Hit enter, and a few moments later the programme will tell you something along the lines of  CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second  Open the text file in Notepad++, and you\u2019ll see output like this:  In the name of the  LOCATION Republic of Texas /LOCATION , Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I  PERSON Sam Houston /PERSON  President thereof send Greeting  Congratulations \u2013 you\u2019ve successfully tagged a document using a named entity recognizer!  Now you need to do a bit more data-munging before you can do anything useful.  Imagine you wanted to eventually visualize this as a network . You will need your regex skills again...", 
            "title": "Manipulating that data"
        }, 
        {
            "location": "/supporting materials/open-refine/", 
            "text": "An Introduction to Open Refine\n\n\nThis text was adopted from the first drafts of The Macroscope\n\n\n(An alternative Open Refine exercise is offered by \nThomas Padilla\n and you may wish to give it a try instead. That would be acceptable.)\n\n\nInstall Open Refine\n\n\n\n\nOpenRefine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; extending it with web services; and linking it to databases like Freebase.\n\n\n\n\nThis exercise does not use DHBox.\n\n\nIn this exercise, we are going to use a tool that originated with Google. Since 2012, it has been open-sourced and freely available on the net. Using it takes a bit of getting used to, however. Visit the \nOpen Refine\n home page and watch the three videos. Then, \ndownload it to your machine\n\n\nFollow the installation instructions. Start Open Refine by double clicking on its icon. This will open a new browser window, pointing to \nhttp://127.0.0.1:3333\n. This location is your own computer, so even though it looks like it\u2019s running on the internet, it isn\u2019t. The \u20183333\u2019 is a \u2018port\u2019, meaning that Open Refine is running much like a server, serving up a webpage via that port to the browser. (If the browser window doesn't open automatically, open one and put \nhttp://127.0.0.1:3333\n in the address bar).\n\n\nStart Cleaning our Texan Correspondence.\n\n\n\n\n\n\nMake sure you have your data handy that you created in exercise 1, the Texan correspondence. You can get it out of your DHBox by using the DHBox filemanager. Navigate to where you were working on it, then click on the file name. This will download it to your downloads folder. Then move it to somewhere safe on your computer.\n\n\n\n\n\n\nStart a new project by clicking on the \u2018create project\u2019 tab on the left hand side of the screen. Click on \u2018choose files\u2019 and select the Texan correspondence csv file. Open Refine will load this data, and it will give you a preview of your data. Name your project in the box on the top right side (eg., 'm3-exercise2' or similar) and then click \u2018create project\u2019. It may take a few minutes.\n\n\n\n\n\n\nOnce your project has started, one of the columns that should be visible in your data is \u2018sender\u2019. Click on the arrow to the left of \"Sender\" in OpenRefine and select Facet-\nText Facet. Do the same with the arrow next to \"Recipient\". A box will appear on the left side of the browser showing all 189 names listed as senders in the spreadsheet. The spreadsheet itself is nearly a thousand rows, so immediately we see that, in this correspondence collection, some names are used multiple times. You may also have noticed that many of the names suffered from errors in the text scan (OCR or Optical Character Recognition errors), rendering some identical names from the book as similar, but not the same, in the spreadsheet. For example the recipient \"Juan de Dios Cafiedo\" is occasionally listed as \"Juan de Dios CaAedo\". Any subsequent analysis will need these errors to be cleared up, and OpenRefine will help fix them.\n\n\n\n\n\n\nWithin the \"Sender\" facet box on the left-hand side, click on the button labeled \"Cluster\". This feature presents various automatic ways of merging values that appear to be the same.   Play with the values in the drop-down boxes and notice how the number of clusters found change depending on which method is used. Because the methods are slightly different, each will present different matches that may or may not be useful. If you see two values which should be merged, e.g. \"Ashbel Smith\" and \". Ashbel Smith\", check the box to the right in the 'Merge' column and click the 'Merge Selected \n Re-Cluster' button below.\n\n\n\n\n\n\nGo through the various cluster methods one-at-a-time, including changing number values, and merge the values which appear to be the same. \"Juan de Dios CaAedo\" clearly should be merged with \"Juan de Dios Cafiedo\", however \"Correspondent in Tampico\" probably should not be merged with \"Correspondent at Vera Cruz.\" Since we are not experts, we will have to use our best judgement in these cases - or get cracking on some more research to help us make the call. By the end, you should have reduced the number of unique Senders from 189 to around 150. Repeat these steps with Recipients, reducing unique Recipients from 192 to about 160. To finish the automatic cleaning of the data, click the arrow next to \"Sender\" and select 'Edit Cells-\nCommon transforms-\nTrim leading and trailing whitespace'. Repeat for \"Recipient\". The resulting spreadsheet will not be perfect, but it will be much easier to clean by hand than it would have been before taking this step. Click on \u2018export\u2019 at the top right of the window to get your data back out as a .csv file.\n\n\n\n\n\n\nNow what?\n\n\nThe text you've just cleaned could now be loaded into something like \nPalladio\n or \nGephi\n or \nConnect the Dots\n for network analysis! However, every network analysis program has its own idiosyncracies. Gephi and Connect the Dots, for instance, can import lists of relationships if the CSV file has columns labelled 'source' and 'target' (Connect the Dots will \nonly\n accept those two columns, so you'd have to delete the date column if you wanted to give that a try). So let's assume that's where we want to visualize \n analyze this data:\n\n\n\n\n\n\nIn order to get this correspondence data into a network visualization tool, we will have to rename the \"Sender\" column to \"source\" and the \"Recipient\" column to \"target\". You could do this in a spreadsheet, of course. But since you have Open Refine running.... In the arrow to the left of Sender in the main OpenRefine window, select Edit column-\nRename this column, and rename the column \"source\". Now in the top right of the window, select 'Export-\nCustom tabular exporter'. Notice that \"source\", \"target\", and \"Date\" are checked in the content tab; uncheck \"Date\", as it will not be used in Gephi (networks where the nodes have different dates, ie dynamic networks, are beyond us for the moment). Go to the download tab and change the download option from 'Tab-separated values (TSV)' to 'Comma-separated values (CSV)' and press download.  The file will likely download to your automatic download directory. We will revisit this file later. You can drop it into the Palladio interface right now though! Go ahead and do that. Do you see any interesting patterns? Make a note!\n\n\n\n\n\n\nupload your cleaned file with a new name back \ninto\n your DHBox; we will use this in the next module.\n\n\n\n\n\n\nremember to copy your notes and any other information/observations/thoughts to your Github repo\n\n\nOPTIONAL: Going further with Open Refine: Named Entity Extraction\n\n\nSay we wanted, instead of the correspondence network, a visualization of all the places named in this body of letters. It might be interesting to visualize on a map the changing focus of Texas' diplomatic attention over time. There is a plugin for Open Refine that does what is called \nNamed Entity Extraction\n. The plugin, and how to install \n use it, is available \nhere\n.\n+ Use regex on your original document containing the letters to clean up the data so that you have one letter per line (rather than the index - did you notice that the full text of all the letters was in the original file?).\n+ Import the file into Open Refine\n+ Extract Named Entities\n+ Visualize the results in a spreadsheet\n+ Write up a 'how to' in your notebook explaining these steps in detail.\n\n\nAn interesting use case is discussed \nhere\n and \nhere\n\n\n\n\nFurther Help * \nsee this page by Kalani Craig\n\n\n\n\nOptional: Exploring Other Named Entity Extraction tools\n\n\nVoyant Tools RezoViz\n\n\nVoyant-Tools\n is a text analysis suite that we will explore in more depth in the next module. Feel free to load your material into it and begin exploring; there's nothing you can break. One interesting tool is called 'RezoViz', which will extract entities and tie them together into a network based on appearing in the same document. Upload some of your Equity texts to Voyant-Tools. In the top right, there's a 'save' icon. Select 'url for a different tool/skin'. Select 'RezoViz' from the tools list that pops up. A new URL will appear in the box. Copy, paste into a new browser window. Works best on Chrome. What kinds of questions could this answer?\n\n\nStanford NER\n\n\nStanford NER\n download.\n\n\n\n\n\n\nMac instructions for use\n. The link is to Michelle Moravec's instructions, for Mac.\n\n\n\n\n\n\nWindows: If you're on windows and want to do this, things are a bit more complicated. Download and unzip the NER package.\n\n\n\n\n\n\nOpen a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).\n\n\n\n\n\n\nChanging the file names as appropriate, type the following as a single line (highlight the text with your mouse - it scrolls to the right beyond the page, and then copy it):\n\n\n\n\n\n\njava -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \n \u201cmy-ner-output.txt\u201d\n\n\n\n\nThe first bit, java \u2013mx500m says how much memory to use. If you have 1gb of memory available, you can type java \u2013mx 1g (or 2g, or 3g, etc). The next part of the command calls the NER programme itself. You can set which classifier to use after the \u2013loadClassifier classifiers/ by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). At \u2013textFile you give it the name of your input file (on our machine, called \u2018texas-letters.txt\u2019, and then specify the outputFormat. The \n character sends the output to a new text file, here called \u201cmy-ner-output.txt\u201d. Hit enter, and a few moments later the programme will tell you something along the lines of\n\n\n\n\nCRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second\n\n\n\n\nOpen the text file in your text editor, and you\u2019ll see output like this:\n\n\n\n\nIn the name of the \nLOCATION\nRepublic of Texas\n/LOCATION\n, Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I \nPERSON\nSam Houston\n/PERSON\n President thereof send Greeting\n\n\n\n\nCongratulations! You've tagged a body of letters. What next? You could organize this into XML, you could visualize, you could regex to find and extract all of your locations, or persons, or...", 
            "title": "Open Refine"
        }, 
        {
            "location": "/supporting materials/open-refine/#an-introduction-to-open-refine", 
            "text": "This text was adopted from the first drafts of The Macroscope  (An alternative Open Refine exercise is offered by  Thomas Padilla  and you may wish to give it a try instead. That would be acceptable.)", 
            "title": "An Introduction to Open Refine"
        }, 
        {
            "location": "/supporting materials/open-refine/#install-open-refine", 
            "text": "OpenRefine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; extending it with web services; and linking it to databases like Freebase.   This exercise does not use DHBox.  In this exercise, we are going to use a tool that originated with Google. Since 2012, it has been open-sourced and freely available on the net. Using it takes a bit of getting used to, however. Visit the  Open Refine  home page and watch the three videos. Then,  download it to your machine  Follow the installation instructions. Start Open Refine by double clicking on its icon. This will open a new browser window, pointing to  http://127.0.0.1:3333 . This location is your own computer, so even though it looks like it\u2019s running on the internet, it isn\u2019t. The \u20183333\u2019 is a \u2018port\u2019, meaning that Open Refine is running much like a server, serving up a webpage via that port to the browser. (If the browser window doesn't open automatically, open one and put  http://127.0.0.1:3333  in the address bar).", 
            "title": "Install Open Refine"
        }, 
        {
            "location": "/supporting materials/open-refine/#start-cleaning-our-texan-correspondence", 
            "text": "Make sure you have your data handy that you created in exercise 1, the Texan correspondence. You can get it out of your DHBox by using the DHBox filemanager. Navigate to where you were working on it, then click on the file name. This will download it to your downloads folder. Then move it to somewhere safe on your computer.    Start a new project by clicking on the \u2018create project\u2019 tab on the left hand side of the screen. Click on \u2018choose files\u2019 and select the Texan correspondence csv file. Open Refine will load this data, and it will give you a preview of your data. Name your project in the box on the top right side (eg., 'm3-exercise2' or similar) and then click \u2018create project\u2019. It may take a few minutes.    Once your project has started, one of the columns that should be visible in your data is \u2018sender\u2019. Click on the arrow to the left of \"Sender\" in OpenRefine and select Facet- Text Facet. Do the same with the arrow next to \"Recipient\". A box will appear on the left side of the browser showing all 189 names listed as senders in the spreadsheet. The spreadsheet itself is nearly a thousand rows, so immediately we see that, in this correspondence collection, some names are used multiple times. You may also have noticed that many of the names suffered from errors in the text scan (OCR or Optical Character Recognition errors), rendering some identical names from the book as similar, but not the same, in the spreadsheet. For example the recipient \"Juan de Dios Cafiedo\" is occasionally listed as \"Juan de Dios CaAedo\". Any subsequent analysis will need these errors to be cleared up, and OpenRefine will help fix them.    Within the \"Sender\" facet box on the left-hand side, click on the button labeled \"Cluster\". This feature presents various automatic ways of merging values that appear to be the same.   Play with the values in the drop-down boxes and notice how the number of clusters found change depending on which method is used. Because the methods are slightly different, each will present different matches that may or may not be useful. If you see two values which should be merged, e.g. \"Ashbel Smith\" and \". Ashbel Smith\", check the box to the right in the 'Merge' column and click the 'Merge Selected   Re-Cluster' button below.    Go through the various cluster methods one-at-a-time, including changing number values, and merge the values which appear to be the same. \"Juan de Dios CaAedo\" clearly should be merged with \"Juan de Dios Cafiedo\", however \"Correspondent in Tampico\" probably should not be merged with \"Correspondent at Vera Cruz.\" Since we are not experts, we will have to use our best judgement in these cases - or get cracking on some more research to help us make the call. By the end, you should have reduced the number of unique Senders from 189 to around 150. Repeat these steps with Recipients, reducing unique Recipients from 192 to about 160. To finish the automatic cleaning of the data, click the arrow next to \"Sender\" and select 'Edit Cells- Common transforms- Trim leading and trailing whitespace'. Repeat for \"Recipient\". The resulting spreadsheet will not be perfect, but it will be much easier to clean by hand than it would have been before taking this step. Click on \u2018export\u2019 at the top right of the window to get your data back out as a .csv file.", 
            "title": "Start Cleaning our Texan Correspondence."
        }, 
        {
            "location": "/supporting materials/open-refine/#now-what", 
            "text": "The text you've just cleaned could now be loaded into something like  Palladio  or  Gephi  or  Connect the Dots  for network analysis! However, every network analysis program has its own idiosyncracies. Gephi and Connect the Dots, for instance, can import lists of relationships if the CSV file has columns labelled 'source' and 'target' (Connect the Dots will  only  accept those two columns, so you'd have to delete the date column if you wanted to give that a try). So let's assume that's where we want to visualize   analyze this data:    In order to get this correspondence data into a network visualization tool, we will have to rename the \"Sender\" column to \"source\" and the \"Recipient\" column to \"target\". You could do this in a spreadsheet, of course. But since you have Open Refine running.... In the arrow to the left of Sender in the main OpenRefine window, select Edit column- Rename this column, and rename the column \"source\". Now in the top right of the window, select 'Export- Custom tabular exporter'. Notice that \"source\", \"target\", and \"Date\" are checked in the content tab; uncheck \"Date\", as it will not be used in Gephi (networks where the nodes have different dates, ie dynamic networks, are beyond us for the moment). Go to the download tab and change the download option from 'Tab-separated values (TSV)' to 'Comma-separated values (CSV)' and press download.  The file will likely download to your automatic download directory. We will revisit this file later. You can drop it into the Palladio interface right now though! Go ahead and do that. Do you see any interesting patterns? Make a note!    upload your cleaned file with a new name back  into  your DHBox; we will use this in the next module.    remember to copy your notes and any other information/observations/thoughts to your Github repo  OPTIONAL: Going further with Open Refine: Named Entity Extraction  Say we wanted, instead of the correspondence network, a visualization of all the places named in this body of letters. It might be interesting to visualize on a map the changing focus of Texas' diplomatic attention over time. There is a plugin for Open Refine that does what is called  Named Entity Extraction . The plugin, and how to install   use it, is available  here .\n+ Use regex on your original document containing the letters to clean up the data so that you have one letter per line (rather than the index - did you notice that the full text of all the letters was in the original file?).\n+ Import the file into Open Refine\n+ Extract Named Entities\n+ Visualize the results in a spreadsheet\n+ Write up a 'how to' in your notebook explaining these steps in detail.  An interesting use case is discussed  here  and  here   Further Help *  see this page by Kalani Craig   Optional: Exploring Other Named Entity Extraction tools  Voyant Tools RezoViz  Voyant-Tools  is a text analysis suite that we will explore in more depth in the next module. Feel free to load your material into it and begin exploring; there's nothing you can break. One interesting tool is called 'RezoViz', which will extract entities and tie them together into a network based on appearing in the same document. Upload some of your Equity texts to Voyant-Tools. In the top right, there's a 'save' icon. Select 'url for a different tool/skin'. Select 'RezoViz' from the tools list that pops up. A new URL will appear in the box. Copy, paste into a new browser window. Works best on Chrome. What kinds of questions could this answer?  Stanford NER  Stanford NER  download.    Mac instructions for use . The link is to Michelle Moravec's instructions, for Mac.    Windows: If you're on windows and want to do this, things are a bit more complicated. Download and unzip the NER package.    Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).    Changing the file names as appropriate, type the following as a single line (highlight the text with your mouse - it scrolls to the right beyond the page, and then copy it):    java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML   \u201cmy-ner-output.txt\u201d  The first bit, java \u2013mx500m says how much memory to use. If you have 1gb of memory available, you can type java \u2013mx 1g (or 2g, or 3g, etc). The next part of the command calls the NER programme itself. You can set which classifier to use after the \u2013loadClassifier classifiers/ by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). At \u2013textFile you give it the name of your input file (on our machine, called \u2018texas-letters.txt\u2019, and then specify the outputFormat. The   character sends the output to a new text file, here called \u201cmy-ner-output.txt\u201d. Hit enter, and a few moments later the programme will tell you something along the lines of   CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second   Open the text file in your text editor, and you\u2019ll see output like this:   In the name of the  LOCATION Republic of Texas /LOCATION , Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I  PERSON Sam Houston /PERSON  President thereof send Greeting   Congratulations! You've tagged a body of letters. What next? You could organize this into XML, you could visualize, you could regex to find and extract all of your locations, or persons, or...", 
            "title": "Now what?"
        }, 
        {
            "location": "/supporting materials/regex/", 
            "text": "A gentle introduction to Regular Expressions\n\n\nthis text is adopted from the first drafts of\n The Macroscope \nwhich is was published by Imperial College Press.\n\n\nIntroduction\n\n\nA regular expression (also called regex) is a powerful tool for finding and manipulating text.  At its simplest, a regular expression is just a way of looking through texts to locate patterns. A regular expression can help you find every line that begins with a number, or every instance of an email address, or whenever a word is used even if there are slight variations in how it's spelled. As long as you can describe the pattern you're looking for, regular expressions can help you find it. Once you've found your patterns, they can then help you manipulate your text so that it fits just what you need.\n\n\nRegular expressions can look pretty complex, but once you know the basic syntax and vocabulary, simple \u2018regexes\u2019 will be easy. Regular expressions can often be used right inside the 'Find and Replace' box in many text and document editors, such as Sublime Text, Atom, or Notepad++. You cannot use Microsoft Word, however! Tr/).\n\n\n\n\nNB in text editors, you have to indicate that you wish to do a regex search. For instance, in Notepad++ when you do a search, to use regular expressions you must tick off the checkbox enabling them. Otherwise, Notepad++ will treat your search literally, looking for that exact \ntext\n rather than the \npattern\n. Similarly in Textwrangler, you need to tick off the box marked 'grep' when you bring up the search dialogue panel. In Sublime Text, you need to tick the box that has \n.*\n in the search panel to enable regular expression searches.\n\n\n\n\nPlease also note that while this information on regex basics was initially written with the text editors Notepad++ and Textwrangler in mind, all that follows applies equally to other text editors that can work with regular expressions.\n\n\nFor now, just read along. In the actual exercise, we will not be using a text editor, but you may wish to some day in the future.\n\n\nSome basic principles\n\n\n\n\nprotip: there are libraries of regular expressions, online. For example, if you want to find all postal codes, you can search \u201cregular expression Canadian postal code\u201d and learn what \u2018formula\u2019 to search for to find them\n\n\n\n\nLet's say you're looking for all the instances of \"cat\" or \"dog\" in your document. When you type the vertical bar on your keyboard (it looks like \n|\n, shift+backslash on windows keyboards), that means 'or' in regular expressions. So, if your query is dog|cat and you press 'find', it will show you the first time either dog or cat appears in your text.\n\n\nIf you want to replace every instance of either \"cat\" or \"dog\" in your document with the world \"animal\", you would open your find-and-replace box, put \ndog|cat\n in the search query, put animal in the 'replace' box, hit 'replace all', and watch your entire document fill up with references to animals instead of dogs and cats.\n\n\nThe astute reader will have noticed a problem with the instructions above; simply replacing every instance of \"dog\" or \"cat\" with \"animal\" is bound to create problems. Simple searches don't differentiate between letters and spaces, so every time \"cat\" or \"dog\" appear within words, they'll also be replaced with \"animal\". \"catch\" will become \"animalch\"; \"dogma\" will become \"animalma\"; \"certificate\" will become \"certifianimale\". In this case, the solution appears simple; put a space before and after your search query, so now it reads:\n\n\ndog | cat\n  \n\n\nWith the spaces, \"animal\" replace \"dog\" or \"cat\" only in those instances where they're definitely complete words; that is, when they're separated by spaces.\n\n\nThe even more astute reader will notice that this still does not solve our problem of replacing every instance of \"dog\" or \"cat\". What if the word comes at the beginning of a line, so it is not in front of a space? What if the word is at the end of a sentence or a clause, and thus followed by a punctuation? Luckily, in the language of regex, you can represent the beginning or end of a word using special characters.\n\n\n\\\n\n\nmeans the beginning of a word. In some programs, like TextWrangler, this is used instead:\n\n\n\\b\n\n\nso if you search for \n\\\ncat\n , (or, in TextWrangler, \n\\bcat\n )it will find \"cat\", \"catch\", and \"catsup\", but not \"copycat\", because your query searched for words beginning with \"cat\". For patterns at the end of the line, you would use:\n\n\n\\\n\n\nor in TextWrangler,\n\n\n\\b\n\n\nagain.  The remainder of this walk-through imagines that you are using Notepad++, but if you\u2019re using Textwrangler, keep this quirk in mind. If you search for\n\n\ncat\\\n\n\nit will find \"cat\" and \"copycat\", but not \"catch,\" because your query searched for words ending with -\"cat\".\n\n\nRegular expressions can be mixed, so if you wanted to find words only matching \"cat\", no matter where in the sentence, you'd search for\n\n\n\\\ncat\\\n\n\nwhich would find every instance. And, because all regular expressions can be mixed, if you searched for (in Notepad++; what would you change, if you were using TextWrangler?)\n\n\n\\\ncat|dog\\\n\n\nand replaced all with \"animal\", you would have a document that replaced all instances of \"dog\" or \"cat\" with \"animal\", no matter where in the sentence they appear. You can also search for variations within a single word using parentheses. For example if you were looking for instances of \"gray\" or \"grey\", instead of the search query\n\n\ngray|grey\n\n\nyou could type\n\n\ngr(a|e)y\n\n\ninstead. The parentheses signify a group, and like the order of operations in arithmetic, regular expressions read the parentheses before anything else. Similarly, if you wanted to find instances of either \"that dog\" or \"that cat\", you would search for:\n\n\n(that dog)|(that cat)\n\n\nNotice that the vertical bar | can appear either inside or outside the parentheses, depending on what you want to search for.\n\n\nThe period character . in regular expressions directs the search to just find any character at all. For example, if we searched for:\n\n\nd.g\n\n\nthe search would return \"dig\", \"dog\", \"dug\", and so forth.\n\n\nAnother special character from our cheat sheet, the plus + instructs the program to find any number of the previous character. If we search for\n\n\ndo+g\n\n\nit would return any words that looked like \"dog\", \"doog\", \"dooog\", and so forth. Adding parentheses before the plus would make a search for repetitions of whatever is in the parentheses, for example querying\n\n\n(do)+g\n\n\nwould return \"dog\", \"dodog\", \"dododog\", and so forth.\n\n\nCombining the plus '+' and period '.' characters can be particularly powerful in regular expressions, instructing the program to find any amount of any characters within your search. A search for\n\n\nd.+g\n\n\nfor example, might return \"dried fruits are g\", because the string begins with \"d\" and ends with \"g\", and has various characters in the middle. Searching for simply \".+\" will yield query results that are entire lines of text, because you are searching for any character, and any amount of them.\n\n\nParentheses in regular expressions are also very useful when replacing text. The text within a regular expression forms what's called a group, and the software you use to search remembers which groups you queried in order of their appearance. For example, if you search for\n\n\n(dogs)( and )(cats)\n\n\nwhich would find all instances of \"dogs and cats\" in your document, your program would remember \"dogs\" is group 1, \" and \" is group 2, and \"cats\" is group 3. Notepad++ remembers them as \n\"\\1\"\n, \n\"\\2\"\n, and \n\"\\3\"\n for each group respectively.\n\n\nIf you wanted to switch the order of \"dogs\" and \"cats\" every time the phrase \"dogs and cats\" appeared in your document, you would type\n\n\n(dogs)( and )(cats)\n\n\nin the 'find' box, and\n\n\n\\3\\2\\1\n\n\nin the 'replace' box. That would replace the entire string with group 3 (\"cats\") in the first spot, group 2 (\" and \") in the second spot, and group 1 (\"dogs\") in the last spot, thus changing the result to \"cats and dogs\".\n\n\nThe vocabulary of regular expressions is pretty large, but there are many cheat sheets for regex online (one that I sometimes use is \nhttp://regexlib.com/CheatSheet.aspx\n. Another good one is at \nhttp://docs.activestate.com/komodo/4.4/regex-intro.html\n)\n\n\nNow, continue on to the \nmain exercise", 
            "title": "Regular Expressions (regex)"
        }, 
        {
            "location": "/supporting materials/regex/#a-gentle-introduction-to-regular-expressions", 
            "text": "this text is adopted from the first drafts of  The Macroscope  which is was published by Imperial College Press.", 
            "title": "A gentle introduction to Regular Expressions"
        }, 
        {
            "location": "/supporting materials/regex/#introduction", 
            "text": "A regular expression (also called regex) is a powerful tool for finding and manipulating text.  At its simplest, a regular expression is just a way of looking through texts to locate patterns. A regular expression can help you find every line that begins with a number, or every instance of an email address, or whenever a word is used even if there are slight variations in how it's spelled. As long as you can describe the pattern you're looking for, regular expressions can help you find it. Once you've found your patterns, they can then help you manipulate your text so that it fits just what you need.  Regular expressions can look pretty complex, but once you know the basic syntax and vocabulary, simple \u2018regexes\u2019 will be easy. Regular expressions can often be used right inside the 'Find and Replace' box in many text and document editors, such as Sublime Text, Atom, or Notepad++. You cannot use Microsoft Word, however! Tr/).   NB in text editors, you have to indicate that you wish to do a regex search. For instance, in Notepad++ when you do a search, to use regular expressions you must tick off the checkbox enabling them. Otherwise, Notepad++ will treat your search literally, looking for that exact  text  rather than the  pattern . Similarly in Textwrangler, you need to tick off the box marked 'grep' when you bring up the search dialogue panel. In Sublime Text, you need to tick the box that has  .*  in the search panel to enable regular expression searches.   Please also note that while this information on regex basics was initially written with the text editors Notepad++ and Textwrangler in mind, all that follows applies equally to other text editors that can work with regular expressions.  For now, just read along. In the actual exercise, we will not be using a text editor, but you may wish to some day in the future.", 
            "title": "Introduction"
        }, 
        {
            "location": "/supporting materials/regex/#some-basic-principles", 
            "text": "protip: there are libraries of regular expressions, online. For example, if you want to find all postal codes, you can search \u201cregular expression Canadian postal code\u201d and learn what \u2018formula\u2019 to search for to find them   Let's say you're looking for all the instances of \"cat\" or \"dog\" in your document. When you type the vertical bar on your keyboard (it looks like  | , shift+backslash on windows keyboards), that means 'or' in regular expressions. So, if your query is dog|cat and you press 'find', it will show you the first time either dog or cat appears in your text.  If you want to replace every instance of either \"cat\" or \"dog\" in your document with the world \"animal\", you would open your find-and-replace box, put  dog|cat  in the search query, put animal in the 'replace' box, hit 'replace all', and watch your entire document fill up with references to animals instead of dogs and cats.  The astute reader will have noticed a problem with the instructions above; simply replacing every instance of \"dog\" or \"cat\" with \"animal\" is bound to create problems. Simple searches don't differentiate between letters and spaces, so every time \"cat\" or \"dog\" appear within words, they'll also be replaced with \"animal\". \"catch\" will become \"animalch\"; \"dogma\" will become \"animalma\"; \"certificate\" will become \"certifianimale\". In this case, the solution appears simple; put a space before and after your search query, so now it reads:  dog | cat     With the spaces, \"animal\" replace \"dog\" or \"cat\" only in those instances where they're definitely complete words; that is, when they're separated by spaces.  The even more astute reader will notice that this still does not solve our problem of replacing every instance of \"dog\" or \"cat\". What if the word comes at the beginning of a line, so it is not in front of a space? What if the word is at the end of a sentence or a clause, and thus followed by a punctuation? Luckily, in the language of regex, you can represent the beginning or end of a word using special characters.  \\  means the beginning of a word. In some programs, like TextWrangler, this is used instead:  \\b  so if you search for  \\ cat  , (or, in TextWrangler,  \\bcat  )it will find \"cat\", \"catch\", and \"catsup\", but not \"copycat\", because your query searched for words beginning with \"cat\". For patterns at the end of the line, you would use:  \\  or in TextWrangler,  \\b  again.  The remainder of this walk-through imagines that you are using Notepad++, but if you\u2019re using Textwrangler, keep this quirk in mind. If you search for  cat\\  it will find \"cat\" and \"copycat\", but not \"catch,\" because your query searched for words ending with -\"cat\".  Regular expressions can be mixed, so if you wanted to find words only matching \"cat\", no matter where in the sentence, you'd search for  \\ cat\\  which would find every instance. And, because all regular expressions can be mixed, if you searched for (in Notepad++; what would you change, if you were using TextWrangler?)  \\ cat|dog\\  and replaced all with \"animal\", you would have a document that replaced all instances of \"dog\" or \"cat\" with \"animal\", no matter where in the sentence they appear. You can also search for variations within a single word using parentheses. For example if you were looking for instances of \"gray\" or \"grey\", instead of the search query  gray|grey  you could type  gr(a|e)y  instead. The parentheses signify a group, and like the order of operations in arithmetic, regular expressions read the parentheses before anything else. Similarly, if you wanted to find instances of either \"that dog\" or \"that cat\", you would search for:  (that dog)|(that cat)  Notice that the vertical bar | can appear either inside or outside the parentheses, depending on what you want to search for.  The period character . in regular expressions directs the search to just find any character at all. For example, if we searched for:  d.g  the search would return \"dig\", \"dog\", \"dug\", and so forth.  Another special character from our cheat sheet, the plus + instructs the program to find any number of the previous character. If we search for  do+g  it would return any words that looked like \"dog\", \"doog\", \"dooog\", and so forth. Adding parentheses before the plus would make a search for repetitions of whatever is in the parentheses, for example querying  (do)+g  would return \"dog\", \"dodog\", \"dododog\", and so forth.  Combining the plus '+' and period '.' characters can be particularly powerful in regular expressions, instructing the program to find any amount of any characters within your search. A search for  d.+g  for example, might return \"dried fruits are g\", because the string begins with \"d\" and ends with \"g\", and has various characters in the middle. Searching for simply \".+\" will yield query results that are entire lines of text, because you are searching for any character, and any amount of them.  Parentheses in regular expressions are also very useful when replacing text. The text within a regular expression forms what's called a group, and the software you use to search remembers which groups you queried in order of their appearance. For example, if you search for  (dogs)( and )(cats)  which would find all instances of \"dogs and cats\" in your document, your program would remember \"dogs\" is group 1, \" and \" is group 2, and \"cats\" is group 3. Notepad++ remembers them as  \"\\1\" ,  \"\\2\" , and  \"\\3\"  for each group respectively.  If you wanted to switch the order of \"dogs\" and \"cats\" every time the phrase \"dogs and cats\" appeared in your document, you would type  (dogs)( and )(cats)  in the 'find' box, and  \\3\\2\\1  in the 'replace' box. That would replace the entire string with group 3 (\"cats\") in the first spot, group 2 (\" and \") in the second spot, and group 1 (\"dogs\") in the last spot, thus changing the result to \"cats and dogs\".  The vocabulary of regular expressions is pretty large, but there are many cheat sheets for regex online (one that I sometimes use is  http://regexlib.com/CheatSheet.aspx . Another good one is at  http://docs.activestate.com/komodo/4.4/regex-intro.html )", 
            "title": "Some basic principles"
        }, 
        {
            "location": "/supporting materials/regex/#now-continue-on-to-the-main-exercise", 
            "text": "", 
            "title": "Now, continue on to the main exercise"
        }, 
        {
            "location": "/supporting materials/regex-ner/", 
            "text": "Further Munging the output of NER\n\n\nSo, you've learned how to tag a corpus using the Stanford NER (\nhere is that exercise again, as a reminder\n). There are several ways we might like to visualize that output.\n\n\nUnfortunately, depending on what you want to \ndo\n with that data, you are going to have to go back to the data-munging cycle. Let us imagine that we wanted to visualize the locations mentioned in the letters as a kind of network.\n\n\nWe will use regular expressions to further manipulate the data into a useful source -\n target list.\n\n\nREGEX on Mac to Useful Output (PC, please skip to next section):\n\n\nWe need to organize those locations so that locations mentioned in a letter are connected to each other. To begin, we trim away any line that does not start with LOCATION. We do this by using our regular expression skills from module 3, and adding in a few more commands:\n\n\nFIND: \n^(?!.*LOCATION).*$\n\n\nand replace with nothing. We had a few new commands in there: the \n?!\n tells it to begin looking ahead for the literal phrase LOCATION, and then the dot and dollar sign let us know to end at the end of the line. In any case, this will delete everything that doesn't have the location tag in front. Now, let us also mark those blank lines we noticed as the start of a new letter.\n\n\nFIND: \n^\\s*$\n\n\nwhere:\n\n^\n marks the beginning of a line\n\n\n$\n marks the end of a line\n\n\n\\s\n indicates \u2018whitespace\u2019\n\n\n\\*\n indicates a zero or more repetition of the thing in front of it (in this case, whitespace)\n\n\nand we replace with the phrase, \u201cblankspace\u201d. Because there might be a few blank lines, you might see \u2018blankspace\u2019 in places. Where you\u2019ve got \u2018blankspaceblankspace\u2019, that\u2019s showing us the spaces between the original documents (whereas one \u2018blankspace\u2019 was where an ORGANIZATION or PERSON tag was removed).\n\n\nAt this point, we might want to remove spaces between place names and use underscores instead, so that when we finally get this into a comma separated format, places like \u2018United States\u2019 will be \u2018United_States\u2019 rather than \u2018united, states\u2019.\n\n\nFind:  \n-a single space\n\nReplace: \n\\_\n\n\nNow, we want to reintroduce the space after \u2018LOCATION:\u2019, so\n\n\nFind: \n:_\n       \n-this is a colon, followed by an underscore\n\nReplace: \n:\n \n-this is a colon, followed by a space\n\n\nNow we want to get all of the locations for a single letter into a single line. So we want to get rid of new lines and LOCATION:\n\n\nFind: \n\\n(LOCATION:)\n\nReplace:\n\n\nIt is beginning to look like a list. Let\u2019s replace \u2018blankspaceblankspace\u2019 with \u2018new-document\u2019.\n\n\nFind: \nblankspaceblankspace\n\nReplace: new-document\n\n\nAnd now let\u2019s get those single blankspace lines excised:\n\n\nFind \n\\n(blankspace)\n\nReplace:\n\n\nNow you have something that looks like this:\n\n\n\nnew-document Houston Republic_of_Texas United_States Texas\n\nnew-document Texas\n\nnew-document United_States Town_of_Columbia\n\nnew-document United_States Texas\n\nnew-document United_States United_States_of_ Republic_of_Texas\n\nnew-document New_Orleans United_States_Govern\n\nnew-document United_States Houston United_States Texas united_states\n\nnew-document United_States New_Orleans United_States\n\nnew-document Houston United_States Texas United_States\n\nnew-document New_Orleans\n\n\n\n\n\nWhy not leave \u2018new-document\u2019 in the file? It\u2019ll make a handy marker. Let\u2019s replace the spaces with commas:\n\n\nFind: \n     \n-a single space\n\nReplace: \n,\n\n\nSave your work as a *.csv file. You now have a file that you can load into a variety of other platforms or tools to perform your analysis. Of course, NER is not perfect, especially when dealing with names like \u2018Houston\u2019 that were a personal name long before they were a place name.\n\n\nREGEX on PC to Useful Output:\n\n\nOpen your tagged file in Notepad++.  There are a lot of carriage returns, line breaks, and white spaces in this document that will make our regex very complicated and will in fact break it periodically, if we try to work with it as it is. Instead, let us remove all new lines and whitespaces and the outset, and use regex to put structure back. To begin, we want to get the entire document into a single line:\n\n\nFind: \n\\n\n\nReplace with nothing.\n\n\nNotepad++ reports the number of lines in the document at the bottom of the window. It should now report lines: 1.\n\n\nLet\u2019s introduce line breaks to correspond with the original letters (ie, a line break signals a new letter), since we want to visualize the network of locations mentioned where we join places together on the basis of being mentioned in the same letter. If we examine the original document, one candidate for something to search for, to signal a new letter, could be  \nPERSON\n to \nPERSON\n but the named entity recognizer sometimes confuses persons for places or organizations; the object character recognition also makes things complicated.  Since the letters are organized chronologically, and we can see \u2018Digitized by Google\u2019 at the bottom of every page (and since no writer in the 1840s is going to use the word digitized) let\u2019s use that as our page break marker. For the most part, one letter corresponds to one page in the book. It\u2019s not ideal, but this is something that the historian will have to discuss in her methods and conclusions. Perhaps, over the seven hundred odd letters in this collection, it doesn\u2019t actually make much difference.\n\n\nFind: \n(digitized)\n\nReplace:\n\\n\\1\n\n\nYou now have on the order of 700 odd lines in the document.\n\n\nLet\u2019s find the locations and put them on individual lines, so that we can strip out everything else.\n\n\nFind: \n(LOCATION)(.\\*?)(LOCATION)\n\nReplace: \n\\n\\2\n\n\nIn the search string above, the \n.\\*\n would look for everything between the first \nlocation\n on the line and the last \nlocation\nin the line, so we would get a lot of junk. By using \n.\\*?\n we just get the text between \nlocation\nand the next \nlocation\n tag.\nW\ne need to replace the \n and \n from the location tags now, as those are characters with special meanings in our regular expressions. Turn off the regular expressions in notepad ++ by unticking the \u2018regular expression\u2019 box. Now search for \n, \n, and \\ in turn, replacing them with tildes:\n\n\nLOCATION\nTexas\n/LOCATION\n becomes \n~Texas~~~\n\n\nNow we want to delete in each line everything that isn\u2019t a location. Our locations start the line and are trailed by three tildes, so we just need to find those three tildes and everything that comes after, and delete. Turn regular expressions back on in the search window.\n\n\nFind: \n(~~~)(.*)\n\nReplace with nothing.\n\n\nOur marker for a new page in this book was the line that began with \u2018Digitized by\u2019. We need to delete that line in its entirety, leaving a blank line.\n\n\nFind: \n(Digitized)(.\\*)\n\nReplace: \n\\n\n\n\nNow we want to get those locations all in the same line again. We need to find the new line character followed by the tilde, and then delete that new line, replacing it with a comma:\n\n\nFind: \n(\\n)(~)\n\nReplace: \n,\n\n\nNow we remove extraneous blank lines.\n\n\nFind: \n\\s*$\n\nReplace with nothing.\n\n\nWe\u2019re almost there. Let\u2019s remove the comma that starts each line by searching for\n\n^,\n (remembering that the carat character indicates the start of a line) and replacing with nothing. Save this as 'cleaned-locations.csv' Congratulations! You\u2019ve taken quite complicated output and cleaned it so that every place mentioned on a single page in the original publication is now on its own line, which means you can import it into a network visualization package, a spreadsheet, or some other tool!\n\n\nNow you can upload this csv to Gephi.", 
            "title": "Going further with regex"
        }, 
        {
            "location": "/supporting materials/regex-ner/#further-munging-the-output-of-ner", 
            "text": "So, you've learned how to tag a corpus using the Stanford NER ( here is that exercise again, as a reminder ). There are several ways we might like to visualize that output.  Unfortunately, depending on what you want to  do  with that data, you are going to have to go back to the data-munging cycle. Let us imagine that we wanted to visualize the locations mentioned in the letters as a kind of network.  We will use regular expressions to further manipulate the data into a useful source -  target list.  REGEX on Mac to Useful Output (PC, please skip to next section):  We need to organize those locations so that locations mentioned in a letter are connected to each other. To begin, we trim away any line that does not start with LOCATION. We do this by using our regular expression skills from module 3, and adding in a few more commands:  FIND:  ^(?!.*LOCATION).*$  and replace with nothing. We had a few new commands in there: the  ?!  tells it to begin looking ahead for the literal phrase LOCATION, and then the dot and dollar sign let us know to end at the end of the line. In any case, this will delete everything that doesn't have the location tag in front. Now, let us also mark those blank lines we noticed as the start of a new letter.  FIND:  ^\\s*$  where: ^  marks the beginning of a line  $  marks the end of a line  \\s  indicates \u2018whitespace\u2019  \\*  indicates a zero or more repetition of the thing in front of it (in this case, whitespace)  and we replace with the phrase, \u201cblankspace\u201d. Because there might be a few blank lines, you might see \u2018blankspace\u2019 in places. Where you\u2019ve got \u2018blankspaceblankspace\u2019, that\u2019s showing us the spaces between the original documents (whereas one \u2018blankspace\u2019 was where an ORGANIZATION or PERSON tag was removed).  At this point, we might want to remove spaces between place names and use underscores instead, so that when we finally get this into a comma separated format, places like \u2018United States\u2019 will be \u2018United_States\u2019 rather than \u2018united, states\u2019.  Find:   -a single space \nReplace:  \\_  Now, we want to reintroduce the space after \u2018LOCATION:\u2019, so  Find:  :_         -this is a colon, followed by an underscore \nReplace:  :   -this is a colon, followed by a space  Now we want to get all of the locations for a single letter into a single line. So we want to get rid of new lines and LOCATION:  Find:  \\n(LOCATION:) \nReplace:  It is beginning to look like a list. Let\u2019s replace \u2018blankspaceblankspace\u2019 with \u2018new-document\u2019.  Find:  blankspaceblankspace \nReplace: new-document  And now let\u2019s get those single blankspace lines excised:  Find  \\n(blankspace) \nReplace:  Now you have something that looks like this:  \nnew-document Houston Republic_of_Texas United_States Texas \nnew-document Texas \nnew-document United_States Town_of_Columbia \nnew-document United_States Texas \nnew-document United_States United_States_of_ Republic_of_Texas \nnew-document New_Orleans United_States_Govern \nnew-document United_States Houston United_States Texas united_states \nnew-document United_States New_Orleans United_States \nnew-document Houston United_States Texas United_States \nnew-document New_Orleans   Why not leave \u2018new-document\u2019 in the file? It\u2019ll make a handy marker. Let\u2019s replace the spaces with commas:  Find:        -a single space \nReplace:  ,  Save your work as a *.csv file. You now have a file that you can load into a variety of other platforms or tools to perform your analysis. Of course, NER is not perfect, especially when dealing with names like \u2018Houston\u2019 that were a personal name long before they were a place name.  REGEX on PC to Useful Output:  Open your tagged file in Notepad++.  There are a lot of carriage returns, line breaks, and white spaces in this document that will make our regex very complicated and will in fact break it periodically, if we try to work with it as it is. Instead, let us remove all new lines and whitespaces and the outset, and use regex to put structure back. To begin, we want to get the entire document into a single line:  Find:  \\n \nReplace with nothing.  Notepad++ reports the number of lines in the document at the bottom of the window. It should now report lines: 1.  Let\u2019s introduce line breaks to correspond with the original letters (ie, a line break signals a new letter), since we want to visualize the network of locations mentioned where we join places together on the basis of being mentioned in the same letter. If we examine the original document, one candidate for something to search for, to signal a new letter, could be   PERSON  to  PERSON  but the named entity recognizer sometimes confuses persons for places or organizations; the object character recognition also makes things complicated.  Since the letters are organized chronologically, and we can see \u2018Digitized by Google\u2019 at the bottom of every page (and since no writer in the 1840s is going to use the word digitized) let\u2019s use that as our page break marker. For the most part, one letter corresponds to one page in the book. It\u2019s not ideal, but this is something that the historian will have to discuss in her methods and conclusions. Perhaps, over the seven hundred odd letters in this collection, it doesn\u2019t actually make much difference.  Find:  (digitized) \nReplace: \\n\\1  You now have on the order of 700 odd lines in the document.  Let\u2019s find the locations and put them on individual lines, so that we can strip out everything else.  Find:  (LOCATION)(.\\*?)(LOCATION) \nReplace:  \\n\\2  In the search string above, the  .\\*  would look for everything between the first  location  on the line and the last  location in the line, so we would get a lot of junk. By using  .\\*?  we just get the text between  location and the next  location  tag.\nW\ne need to replace the   and   from the location tags now, as those are characters with special meanings in our regular expressions. Turn off the regular expressions in notepad ++ by unticking the \u2018regular expression\u2019 box. Now search for  ,  , and \\ in turn, replacing them with tildes:  LOCATION Texas /LOCATION  becomes  ~Texas~~~  Now we want to delete in each line everything that isn\u2019t a location. Our locations start the line and are trailed by three tildes, so we just need to find those three tildes and everything that comes after, and delete. Turn regular expressions back on in the search window.  Find:  (~~~)(.*) \nReplace with nothing.  Our marker for a new page in this book was the line that began with \u2018Digitized by\u2019. We need to delete that line in its entirety, leaving a blank line.  Find:  (Digitized)(.\\*) \nReplace:  \\n  Now we want to get those locations all in the same line again. We need to find the new line character followed by the tilde, and then delete that new line, replacing it with a comma:  Find:  (\\n)(~) \nReplace:  ,  Now we remove extraneous blank lines.  Find:  \\s*$ \nReplace with nothing.  We\u2019re almost there. Let\u2019s remove the comma that starts each line by searching for ^,  (remembering that the carat character indicates the start of a line) and replacing with nothing. Save this as 'cleaned-locations.csv' Congratulations! You\u2019ve taken quite complicated output and cleaned it so that every place mentioned on a single page in the original publication is now on its own line, which means you can import it into a network visualization package, a spreadsheet, or some other tool!  Now you can upload this csv to Gephi.", 
            "title": "Further Munging the output of NER"
        }, 
        {
            "location": "/supporting materials/regexex/", 
            "text": "REGEX and the Republic of Texas\n\n\nRegex comes in several different flavours. A good text editor on your own computer like Sublime Text or Atom can do Regex searches and replaces from the find-and-replace box; Word cannot do that. Remember, Regex searches for \npatterns\n in the text. The correspondence of the Republic of Texas was collated into a single volume and published with a helpful index in 1911. It was scanned and OCR'd by Google, and is now available as a text file from the Internet Archive. You can see the OCR'd text at \nthis page\n. We are going to grab the index from that file, and transform it using regex.\n\n\nThere are several hundred entries in that index. You could clean them up by hand, deleting and cutting and pasting, but with the power of regex, we'll go from this:\n\n\nSam Houston to A. B. Roman, September 12, 1842 101\nSam Houston to A. B. Roman, October 29, 1842 101\nCorrespondence for 1843-1846 \u2014\nIsaac Van Zandt to Anson Jones, January 11, 1843 103\n\n\n\n\n...to nicely CSV-formatted table like this:\n\n\nSam Houston, A. B. Roman, September 12 1842\nSam Houston, A. B. Roman, October 29 1842\nIsaac Van Zandt, Anson Jones, January 11 1843\n\n\n\n\nThe change doesn't look like much, and you might think to yourself, 'hey, I could just do that by hand'. You could; but it'd take you ages, and if you made a mistake somewhere - are you sure you could do this consistently, for a couple of hours at a time? Probably not. Your time is better spent figuring out the search and replace patterns, and then setting your machine loose to implement it.\n\n\nData formatted like this could be fed into a network analysis program, for instance, or otherwise visualized and analyzed (which we will do in the next module). Regex as we are going to use in this tutorial allows us to go from unstructured to structured data. \nThere is a video at the end of this document that captures someone working through this exercise.\n\n\nGetting started\n\n\nIn the previous module, we learned how to automatically grab text from sites like Canadiana. In this particular exercise today, we'll quickly download the file using \ncurl\n (it's like wget, though there are some \ndifferences\n. It's good to know both).\n\n\n\n\nAt the command line, type \n$ curl http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt \n texas.txt\n\n\n\n\nThis downloads the txt file and saves it as texas.txt.\n\n\n\n\nOpen it with Nano and delete everything for the index of the list of letters. To select a lot of text in Nano, you set a starting point (a mark) with ctrl+shift+6 (the 'carat' symbol: ^). Then hit the down arrow on your keyboard, and you will highlight the text. When you've selected everything you want, hit ctrl + k to cut the text.\n\n\n\n\nThat is, you\u2019re looking for the table of letters, starting with \u2018Sam Houston to J. Pinckney Henderson, December 31, 1836 51\u2019 and ending with \u2018Wm. Henry Daingerfield to Ebenezer Allen, February 2, 1846 1582\u2019. Your file will now have approximately 2000 lines in it.\n\n\nNotice that there is a lot of text that we are not interested in at the moment: page numbers, headers, footers, or categories. We're going to use regular expressions to get rid of them. What we want to end up with is a spreadsheet that is arranged in three columns:\n\n\nSender, Recipient, Date\n\n\n\n\nScroll down through the text; notice there are many lines which don't include a letter, because they're either header info, or blank, or some other extraneous text. We're going to get rid of all of those lines too. We want to keep every line that has this information in it: Sender to Recipient, Month, Date, Year, Page\n\n\nWARNING: Regex can be very tricky. When I'm working with Regex, I copy and paste some of the text I'm working on into the box at \nregexr\n and fiddle with the pattern until it does what I want. In fact, spend some time looking at their examples before you go any further in this exercise.\n\n\nThe workflow\n\n\nWe start by finding every line that looks like a reference to a letter, and put a tilde (a \n~\n symbol) at the beginning of it so we know to save it for later. Next, we get rid of all the lines that don't start with tildes, so that we're left with only the relevant text. After this is done, we format the remaining text by putting commas in appropriate places, so we can import it into a spreadsheet and do further edits there.\n\n\nWe're going to use the \nsed\n and \ngrep\n commands at the command prompt in our DHBox. Sed works by first identifying text that matches a pattern, and then swapping in the text we want to have-\n\n\n$ sed 's/old text/new text/g' filename\n\n\nand grep works like this:\n\n\n$ grep 'PATTERN WE WANT' inputfile\n which will print the results to the screen. If we want to redirect the results to a new file, we add this to the end: \n outputfile\n.\n\n\nStep One\n\n\nIdentifying Lines that have Correspondence Senders and Receivers in them\n\n\nDiscussion\n Read in full before doing any manipulation of your text!\n\n\nIf you were using a text editor on your own computer\n like, for instance Notepad++, you would press ctrl-f or search-\nfind to open the find dialogue box.  In that box, go to the 'Replace' tab, and check the radio box for 'Regular expression' at the bottom of the search box. In TextWrangler, hit command+f to open the find and replace dialogue box.  Tick off the \u2018grep\u2019 radio button (which tells TextWrangler that we want to do a regex search) and the \u2018wraparound\u2019 button (which tells TextWrangler to search everywhere). In Sublime text, command+f opens the 'find' box (and shift+command+f opens find \nand\n replace). Tick the '.*' button to tell Sublime we're working with regular expressions.\n\n\nHowever, we're going to use the two commands sed and grep at the command prompt in DHBox\n.\n\n\nRemember from our basic introduction that there's a way to see if the word \"to\" appears in full. Type\n\n\n$ grep '\\bto\\b' texas.txt\n\n\n\n\nThe results print out to the screen. This command finds every instance of the word \"to\" (and not, for instance, also \u2018potato\u2019 or \u2018tomorrow\u2019 - try \ngrep 'to' texas.txt\n instead to see the difference).\n\n\nWe don't just want to find \"to\", but the entire line that contains it. We assume that every line that contains the word \u201cto\u201d in full is a line that has relevant letter information, and every line that does not is one we do not need. You learned earlier that the query \n.+\n returns any amount of text, no matter what it says. Thus, the pattern that we will build when we are ready to use the \nsed\n command (where the 's' means 'stream' and 'ed' means 'editor') will include\n\n\n .+\\bto\\b.+\n ```\n\nso that we edit every line which includes the word \nto\n in full, no matter what comes before or after it, and none of the lines which don't.\n\nAs mentioned earlier, we want to add a tilde ~ before each of the lines that look like letters, so we can save them for later. This involves the find-and-replace function, and a query identical to the one before, but with parentheses around it, so it looks like\n\n\n\n\n\n(.+\\\nto>)\n\n\n\nand the entire line is placed within a parenthetical group. Since this the first group in our search expression, we can replace that group with `\\1` and put the tilde in front of it like so: `~\\1`.\n\n+ Copy and past some of your text into [RegExr.com](http://RegExr.com). Write your regular expression (ie, what you're trying to find), and your substitution (ie, what you're replace with) in the RegExr interface. Once you're satisfied that you've got it right, we put the complete expression into our sed command:\n\n`sed -r -i.bak 's/(.+\\bto\\b.+)/~\\1/g' texas.txt`\n\nwhere:\n\n`-r` means extended regex. this saves us from having to 'escape' certain characters\n`-i.bak` means make a backup of the original input file, in case things go wrong.\n`-'s/old-pattern/newpattern/g'` is how we find and switch what we're looking for. the final g means 'globally', everywhere in the file\n`texas.txt` the filename that we're looking to change.\n\nWhen you hit enter, the computer seems to pause for a moment, and then gives you the command prompt again. Type `ls` and you'll see that a new file, `texas.txt.bak` has been created. Type `nano texas.txt` and examine the file. You should now have ~ characters at the start of each entry of the index! If for some reason you don't, or you've mangled your original file, you can replace texas.txt with the backup file you made like so:\n\n`$ mv old-file-name new-file-name` thus,\n\n`$ mv texas.txt.bak texas.txt`. Use Nano to confirm that you're back to where you needed to be, and try again.\n\n### Step Two\n\n*Removing Lines that Aren\u2019t Relevant*\n\n_Discussion_\n\nAfter running the find-and-replace, you should note your document now has most of the lines with tildes in front of it, and a few which do not. The next step is to remove all the lines that do not include a tilde. **If you were using a text editor on your own computer**, the search string to find all lines which don't begin with tildes is `\\n[^~].+ `\n\nA ``` \\n ``` at the beginning of a query searches for a new line, which means it's going to start searching at the first character of each new line.  \n\n*However, given the evolution of computing, it may well be that this won\u2019t quite work on your system*.\n\nLinux based systems use ``` \\n ``` for a new line, while Windows often uses ``` \\r\\n ```, and older Macs just use ``` \\r ````. These are the sorts of things that can drive us crazy, and so we digital historians need to keep in mind! Since this will likely cause much frustration, your safest bet will be to save a copy of what you are working on, and then experiment to see what gives you the best result. In most cases, this will be:\n\n``` \\r\\n[^~].+ ```\n\nWithin a set of square brackets ``` [] ``` the carrot ``` ^ ``` means search for anything that isn't within these brackets; in this case, the tilde ``` ~ ```. The  ```.+ ``` as before means search for all the rest of the characters in the line as well. All together, the query returns any full line which does not begin with a tilde; that is, the lines we did not mark as looking like letters.\n\nBy finding all ``` \\r\\n[^~].+ ```and replacing it with nothing, you effectively delete all the lines that don't look like the index entries. What you're left with is a series of entries, and a series of blank lines.\n\n**But DHBox makes this so much easier**\n\nWe are simply going to get grep to find all the lines that have a tilde in them, and write them to a new file:\n\n`$ grep '~' texas.txt \n index.txt`\n\nUse Nano to confirm that this is true. Wasn't that easy?\n\n### Step Three\n\n*Transforming into CSV format*\n\n_Discussion_\n\nTo turn this text file into a spreadsheet, we'll want to separate it out into one column for sender, one for recipient, and one for date, each separated by a single comma. Notice that most lines have extraneous page numbers attached to them; we can get rid of those with regular expressions. There's also usually a comma separating the month-date and the year, which we'll get rid of as well. In the end, the first line should go from looking like:\n\n``` ~Sam Houston to J. Pinckney Henderson, December 31, 1836 51 ```\n\nto\n\n``` Sam Houston, J. Pinckney Henderson, December 31 1836 ```\n\nsuch that each data point is in its own column.\n\nYou will start by removing the page number after the year and the comma between the year and the month-date. To do this, first locate the year on each line by using the regex:\n\n\n\n\n\n[0-9]{4}\n```\n\n\nWe can find any digit between 0 and 9 by searching for \n[0-9]\n, and \n{4}\nwill find four of them together. Now extend that search out by appending \n.+\n to the end of the query; as seen before, it will capture the entire rest of the line. The query\n\n\n[0-9]{4}.+\n\n\nwill return, for example, \"1836 51\", \"1839 52\", and \"1839 53\" from the first three lines of the text. We also want to capture the comma preceding the year, so add a comma and a space before the query, resulting in\n\n\n, [0-9]{4}.+\n\n\nwhich will return \", 1836 51\", \", 1839 52\", etc.\n\n\nThe next step is making the parenthetical groups which will be used to remove parts of the text with find-and-replace. In this case, we want to remove the comma and everything after year, but not the year or the space before it. Thus our query will look like:\n\n\n(,)( [0-9]{4})(.+)\n\n\nwith the comma as the first group \n\"\\1\"\n, the space and the year as the second \n\"\\2\"\n, and the rest of the line as the third \n\"\\3\"\n.  Given that all we care about retaining is the second group (we want to keep the year, but not the comma or the page number), what will the \nreplace\n look like?\n\n\n\n\nFind the dates using a regex, and replace so that only the \nsecond\n group in the expression is kept. You might want to consult the introduction to regex again before you execute this one.\n\n\n\n\nRemember, the first part of the sed command will be: \nsed -r -i.bak\n then the pattern to find, the pattern to replace with, and the file name. You want to use sed on the new index.txt file you made. Can you devise the right pattern?\n\n\nStep Four\n\n\nRemoving the tildes\n\n\n\n\nFind the tildes that we used to mark off our text of interest, and replace them with nothing to delete them.\n\n\n\n\nStep Five\n\n\nSeparating Senders and Receivers\n\n\nDiscussion\n\n\nFinally, to separate the sender and recipient by a comma, we find all instances of the word \"to\" and replace it with a comma. Although we used \n\\b\n and \n\\b\n to denote the beginning and end of a word earlier in the lesson, we don't exactly do that here. We include the space preceding \u201cto\u201d in the regular expression, as well as the \n\\b\n to denote the word ending. Once we find instances of the word and the space preceding it, \nto\\b\n we replace it with a comma \n,\n.\n\n\n\n\nDevise the regex to find the word, and replace with a comma.\n\n\n\n\nStep Six\n\n\nCleaning up\n\n\nDiscussion\n\n\nYou may notice that some lines still do not fit our criteria. Line 22, for example, reads \"Abner S. Lipscomb, James Hamilton and A. T. Bumley, AugUHt 15, \". It has an incomplete date; these we don't need to worry about for our purposes. More worrisome are lines, like 61 \"Copy and summary of instructions United States Department of State, \" which include none of the information we want. We can get rid of these lines later in a spreadsheet.  \n\n\nThe only non-standard lines we need to worry about with regular expressions are the ones with more than 2 commas, like line 178, \"A. J. Donelson, Secretary of State [Allen,. arf interim], December 10 1844\". Notice that our second column, the name of the recipient, has a comma inside of it. If you were to import this directly into a spreadsheet, you would get four columns, one for sender, two for recipient, and one for date, which would break any analysis you would then like to run. Unfortunately these lines need to be fixed by hand, but happily regular expressions make finding them easy. The query:\n\n\n.+,.+,.+,\n\n\nwill show you every line with more than 2 commas, because it finds any line that has any set of characters, then a comma, then any other set, then another comma, and so forth. Use Grep to find these.  \n\n\n\n\nAt the top of the file, add a new line that simply reads \"Sender, Recipient, Date\". These will be the column headers. Make a copy as a csv file by using the \ncp\n command: \ncp index.txt cleaned-correspondence.csv\n.\n\n\n\n\nCongratulations!\n\n\nYou've now used regex to extract, transform, and clean historical text. As a csv file, you could now load this data into a network analysis program such as \nGephi\n to explore the ramifications of this correspondence network. Upload your file to your repository, and make a note of the original location of the file, the transformations that you've done, and the date/time. You will be using your \ncleaned-correspondence.csv\n file in the next exercise using \nOpen Refine\n, where we'll sort out some of the messy OCR (fixing names, and so on).\n\n\nHi - the pattern you want in step three is \nsed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt\n. The pattern for step four is \nsed -r -i.bak 's/~//g' index.txt\n. The pattern for step five is: \nsed -r -i.bak 's/(\\b to \\b)/,/g' index.txt", 
            "title": "Regex & the Republic of Texas"
        }, 
        {
            "location": "/supporting materials/regexex/#regex-and-the-republic-of-texas", 
            "text": "Regex comes in several different flavours. A good text editor on your own computer like Sublime Text or Atom can do Regex searches and replaces from the find-and-replace box; Word cannot do that. Remember, Regex searches for  patterns  in the text. The correspondence of the Republic of Texas was collated into a single volume and published with a helpful index in 1911. It was scanned and OCR'd by Google, and is now available as a text file from the Internet Archive. You can see the OCR'd text at  this page . We are going to grab the index from that file, and transform it using regex.  There are several hundred entries in that index. You could clean them up by hand, deleting and cutting and pasting, but with the power of regex, we'll go from this:  Sam Houston to A. B. Roman, September 12, 1842 101\nSam Houston to A. B. Roman, October 29, 1842 101\nCorrespondence for 1843-1846 \u2014\nIsaac Van Zandt to Anson Jones, January 11, 1843 103  ...to nicely CSV-formatted table like this:  Sam Houston, A. B. Roman, September 12 1842\nSam Houston, A. B. Roman, October 29 1842\nIsaac Van Zandt, Anson Jones, January 11 1843  The change doesn't look like much, and you might think to yourself, 'hey, I could just do that by hand'. You could; but it'd take you ages, and if you made a mistake somewhere - are you sure you could do this consistently, for a couple of hours at a time? Probably not. Your time is better spent figuring out the search and replace patterns, and then setting your machine loose to implement it.  Data formatted like this could be fed into a network analysis program, for instance, or otherwise visualized and analyzed (which we will do in the next module). Regex as we are going to use in this tutorial allows us to go from unstructured to structured data.  There is a video at the end of this document that captures someone working through this exercise.", 
            "title": "REGEX and the Republic of Texas"
        }, 
        {
            "location": "/supporting materials/regexex/#getting-started", 
            "text": "In the previous module, we learned how to automatically grab text from sites like Canadiana. In this particular exercise today, we'll quickly download the file using  curl  (it's like wget, though there are some  differences . It's good to know both).   At the command line, type  $ curl http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt   texas.txt   This downloads the txt file and saves it as texas.txt.   Open it with Nano and delete everything for the index of the list of letters. To select a lot of text in Nano, you set a starting point (a mark) with ctrl+shift+6 (the 'carat' symbol: ^). Then hit the down arrow on your keyboard, and you will highlight the text. When you've selected everything you want, hit ctrl + k to cut the text.   That is, you\u2019re looking for the table of letters, starting with \u2018Sam Houston to J. Pinckney Henderson, December 31, 1836 51\u2019 and ending with \u2018Wm. Henry Daingerfield to Ebenezer Allen, February 2, 1846 1582\u2019. Your file will now have approximately 2000 lines in it.  Notice that there is a lot of text that we are not interested in at the moment: page numbers, headers, footers, or categories. We're going to use regular expressions to get rid of them. What we want to end up with is a spreadsheet that is arranged in three columns:  Sender, Recipient, Date  Scroll down through the text; notice there are many lines which don't include a letter, because they're either header info, or blank, or some other extraneous text. We're going to get rid of all of those lines too. We want to keep every line that has this information in it: Sender to Recipient, Month, Date, Year, Page  WARNING: Regex can be very tricky. When I'm working with Regex, I copy and paste some of the text I'm working on into the box at  regexr  and fiddle with the pattern until it does what I want. In fact, spend some time looking at their examples before you go any further in this exercise.", 
            "title": "Getting started"
        }, 
        {
            "location": "/supporting materials/regexex/#the-workflow", 
            "text": "We start by finding every line that looks like a reference to a letter, and put a tilde (a  ~  symbol) at the beginning of it so we know to save it for later. Next, we get rid of all the lines that don't start with tildes, so that we're left with only the relevant text. After this is done, we format the remaining text by putting commas in appropriate places, so we can import it into a spreadsheet and do further edits there.  We're going to use the  sed  and  grep  commands at the command prompt in our DHBox. Sed works by first identifying text that matches a pattern, and then swapping in the text we want to have-  $ sed 's/old text/new text/g' filename  and grep works like this:  $ grep 'PATTERN WE WANT' inputfile  which will print the results to the screen. If we want to redirect the results to a new file, we add this to the end:   outputfile .  Step One  Identifying Lines that have Correspondence Senders and Receivers in them  Discussion  Read in full before doing any manipulation of your text!  If you were using a text editor on your own computer  like, for instance Notepad++, you would press ctrl-f or search- find to open the find dialogue box.  In that box, go to the 'Replace' tab, and check the radio box for 'Regular expression' at the bottom of the search box. In TextWrangler, hit command+f to open the find and replace dialogue box.  Tick off the \u2018grep\u2019 radio button (which tells TextWrangler that we want to do a regex search) and the \u2018wraparound\u2019 button (which tells TextWrangler to search everywhere). In Sublime text, command+f opens the 'find' box (and shift+command+f opens find  and  replace). Tick the '.*' button to tell Sublime we're working with regular expressions.  However, we're going to use the two commands sed and grep at the command prompt in DHBox .  Remember from our basic introduction that there's a way to see if the word \"to\" appears in full. Type  $ grep '\\bto\\b' texas.txt  The results print out to the screen. This command finds every instance of the word \"to\" (and not, for instance, also \u2018potato\u2019 or \u2018tomorrow\u2019 - try  grep 'to' texas.txt  instead to see the difference).  We don't just want to find \"to\", but the entire line that contains it. We assume that every line that contains the word \u201cto\u201d in full is a line that has relevant letter information, and every line that does not is one we do not need. You learned earlier that the query  .+  returns any amount of text, no matter what it says. Thus, the pattern that we will build when we are ready to use the  sed  command (where the 's' means 'stream' and 'ed' means 'editor') will include   .+\\bto\\b.+\n ```\n\nso that we edit every line which includes the word  to  in full, no matter what comes before or after it, and none of the lines which don't.\n\nAs mentioned earlier, we want to add a tilde ~ before each of the lines that look like letters, so we can save them for later. This involves the find-and-replace function, and a query identical to the one before, but with parentheses around it, so it looks like  (.+\\ to>)  \nand the entire line is placed within a parenthetical group. Since this the first group in our search expression, we can replace that group with `\\1` and put the tilde in front of it like so: `~\\1`.\n\n+ Copy and past some of your text into [RegExr.com](http://RegExr.com). Write your regular expression (ie, what you're trying to find), and your substitution (ie, what you're replace with) in the RegExr interface. Once you're satisfied that you've got it right, we put the complete expression into our sed command:\n\n`sed -r -i.bak 's/(.+\\bto\\b.+)/~\\1/g' texas.txt`\n\nwhere:\n\n`-r` means extended regex. this saves us from having to 'escape' certain characters\n`-i.bak` means make a backup of the original input file, in case things go wrong.\n`-'s/old-pattern/newpattern/g'` is how we find and switch what we're looking for. the final g means 'globally', everywhere in the file\n`texas.txt` the filename that we're looking to change.\n\nWhen you hit enter, the computer seems to pause for a moment, and then gives you the command prompt again. Type `ls` and you'll see that a new file, `texas.txt.bak` has been created. Type `nano texas.txt` and examine the file. You should now have ~ characters at the start of each entry of the index! If for some reason you don't, or you've mangled your original file, you can replace texas.txt with the backup file you made like so:\n\n`$ mv old-file-name new-file-name` thus,\n\n`$ mv texas.txt.bak texas.txt`. Use Nano to confirm that you're back to where you needed to be, and try again.\n\n### Step Two\n\n*Removing Lines that Aren\u2019t Relevant*\n\n_Discussion_\n\nAfter running the find-and-replace, you should note your document now has most of the lines with tildes in front of it, and a few which do not. The next step is to remove all the lines that do not include a tilde. **If you were using a text editor on your own computer**, the search string to find all lines which don't begin with tildes is `\\n[^~].+ `\n\nA ``` \\n ``` at the beginning of a query searches for a new line, which means it's going to start searching at the first character of each new line.  \n\n*However, given the evolution of computing, it may well be that this won\u2019t quite work on your system*.\n\nLinux based systems use ``` \\n ``` for a new line, while Windows often uses ``` \\r\\n ```, and older Macs just use ``` \\r ````. These are the sorts of things that can drive us crazy, and so we digital historians need to keep in mind! Since this will likely cause much frustration, your safest bet will be to save a copy of what you are working on, and then experiment to see what gives you the best result. In most cases, this will be:\n\n``` \\r\\n[^~].+ ```\n\nWithin a set of square brackets ``` [] ``` the carrot ``` ^ ``` means search for anything that isn't within these brackets; in this case, the tilde ``` ~ ```. The  ```.+ ``` as before means search for all the rest of the characters in the line as well. All together, the query returns any full line which does not begin with a tilde; that is, the lines we did not mark as looking like letters.\n\nBy finding all ``` \\r\\n[^~].+ ```and replacing it with nothing, you effectively delete all the lines that don't look like the index entries. What you're left with is a series of entries, and a series of blank lines.\n\n**But DHBox makes this so much easier**\n\nWe are simply going to get grep to find all the lines that have a tilde in them, and write them to a new file:\n\n`$ grep '~' texas.txt   index.txt`\n\nUse Nano to confirm that this is true. Wasn't that easy?\n\n### Step Three\n\n*Transforming into CSV format*\n\n_Discussion_\n\nTo turn this text file into a spreadsheet, we'll want to separate it out into one column for sender, one for recipient, and one for date, each separated by a single comma. Notice that most lines have extraneous page numbers attached to them; we can get rid of those with regular expressions. There's also usually a comma separating the month-date and the year, which we'll get rid of as well. In the end, the first line should go from looking like:\n\n``` ~Sam Houston to J. Pinckney Henderson, December 31, 1836 51 ```\n\nto\n\n``` Sam Houston, J. Pinckney Henderson, December 31 1836 ```\n\nsuch that each data point is in its own column.\n\nYou will start by removing the page number after the year and the comma between the year and the month-date. To do this, first locate the year on each line by using the regex:  [0-9]{4}\n```  We can find any digit between 0 and 9 by searching for  [0-9] , and  {4} will find four of them together. Now extend that search out by appending  .+  to the end of the query; as seen before, it will capture the entire rest of the line. The query  [0-9]{4}.+  will return, for example, \"1836 51\", \"1839 52\", and \"1839 53\" from the first three lines of the text. We also want to capture the comma preceding the year, so add a comma and a space before the query, resulting in  , [0-9]{4}.+  which will return \", 1836 51\", \", 1839 52\", etc.  The next step is making the parenthetical groups which will be used to remove parts of the text with find-and-replace. In this case, we want to remove the comma and everything after year, but not the year or the space before it. Thus our query will look like:  (,)( [0-9]{4})(.+)  with the comma as the first group  \"\\1\" , the space and the year as the second  \"\\2\" , and the rest of the line as the third  \"\\3\" .  Given that all we care about retaining is the second group (we want to keep the year, but not the comma or the page number), what will the  replace  look like?   Find the dates using a regex, and replace so that only the  second  group in the expression is kept. You might want to consult the introduction to regex again before you execute this one.   Remember, the first part of the sed command will be:  sed -r -i.bak  then the pattern to find, the pattern to replace with, and the file name. You want to use sed on the new index.txt file you made. Can you devise the right pattern?  Step Four  Removing the tildes   Find the tildes that we used to mark off our text of interest, and replace them with nothing to delete them.   Step Five  Separating Senders and Receivers  Discussion  Finally, to separate the sender and recipient by a comma, we find all instances of the word \"to\" and replace it with a comma. Although we used  \\b  and  \\b  to denote the beginning and end of a word earlier in the lesson, we don't exactly do that here. We include the space preceding \u201cto\u201d in the regular expression, as well as the  \\b  to denote the word ending. Once we find instances of the word and the space preceding it,  to\\b  we replace it with a comma  , .   Devise the regex to find the word, and replace with a comma.   Step Six  Cleaning up  Discussion  You may notice that some lines still do not fit our criteria. Line 22, for example, reads \"Abner S. Lipscomb, James Hamilton and A. T. Bumley, AugUHt 15, \". It has an incomplete date; these we don't need to worry about for our purposes. More worrisome are lines, like 61 \"Copy and summary of instructions United States Department of State, \" which include none of the information we want. We can get rid of these lines later in a spreadsheet.    The only non-standard lines we need to worry about with regular expressions are the ones with more than 2 commas, like line 178, \"A. J. Donelson, Secretary of State [Allen,. arf interim], December 10 1844\". Notice that our second column, the name of the recipient, has a comma inside of it. If you were to import this directly into a spreadsheet, you would get four columns, one for sender, two for recipient, and one for date, which would break any analysis you would then like to run. Unfortunately these lines need to be fixed by hand, but happily regular expressions make finding them easy. The query:  .+,.+,.+,  will show you every line with more than 2 commas, because it finds any line that has any set of characters, then a comma, then any other set, then another comma, and so forth. Use Grep to find these.     At the top of the file, add a new line that simply reads \"Sender, Recipient, Date\". These will be the column headers. Make a copy as a csv file by using the  cp  command:  cp index.txt cleaned-correspondence.csv .   Congratulations!  You've now used regex to extract, transform, and clean historical text. As a csv file, you could now load this data into a network analysis program such as  Gephi  to explore the ramifications of this correspondence network. Upload your file to your repository, and make a note of the original location of the file, the transformations that you've done, and the date/time. You will be using your  cleaned-correspondence.csv  file in the next exercise using  Open Refine , where we'll sort out some of the messy OCR (fixing names, and so on).  Hi - the pattern you want in step three is  sed -r -i.bak 's/(,)( [0-9]{4})(.+)/\\2/g' index.txt . The pattern for step four is  sed -r -i.bak 's/~//g' index.txt . The pattern for step five is:  sed -r -i.bak 's/(\\b to \\b)/,/g' index.txt", 
            "title": "The workflow"
        }, 
        {
            "location": "/supporting materials/cyoa.txt/", 
            "text": "Choose your own adventure!\n\n\nIn this exercise, I want \nyou\n to pick a text or network analysis tool from \nthis list\n or \nthis one\n and figure out how to use it. You can use our version of the \nColonial Newspaper Database\n as your source text. Write up your own basic tutorial explaining what the tool does, how you got it to work on your data, and what you think it might be telling us or be useful for. Fork and add the link to your tutorial (which you'll have as a .md file in your repository) to this document.", 
            "title": "CYOA"
        }, 
        {
            "location": "/supporting materials/cyoa.txt/#choose-your-own-adventure", 
            "text": "In this exercise, I want  you  to pick a text or network analysis tool from  this list  or  this one  and figure out how to use it. You can use our version of the  Colonial Newspaper Database  as your source text. Write up your own basic tutorial explaining what the tool does, how you got it to work on your data, and what you think it might be telling us or be useful for. Fork and add the link to your tutorial (which you'll have as a .md file in your repository) to this document.", 
            "title": "Choose your own adventure!"
        }, 
        {
            "location": "/supporting materials/geoparsing-w-python.txt/", 
            "text": "Geoparsing with Python\n\n\nThis exercise draws from the work of \nFred Gibbs\n\n\n\n\nextract, transform, and save as csv\n\n\nextract geocoded placenames from a text file\n\n\ncreate a kml file with python\n\n\n\n\nIn this exercise, you will need to \nhave Python installed on your machine; download here\n. I have version 2.7.9 on this machine, and know that what follows works with that version.\n\n\nYou should also read and understand Fred Gibbs' tutorial on \ninstalling python modules\n because you will need to install some helper modules.\n\n\nIn module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like \nthis\n.\n\n\n\n\nUse Openrefine to open that csv file. In the same way you tidied up in the \nopenrefine tutorial in module 3\n, clean up this CSV so that you merge together place names appropriately (ie, so that '4ustin' gets merged with 'Austin'). Do this for all the columns.\n\n\nExport the table as a new csv - call it 'cleaned-places.csv'.\n\n\nOpen that csv in your spreadsheet program. Copy and paste all of the columns so that they become a single list. (ie, one column of place names).\n\n\nUsing your spreadsheet's filtering options, see if you can remove any more duplicates. (It might be useful to keep track of how many duplicates you delete, in a new file, eg -\n Texas,200 \n- that kind of information might be handy, as in \nthe mapping texts project\n).\n\n\nSave the file you were removing the duplicates from (which has just a single column of unique place names) as 'placelist.txt'\n\n\n\n\nNow, at this point, we're going to open up our text editor and create a new python program, following Gibbs' \ntutorial\n. His complete script is at the bottom of his post, but make sure you understand everything that is going on. Do you see the places where he has to import new python modules to make his script work? Make sure you've installed those modules. Let's call your completed script 'geoparse.py'.\n\n\nDone that? Good. Open your terminal, navigate to the folder you're working in, and run your script:\n\n\npython geoparse.py\n\n\nDid it work? Did you get an error message? It's entirely possible that you got this message:\n\n\nTraceback (most recent call last):\n  File \ngeolocate.py\n, line 14, in \nmodule\n\n    lat = json['results'][0]['geometry']['location']['lat']\nIndexError: list index out of range\n\n\n\n\n...but check your folder. Do you have a \ngeocoded-places.txt\n file? If so, it worked! Or at least, it got most of your places from the Google maps api. (For the rest, you can try uploading your places list to \nhttp://scargill.inf.ed.ac.uk/geoparser.html\n and then copying and pasting the output to an excel file. This parser will give you several columns of results, where the first column represents its best guess and the other columns other possibilities).\n\n\nYou can now import your geocoded places into many other software packages. Gibbs also shows us how to convert our list into KML, the format that Google Earth and Google Maps can read. \nTry it now\n. You can double-click on the resulting KML file, and if you have Google Earth installed, it will open up there. Within google earth, you can start adding more information, other annotations... pretty soon, you'll have a complete map!\n\n\nRemember to upload your scripts \n data \n obersvations to your open notebook.\n\n\n(Incidentally, if you wanted to load this material into \nPalladio\n you'd need a file that looked like this:\n\n\nPlace   Coordinates\n\nMEXICO  23.634501,-102.552784\n\nCalifornia  36.778261,-119.4179324\n\nBrazos  32.661389,-98.121667\n\n\netc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude. Best way to effect this transformation? Probably using Regex. It's unfortunate that Palladio doesn't accept straightforward place,latitude,longitude comma separated data).", 
            "title": "Geoparsing with Python"
        }, 
        {
            "location": "/supporting materials/geoparsing-w-python.txt/#geoparsing-with-python", 
            "text": "This exercise draws from the work of  Fred Gibbs   extract, transform, and save as csv  extract geocoded placenames from a text file  create a kml file with python   In this exercise, you will need to  have Python installed on your machine; download here . I have version 2.7.9 on this machine, and know that what follows works with that version.  You should also read and understand Fred Gibbs' tutorial on  installing python modules  because you will need to install some helper modules.  In module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like  this .   Use Openrefine to open that csv file. In the same way you tidied up in the  openrefine tutorial in module 3 , clean up this CSV so that you merge together place names appropriately (ie, so that '4ustin' gets merged with 'Austin'). Do this for all the columns.  Export the table as a new csv - call it 'cleaned-places.csv'.  Open that csv in your spreadsheet program. Copy and paste all of the columns so that they become a single list. (ie, one column of place names).  Using your spreadsheet's filtering options, see if you can remove any more duplicates. (It might be useful to keep track of how many duplicates you delete, in a new file, eg -  Texas,200  - that kind of information might be handy, as in  the mapping texts project ).  Save the file you were removing the duplicates from (which has just a single column of unique place names) as 'placelist.txt'   Now, at this point, we're going to open up our text editor and create a new python program, following Gibbs'  tutorial . His complete script is at the bottom of his post, but make sure you understand everything that is going on. Do you see the places where he has to import new python modules to make his script work? Make sure you've installed those modules. Let's call your completed script 'geoparse.py'.  Done that? Good. Open your terminal, navigate to the folder you're working in, and run your script:  python geoparse.py  Did it work? Did you get an error message? It's entirely possible that you got this message:  Traceback (most recent call last):\n  File  geolocate.py , line 14, in  module \n    lat = json['results'][0]['geometry']['location']['lat']\nIndexError: list index out of range  ...but check your folder. Do you have a  geocoded-places.txt  file? If so, it worked! Or at least, it got most of your places from the Google maps api. (For the rest, you can try uploading your places list to  http://scargill.inf.ed.ac.uk/geoparser.html  and then copying and pasting the output to an excel file. This parser will give you several columns of results, where the first column represents its best guess and the other columns other possibilities).  You can now import your geocoded places into many other software packages. Gibbs also shows us how to convert our list into KML, the format that Google Earth and Google Maps can read.  Try it now . You can double-click on the resulting KML file, and if you have Google Earth installed, it will open up there. Within google earth, you can start adding more information, other annotations... pretty soon, you'll have a complete map!  Remember to upload your scripts   data   obersvations to your open notebook.  (Incidentally, if you wanted to load this material into  Palladio  you'd need a file that looked like this:  Place   Coordinates \nMEXICO  23.634501,-102.552784 \nCalifornia  36.778261,-119.4179324 \nBrazos  32.661389,-98.121667  etc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude. Best way to effect this transformation? Probably using Regex. It's unfortunate that Palladio doesn't accept straightforward place,latitude,longitude comma separated data).", 
            "title": "Geoparsing with Python"
        }, 
        {
            "location": "/supporting materials/gephi.txt/", 
            "text": "Working with Gephi\n\n\nBefore we go much further, I would recommend that you look at the work of Clement Levallois, who has a suite of excellent tutorials on working with Gephi at \nhttp://clementlevallois.net/gephi.html\n. The tutorial below is adapted from our open draft of \nThe Macroscope\n.\n\n\nIntroduction\n\n\nGephi is quickly becoming the tool of choice for network analysts who do not need the full suite of algorithms offered by \nPajek\n or \nUCINET\n. It is relatively easy to use (eclipsed in this only by \nNodeXL\n), it is usable on all platforms, it can analyze fairly large networks, and it creates beautiful visualizations. The development community is also extremely active, with improvements being added constantly. We recommend Gephi for the majority of historians undertaking serious network analysis research. Gephi is available at \nhttp://gephi.github.io\n. Download and install it on your machine.\n\n\nWhen Not To Use Networks\n\n\nNetworks analysis can be a powerful method for engaging with a historical dataset but it is often not the most appropriate. Any historical database may be represented as a network with enough contriving but few should be. One could take the Atlantic trade network, including cities, ships, relevant governments and corporations, and connect them all together in a giant multipartite network. It would be extremely difficult to derive any meaning from this network, especially using traditional network analysis methodologies. Clustering coefficients (a useful network metric), for example, would be meaningless in this situation, as it would be impossible for the neighbors of any one node to be neighbors with one another. Network scientists have developed algorithms for \nmultimodal networks\n, but they are often created for fairly specific purposes, and one should be very careful before applying them to a different dataset and using that analysis to explore a historiographical question.\n\n\nNetworks, as they are commonly encoded, also suffer from a profound lack of nuance. It is all well to say that, because Henry Oldenburg corresponded with both Gottfried Leibniz and John Milton, he was the short connection between the two men. However, the encoded network holds no information of whether Oldenburg actually transferred information between the two or whether that connection was at all meaningful. In a flight network, Las Vegas and Atlanta might both have very high betweenness centrality because people from many other cities fly to or from those airports, and yet one is much more of a hub than the other. This is because, largely, people tend to fly directly back and forth from Las Vegas, rather than through it, whereas people tend to fly through Atlanta from one city to another. This is the type of information that network analysis, as it is currently used, is not well equipped to handle. Whether this shortcoming affects a historical inquiry depends primarily on the inquiry itself.\n\n\nWhen deciding whether to use networks to explore a historiographical question, the first question should be: to what extent is connectivity specifically important to this research project? The next should be: can any of the network analyses my collaborators or I know how to employ be specifically useful in the research project? If either answer is negative, another approach is probably warranted.\n\n\nTexan Correspondence\n\n\nYou will need the information you created in Module 3, after cleaning the correspondence data using Open Refine: \n\n\n\n\nIn order to get these data into Gephi, we will have to rename the \u201cSender\u201d column to \u201csource\u201d and the \u201cRecipient\u201d column to \u201ctarget\u201d. In the arrow to the left of Sender in the main OpenRefine window, select Edit column-\nRename this column, and rename the column \u201csource\u201d. Now, in the top right of the window, select \u201cExport-\nCustom tabular exporter.\u201d Notice that \u201csource,\u201d \u201ctarget,\u201d and \u201cDate\u201d are checked in the content tab; uncheck \u201cDate,\u201d as it will not be used in Gephi. Go to the download tab and change the download option from \u201cTab-separated values (TSV)\u201d to \u201cComma-separated values (CSV)\u201d and press download.   The file will likely download to your automatic download directory.\n\n\n\n\n(but see below)\n\n\nQuick instructions for getting the data into Gephi:\n\n\nOpen Gephi by double-clicking its icon. Click \u201cnew project.\u201d The middle pane of the interface window is the \u201cData Laboratory,\u201d where you can interact with your network data in spreadsheet format. This is where we can import the data cleaned up in OpenRefine. \n\n\nIn the Data Laboratory, select \u201cImport Spreadsheet.\u201d Press the ellipsis \u201c...\u201d and locate the CSV you created. Make sure that the Separator is listed as \u201cComma\u201d and the \u201cAs table\u201d is listed as \u201cEdges table.\u201d Press \u201cNext,\u201d then \u201cFinish.\u201d\n\n\nYour data should load up. Click on the \u201coverview\u201d tab and you will be presented with a tangled network graph. Skip down to 'Navigating Gephi'.\n\n\nUmm, I never did manage that openrefine stuff...\n\n\nIn module 3, you used Notepad++ or Textwrangler, regular expressions, and OpenRefine to create a comma-separated value file (*.csv) of the diplomatic correspondence of the Republic of Texas. The final version of the file you created has a row for each letter listed in the volume, and in each row the name of the sender and the recipient. The file should look like this:\n\n\nsource,target\nSam Houston,J. Pinckney Henderson\nJames Webb,Alc6e La Branche\nDavid G. Burnet,Richard G. Dunlap\n\n...\n\n\nThis file is called 'an edge list' - it's a list of connections, or edges, between the individuals. If you no longer have the file, you can find it online \nhere\n. \n\n\nInstalling Gephi on OS 10 Mavericks\n\n\nMac users might have some trouble installing Gephi 0.8 (as I write this, the release of Gephi 0.9 \nshould happen any day\n, and this newer Gephi should solve these problems). We have found that, on Mac OS X Mavericks, Gephi does not load properly after installation. This is a Java-related issue, so you\u2019ll need to install an earlier version of Java than the one provided. To fix this, control click (or right-click) on the Gephi package, and select \u201cshow package contents.\u201d Click on \u201ccontents \n resources \n gephi \n etc.\u201d Control-click (or right-click) on \u201cgephi.conf\u201d and open with your text editor. Find the line reading:\n\n\n#jdkhome=\"/path/to/jdk\"\n\n\nand paste the following underneath:\n\n\njdkhome=\"/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home\n\n\nSave that file. Then, go to \nhttp://support.apple.com/kb/DL1572\n and install the older version of Java (Java 6). Once that is installed, Gephi should run normally.\n\n\nRun Gephi once it is installed. You will be presented with a welcome prompting you to open a recent file, create a new project, or load a sample file. Click \u201cNew Project\u201d and then click the \u201cData Laboratory\u201d tab on the horizontal bar at the top of the Gephi window (Fig. 7.3).\n\n\nNavigating Gephi\n\n\nGephi is broken up into three panes: Overview, Data Laboratory, and Preview. The Overview pane is used to manipulate the visual properties of the network: change colors of nodes or edges, lay them out in different ways, and so forth. The Overview pane is also where you can apply algorithms to the network, like those you learned about in the previous chapter. The Data Laboratory is for adding, manipulating, and removing data. Once your network is as you want it to be, use the Preview pane to do some final tweaks on the look and feel of the network and to export an image for publication.\n\n\nThere is one tweak that needs to done in the Data Table before the dataset is fully ready to be explored in Gephi. Click on the \u201cNodes\u201d tab in the Data Table and notice that, of the three columns, \u201cLabel\u201d (the furthermost field on the right) is blank in every row. This will be a problem when viewing the network visualization, as those labels are essential for the network to be meaningful. \n\n\nIn the \u201cNodes\u201d tab, click \u201cCopy data to other column\u201d at the bottom, select \u201cID\u201d, and press \u201cOk\u201d (Fig. 7.5). Upon doing so, the \u201cLabel\u201d column will be filled with the appropriate labels for each correspondent. While you\u2019re still in the Data Laboratory, look in the \u201cEdges\u201d tab and notice there is a \u201cWeight\u201d column. Gephi automatically counted every time a letter was sent from correspondent A to correspondent B and summed up all the occurrences, resulting in the \u201cWeight.\u201d This means that J. Pinckney Henderson sent three letters to James Webb, because Henderson is in the \u201cSource\u201d column, Webb in the \u201cTarget,\u201d, and the \u201cWeight\u201d is three.\n\n\nClicking on the Overview pane will take you to a visual representation of the network you just imported. In the middle of the screen, you will see your network in the \u201cGraph\u201d tab. The \u201cContext\u201d tab, at the top right, will show that you imported 234 nodes and 394 edges. At first, all the nodes will be randomly strewn across the screen and make little visual sense. Fix this by selecting a layout in the \u201cLayout\u201d tab \u2013 the best one for beginners is \u201cForce Atlas 2.\u201d Press the \u201cRun\u201d button and watch the nodes and edges reorganize on the screen into something slightly more manageable. After the layout runs for a few minutes, re-press the button (now labeled \u201cStop\u201d) to settle the nodes in their place. \n\n\nYou just ran a force-directed layout. Each dot is a correspondent in the network, and lines between dots represent letters sent between individuals. Thicker lines represent more letters, and arrows represent the direction the letters were sent, such that there may be up to two lines connecting any two correspondents (one for each direction).\n\n\nAbout two-dozen smaller components of the network will appear to shoot off into the distance, unconnected from the large, connected component in the middle. For the purpose of this exercise, we are not interested in those disconnected components, so the next step will be to filter them out of the network. The first step is to calculate which components of the network are connected to which others; do this by clicking \u201cRun\u201d next to the text that says \u201cConnected Components\u201d in the \u201cStatistics\u201d tab on the right-hand side.  Once there, select \u201cUnDirected\u201d and press \u201cOK.\u201d Press \u201cClose\u201d when the report pops up indicating that the algorithm has finished running.\n\n\nNow that this is done, Gephi knows which is the giant connected component and has labeled that component \u201c0\u201d. To filter out everything but the giant component, click on the \u201cFilters\u201d tab on the right-hand side and browse to \"Component ID Integer (Node)\" in the folder directory (you\u2019ll find it under \"Attributes,\" then \"Equal\"). Double-click \"Component ID Integer (Node)\" and click the \"Filter\" button at the bottom. Doing this removes the disconnected bundles of nodes.\n\n\nThere are many possible algorithms you could use for the analysis step, but in this case you will use the PageRank of  each node in the network . This measurement calculates the prestige of a correspondent according to how often others write to him or her. The process is circular, such that correspondents with high prestige will confer their prestige on those they write to, who in turn pass their prestige along to their own correspondents. For the moment let us take its results to equate with a correspondent\u2019s importance in the Republic of Texas letter network.\n\n\nCalculate the PageRank by clicking on the \"Run\" button next to \"PageRank\" in the \"Statistics\" tab. You will be presented with a prompt asking for a few parameters; make sure \"Directed\" network is selected and that the algorithm is taking edge weight into account (by selecting \"Use edge weight\"). Leave all other parameters at their default. Press \"OK\".\n\n\nOnce PageRank is calculated, if you click back into the \"Data Laboratory\" and select the \"Nodes\" list in the Data Table, you can see that a new \"PageRank\" column has been added, with values for every node. The higher the PageRank, the more central a correspondent is in the network. Going back to the Overview pane, you can visualize this centrality by changing the size of each correspondent\u2019s node based on its PageRank. Do this in the \"Ranking\" tab on the left side of the Overview pane.\n\n\nMake sure \"Nodes\" is selected, press the icon of a little red diamond, and select PageRank from the drop-down menu. In the parameter options just below, enter the \"Min size\" as 1 and the \"Max size\" as 10. Press \"Apply,\" and watch the nodes resize based on their PageRank. To be on the safe side and decrease clutter, re-run the \"Force Atlas 2\" layout as described above, making sure to keep the \"Prevent Overlap\" box checked.\n\n\nAt this point, the network is processed enough to visualize in the Preview pane, to finally begin making sense of the data. In Preview, on the left-hand side, select \"Show Labels,\" \"Proportional Size,\" \"Rescale Weight,\" and deselect \"Curved\" edges. Press \"Refresh.\" \n\n\nSo what have we got?\n\n\nThe visualization immediately reveals apparent structure: central figures on the top (Ashbel Smith and Anson Jones) and bottom (Mirabeau B. Lamar, James Webb, and James Treat), and two central figures who connect the two split communities (James Hamilton and James S. Mayfield). A quick search online shows the top network to be associated with the last president of the Republic of Texas, Anson Jones; whereas the bottom network largely revolves around the second president, Mirabeau Lamar. Experts on this period in history could use this analysis to understand the structure of communities building to the annexation of Texas or they could ask meta-questions about the nature of the data themselves. For example, why is Sam Houston, the first and third president of the Republic of Texas, barely visible in this network?\n\n\nWrite up your own observations on this process in your open notebook, and export your gephi file as a .graphml file (because Gephi's \n.gephi\n format is a bit unstable, always save as or export your work in a variety of formats). Upload that to your repository.", 
            "title": "SNA with Gephi"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#working-with-gephi", 
            "text": "Before we go much further, I would recommend that you look at the work of Clement Levallois, who has a suite of excellent tutorials on working with Gephi at  http://clementlevallois.net/gephi.html . The tutorial below is adapted from our open draft of  The Macroscope .", 
            "title": "Working with Gephi"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#introduction", 
            "text": "Gephi is quickly becoming the tool of choice for network analysts who do not need the full suite of algorithms offered by  Pajek  or  UCINET . It is relatively easy to use (eclipsed in this only by  NodeXL ), it is usable on all platforms, it can analyze fairly large networks, and it creates beautiful visualizations. The development community is also extremely active, with improvements being added constantly. We recommend Gephi for the majority of historians undertaking serious network analysis research. Gephi is available at  http://gephi.github.io . Download and install it on your machine.", 
            "title": "Introduction"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#when-not-to-use-networks", 
            "text": "Networks analysis can be a powerful method for engaging with a historical dataset but it is often not the most appropriate. Any historical database may be represented as a network with enough contriving but few should be. One could take the Atlantic trade network, including cities, ships, relevant governments and corporations, and connect them all together in a giant multipartite network. It would be extremely difficult to derive any meaning from this network, especially using traditional network analysis methodologies. Clustering coefficients (a useful network metric), for example, would be meaningless in this situation, as it would be impossible for the neighbors of any one node to be neighbors with one another. Network scientists have developed algorithms for  multimodal networks , but they are often created for fairly specific purposes, and one should be very careful before applying them to a different dataset and using that analysis to explore a historiographical question.  Networks, as they are commonly encoded, also suffer from a profound lack of nuance. It is all well to say that, because Henry Oldenburg corresponded with both Gottfried Leibniz and John Milton, he was the short connection between the two men. However, the encoded network holds no information of whether Oldenburg actually transferred information between the two or whether that connection was at all meaningful. In a flight network, Las Vegas and Atlanta might both have very high betweenness centrality because people from many other cities fly to or from those airports, and yet one is much more of a hub than the other. This is because, largely, people tend to fly directly back and forth from Las Vegas, rather than through it, whereas people tend to fly through Atlanta from one city to another. This is the type of information that network analysis, as it is currently used, is not well equipped to handle. Whether this shortcoming affects a historical inquiry depends primarily on the inquiry itself.  When deciding whether to use networks to explore a historiographical question, the first question should be: to what extent is connectivity specifically important to this research project? The next should be: can any of the network analyses my collaborators or I know how to employ be specifically useful in the research project? If either answer is negative, another approach is probably warranted.", 
            "title": "When Not To Use Networks"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#texan-correspondence", 
            "text": "You will need the information you created in Module 3, after cleaning the correspondence data using Open Refine:    In order to get these data into Gephi, we will have to rename the \u201cSender\u201d column to \u201csource\u201d and the \u201cRecipient\u201d column to \u201ctarget\u201d. In the arrow to the left of Sender in the main OpenRefine window, select Edit column- Rename this column, and rename the column \u201csource\u201d. Now, in the top right of the window, select \u201cExport- Custom tabular exporter.\u201d Notice that \u201csource,\u201d \u201ctarget,\u201d and \u201cDate\u201d are checked in the content tab; uncheck \u201cDate,\u201d as it will not be used in Gephi. Go to the download tab and change the download option from \u201cTab-separated values (TSV)\u201d to \u201cComma-separated values (CSV)\u201d and press download.   The file will likely download to your automatic download directory.   (but see below)", 
            "title": "Texan Correspondence"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#quick-instructions-for-getting-the-data-into-gephi", 
            "text": "Open Gephi by double-clicking its icon. Click \u201cnew project.\u201d The middle pane of the interface window is the \u201cData Laboratory,\u201d where you can interact with your network data in spreadsheet format. This is where we can import the data cleaned up in OpenRefine.   In the Data Laboratory, select \u201cImport Spreadsheet.\u201d Press the ellipsis \u201c...\u201d and locate the CSV you created. Make sure that the Separator is listed as \u201cComma\u201d and the \u201cAs table\u201d is listed as \u201cEdges table.\u201d Press \u201cNext,\u201d then \u201cFinish.\u201d  Your data should load up. Click on the \u201coverview\u201d tab and you will be presented with a tangled network graph. Skip down to 'Navigating Gephi'.  Umm, I never did manage that openrefine stuff...  In module 3, you used Notepad++ or Textwrangler, regular expressions, and OpenRefine to create a comma-separated value file (*.csv) of the diplomatic correspondence of the Republic of Texas. The final version of the file you created has a row for each letter listed in the volume, and in each row the name of the sender and the recipient. The file should look like this:  source,target\nSam Houston,J. Pinckney Henderson\nJames Webb,Alc6e La Branche\nDavid G. Burnet,Richard G. Dunlap ...  This file is called 'an edge list' - it's a list of connections, or edges, between the individuals. If you no longer have the file, you can find it online  here .   Installing Gephi on OS 10 Mavericks  Mac users might have some trouble installing Gephi 0.8 (as I write this, the release of Gephi 0.9  should happen any day , and this newer Gephi should solve these problems). We have found that, on Mac OS X Mavericks, Gephi does not load properly after installation. This is a Java-related issue, so you\u2019ll need to install an earlier version of Java than the one provided. To fix this, control click (or right-click) on the Gephi package, and select \u201cshow package contents.\u201d Click on \u201ccontents   resources   gephi   etc.\u201d Control-click (or right-click) on \u201cgephi.conf\u201d and open with your text editor. Find the line reading:  #jdkhome=\"/path/to/jdk\"  and paste the following underneath:  jdkhome=\"/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home  Save that file. Then, go to  http://support.apple.com/kb/DL1572  and install the older version of Java (Java 6). Once that is installed, Gephi should run normally.  Run Gephi once it is installed. You will be presented with a welcome prompting you to open a recent file, create a new project, or load a sample file. Click \u201cNew Project\u201d and then click the \u201cData Laboratory\u201d tab on the horizontal bar at the top of the Gephi window (Fig. 7.3).", 
            "title": "Quick instructions for getting the data into Gephi:"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#navigating-gephi", 
            "text": "Gephi is broken up into three panes: Overview, Data Laboratory, and Preview. The Overview pane is used to manipulate the visual properties of the network: change colors of nodes or edges, lay them out in different ways, and so forth. The Overview pane is also where you can apply algorithms to the network, like those you learned about in the previous chapter. The Data Laboratory is for adding, manipulating, and removing data. Once your network is as you want it to be, use the Preview pane to do some final tweaks on the look and feel of the network and to export an image for publication.  There is one tweak that needs to done in the Data Table before the dataset is fully ready to be explored in Gephi. Click on the \u201cNodes\u201d tab in the Data Table and notice that, of the three columns, \u201cLabel\u201d (the furthermost field on the right) is blank in every row. This will be a problem when viewing the network visualization, as those labels are essential for the network to be meaningful.   In the \u201cNodes\u201d tab, click \u201cCopy data to other column\u201d at the bottom, select \u201cID\u201d, and press \u201cOk\u201d (Fig. 7.5). Upon doing so, the \u201cLabel\u201d column will be filled with the appropriate labels for each correspondent. While you\u2019re still in the Data Laboratory, look in the \u201cEdges\u201d tab and notice there is a \u201cWeight\u201d column. Gephi automatically counted every time a letter was sent from correspondent A to correspondent B and summed up all the occurrences, resulting in the \u201cWeight.\u201d This means that J. Pinckney Henderson sent three letters to James Webb, because Henderson is in the \u201cSource\u201d column, Webb in the \u201cTarget,\u201d, and the \u201cWeight\u201d is three.  Clicking on the Overview pane will take you to a visual representation of the network you just imported. In the middle of the screen, you will see your network in the \u201cGraph\u201d tab. The \u201cContext\u201d tab, at the top right, will show that you imported 234 nodes and 394 edges. At first, all the nodes will be randomly strewn across the screen and make little visual sense. Fix this by selecting a layout in the \u201cLayout\u201d tab \u2013 the best one for beginners is \u201cForce Atlas 2.\u201d Press the \u201cRun\u201d button and watch the nodes and edges reorganize on the screen into something slightly more manageable. After the layout runs for a few minutes, re-press the button (now labeled \u201cStop\u201d) to settle the nodes in their place.   You just ran a force-directed layout. Each dot is a correspondent in the network, and lines between dots represent letters sent between individuals. Thicker lines represent more letters, and arrows represent the direction the letters were sent, such that there may be up to two lines connecting any two correspondents (one for each direction).  About two-dozen smaller components of the network will appear to shoot off into the distance, unconnected from the large, connected component in the middle. For the purpose of this exercise, we are not interested in those disconnected components, so the next step will be to filter them out of the network. The first step is to calculate which components of the network are connected to which others; do this by clicking \u201cRun\u201d next to the text that says \u201cConnected Components\u201d in the \u201cStatistics\u201d tab on the right-hand side.  Once there, select \u201cUnDirected\u201d and press \u201cOK.\u201d Press \u201cClose\u201d when the report pops up indicating that the algorithm has finished running.  Now that this is done, Gephi knows which is the giant connected component and has labeled that component \u201c0\u201d. To filter out everything but the giant component, click on the \u201cFilters\u201d tab on the right-hand side and browse to \"Component ID Integer (Node)\" in the folder directory (you\u2019ll find it under \"Attributes,\" then \"Equal\"). Double-click \"Component ID Integer (Node)\" and click the \"Filter\" button at the bottom. Doing this removes the disconnected bundles of nodes.  There are many possible algorithms you could use for the analysis step, but in this case you will use the PageRank of  each node in the network . This measurement calculates the prestige of a correspondent according to how often others write to him or her. The process is circular, such that correspondents with high prestige will confer their prestige on those they write to, who in turn pass their prestige along to their own correspondents. For the moment let us take its results to equate with a correspondent\u2019s importance in the Republic of Texas letter network.  Calculate the PageRank by clicking on the \"Run\" button next to \"PageRank\" in the \"Statistics\" tab. You will be presented with a prompt asking for a few parameters; make sure \"Directed\" network is selected and that the algorithm is taking edge weight into account (by selecting \"Use edge weight\"). Leave all other parameters at their default. Press \"OK\".  Once PageRank is calculated, if you click back into the \"Data Laboratory\" and select the \"Nodes\" list in the Data Table, you can see that a new \"PageRank\" column has been added, with values for every node. The higher the PageRank, the more central a correspondent is in the network. Going back to the Overview pane, you can visualize this centrality by changing the size of each correspondent\u2019s node based on its PageRank. Do this in the \"Ranking\" tab on the left side of the Overview pane.  Make sure \"Nodes\" is selected, press the icon of a little red diamond, and select PageRank from the drop-down menu. In the parameter options just below, enter the \"Min size\" as 1 and the \"Max size\" as 10. Press \"Apply,\" and watch the nodes resize based on their PageRank. To be on the safe side and decrease clutter, re-run the \"Force Atlas 2\" layout as described above, making sure to keep the \"Prevent Overlap\" box checked.  At this point, the network is processed enough to visualize in the Preview pane, to finally begin making sense of the data. In Preview, on the left-hand side, select \"Show Labels,\" \"Proportional Size,\" \"Rescale Weight,\" and deselect \"Curved\" edges. Press \"Refresh.\"", 
            "title": "Navigating Gephi"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#so-what-have-we-got", 
            "text": "The visualization immediately reveals apparent structure: central figures on the top (Ashbel Smith and Anson Jones) and bottom (Mirabeau B. Lamar, James Webb, and James Treat), and two central figures who connect the two split communities (James Hamilton and James S. Mayfield). A quick search online shows the top network to be associated with the last president of the Republic of Texas, Anson Jones; whereas the bottom network largely revolves around the second president, Mirabeau Lamar. Experts on this period in history could use this analysis to understand the structure of communities building to the annexation of Texas or they could ask meta-questions about the nature of the data themselves. For example, why is Sam Houston, the first and third president of the Republic of Texas, barely visible in this network?  Write up your own observations on this process in your open notebook, and export your gephi file as a .graphml file (because Gephi's  .gephi  format is a bit unstable, always save as or export your work in a variety of formats). Upload that to your repository.", 
            "title": "So what have we got?"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/", 
            "text": "Graphing the Net\n\n\nIt may be that you are interested in the structure of links on the internet. Perhaps you'd like to see how a particular topic plays out across Wikiepedia. What you'd have to do is a \nweb-crawl\n. Here are quick instructions for setting up a webcrawl, which will follow every link on a page to a particular depth, and echo the results through your browser into Gephi for visualization and analysis.\n\n\nYou will need:\n\n\n\n\nThe Chrome browser with \nSite Spider Mk II\n installed\n\n\nGephi \n\n\nHTTP Graph Generator Plugin for Gephi installed\n\n\n\n\nNB\n You can install the graph generator plugin from \nwithin\n Gephi by selecting 'Tools' on the main menu ribbon at the top of the screen (and \nnot\n the 'plugins' item). Then, within 'Tools', select 'plugins' and then 'available plugins'. Search for 'HTTPGraph'. Tick off the box and install it. When finished, close and restart Gephi with a new project.\n\n\nGetting set up to scrape\n\n\nIn Chrome, go to the settings page (the 'hamburg' icon at the extreme right of the address bar, or by typing \nchrome://settings/\n in the address bar. Click on 'show advanced settings' at the bottom of the page. Then, scroll down to 'Network' and click on 'change proxy settings'. \n\n\nIn the popup that opens, click the 'connections' tab, and then the 'LAN Settings' button. \nAnother\n popup will open. Select the 'Use a proxy server for your LAN'. Enter:\n\n\n127.0.0.1\n for the address, and \n8088\n for the port. Now, whenever you go to a website, the info will be echoed through that port. We need to set Gephi up to hear what's being passed.\n\n\nOpen Gephi. Start a new project. Go to 'file' then 'generate' then 'http graph'. A pop up will open, asking you to specify a port. Input \n8088\n. Accept the defaults, and press OK. On the overview panel, nodes will begin to appear when you go to a URL in Chrome. \n\n\nBegin the scrape\n\n\nGo back to Chrome. Put in the URL that you want to start your scrape on, eg \nhttp://en.wikipedia.org/wiki/Archaeology\n. Then click the SiteSpider II button in your toolbar. Site spider will open a popup asking you how far and how deep you want to scrape; set the parameters accordingly. Hit 'go' and then flip over to your gephi window. You'll see the network begin to populate! Let it run as long as you want. When you're finished, save your network by 'exporting' it (on the file menu) as any other format than .gephi. I say this because I find sometimes the .gephi format is unstable. You can then filter your network for resources or html pages or what have you. I suggest deleting the node labelled 127.0.0.1 because that's your computer and it will throw off any metrics you choose to calculate. \n\n\nWhat does it all mean?\n\n\nWell, that depends.\n\n\nFor an example of why you might want to do all this, and what you might find, see \n'Shouting Into the Void?'\n a piece where I tried to understand the shape of the archaeological web.", 
            "title": "Graphing the Net"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#graphing-the-net", 
            "text": "It may be that you are interested in the structure of links on the internet. Perhaps you'd like to see how a particular topic plays out across Wikiepedia. What you'd have to do is a  web-crawl . Here are quick instructions for setting up a webcrawl, which will follow every link on a page to a particular depth, and echo the results through your browser into Gephi for visualization and analysis.", 
            "title": "Graphing the Net"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#you-will-need", 
            "text": "The Chrome browser with  Site Spider Mk II  installed  Gephi   HTTP Graph Generator Plugin for Gephi installed   NB  You can install the graph generator plugin from  within  Gephi by selecting 'Tools' on the main menu ribbon at the top of the screen (and  not  the 'plugins' item). Then, within 'Tools', select 'plugins' and then 'available plugins'. Search for 'HTTPGraph'. Tick off the box and install it. When finished, close and restart Gephi with a new project.", 
            "title": "You will need:"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#getting-set-up-to-scrape", 
            "text": "In Chrome, go to the settings page (the 'hamburg' icon at the extreme right of the address bar, or by typing  chrome://settings/  in the address bar. Click on 'show advanced settings' at the bottom of the page. Then, scroll down to 'Network' and click on 'change proxy settings'.   In the popup that opens, click the 'connections' tab, and then the 'LAN Settings' button.  Another  popup will open. Select the 'Use a proxy server for your LAN'. Enter:  127.0.0.1  for the address, and  8088  for the port. Now, whenever you go to a website, the info will be echoed through that port. We need to set Gephi up to hear what's being passed.  Open Gephi. Start a new project. Go to 'file' then 'generate' then 'http graph'. A pop up will open, asking you to specify a port. Input  8088 . Accept the defaults, and press OK. On the overview panel, nodes will begin to appear when you go to a URL in Chrome.", 
            "title": "Getting set up to scrape"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#begin-the-scrape", 
            "text": "Go back to Chrome. Put in the URL that you want to start your scrape on, eg  http://en.wikipedia.org/wiki/Archaeology . Then click the SiteSpider II button in your toolbar. Site spider will open a popup asking you how far and how deep you want to scrape; set the parameters accordingly. Hit 'go' and then flip over to your gephi window. You'll see the network begin to populate! Let it run as long as you want. When you're finished, save your network by 'exporting' it (on the file menu) as any other format than .gephi. I say this because I find sometimes the .gephi format is unstable. You can then filter your network for resources or html pages or what have you. I suggest deleting the node labelled 127.0.0.1 because that's your computer and it will throw off any metrics you choose to calculate.", 
            "title": "Begin the scrape"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#what-does-it-all-mean", 
            "text": "Well, that depends.  For an example of why you might want to do all this, and what you might find, see  'Shouting Into the Void?'  a piece where I tried to understand the shape of the archaeological web.", 
            "title": "What does it all mean?"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/", 
            "text": "Transforming 2-mode network data to 1-mode\n\n\nNetworks can be composed of all sorts of things. Trains, busses, Uber, metro - all of these can be combined into a 'transportation' network. Books, authors, editors, funders, censors, sponsors - a publishing network. Students, profs, classes, universities - an education network. Anytime you have more than one kind of \nthing\n (however defined) in your network, you formally have a bipartite (or tripartite, and so on, depending on the number of things) network. Visualizing such a network as nodes and edges can be a useful heuristic exercise. \n\n\nBut.\n\n\nIf you are doing this in Gephi, and you run some of Gephi's metrics (statistics) on a bipartite network, your results may not mean what you think they mean. The metrics, the algorithms assume 1-mode networks. Thus, running them on 2-mode (or more!) networks will result in ....issues.\n\n\n\n\nNow in truth, it's not quite so clear-cut to simply say, 'convert every bimodal network to unimodal before running any stats' (see \nScott's discussion here\n), but as a rule of thumb for when you're getting started, you'll be on much firmer methodological grounds if you transform your mutlimodal networks into a series of 1-mode networks. So convert every bimodal network to unimodal before running any statistics on your network(s).\n\n\nThus, this network: \n\nProfA -\n student1   (where -\n is a directed relationship, 'teaches')\n\nProfA -\n student2 \n\nProfB -\n student3 \n\nProbB -\n student1 \n\n\n...can be transformed into \ntwo\n networks, one where profA is connected to profB by virtue of a shared student (student1). That is, profs connected by students; and the inverse: students connected by profs. You could then run your metrics, and get two different perspectives on the educational dynamics of this particular university.\n\n\nIn this exercise, you'll transform a network of women and social organizations into two 1-mode networks.\n\n\nThe data\n\n\nThe data for this exercise comes from a former Carleton public history MA student, Peter Holdsworth. Peter lodged \na copy of his MA research on Figshare\n. Peter was interested in the social networks surrounding ideas of commemoration of the centenerary of the War of 1812, in 1912. He studied the membership rolls for women\u2019s service organization in Ontario both before and after that centenerary. By making his data public, Peter enables others to build upon his own research in a way not commonly done in history. (Peter can be followed on Twitter at \nhttps://twitter.com/P_W_Holdsworth\n).\n\n\nRight-click and 'save link' to get \nthe data files you'll need for this exercise\n\n\nConfiguring Gephi\n\n\nThere is a plugin for Gephi that we will use to transform our network. Open Gephi. Across the top of Gephi in the menu ribbon you\u2019ll see \nFile Workspace View Tools Window Plugins Help\n. \n\n\nTo get and install the plugin, select \nTools \n Plugins\n (The top level menu item 'Plugins' is empty and not used - a useful reminder that Gephi is still in \nbeta\n).\n\n\nIn the popup, under \u2018available plugins\u2019 look for \u2018MultimodeNetworksTransformation\u2019. Tick this box, then click on Install. Follow the instructions, ignore any warnings, click on \u2018finish\u2019. You may or may not need to restart Gephi to get the plugin running. If you suddenly see on the far right of ht Gephi window a new tab besid \u2018statistics\u2019, \u2018filters\u2019, called \u2018Multimode Network\u2019, then you\u2019re ok.\n\n\n\n\nImporting the data\n\n\n\n\nUnder \u2018file\u2019, select -\n New project.\n\n\nOn the data  laboratory tab, select Import-spreadsheet, and in the pop-up, make sure to select under \u2018as table: EDGES table. Select women-orgs.csv.  Click \u2018next\u2019, click finish.\n\n\n\n\nOn the data table, have \u2018edges\u2019 selected. This is showing you the source and the target for each link (aka \u2018edge\u2019). This implies a directionality to the relationship that we just don\u2019t know \u2013 so down below, when we get to statistics, we will always have to make sure to tell Gephi that we want the network treated as \u2018undirected\u2019. More on that below.\n\n\n\n\nLoading your csv file, step 1.\n\n\n\n\nLoading your CSV file, step 2\n\n\n\n\n\n\nClick on \u2018copy data to other column\u2019. Select \u2018Id\u2019. In the pop-up, select \u2018Label\u2019. You now have your edges labelled.\n\n\n\n\n\n\nJust as you did above, now import NODES (Women-names.csv)\n\n\n\n\nmaking sure you're on the Nodes page in the data laboratory, copy ID to Label to that your nodes are labelled.\n\n\n\n\n(nb. You can always add more attribute data to your network this way, as long as you always use a column called Id so that Gephi knows where to slot the new information. Make sure to never tick off the box labeled \u2018force nodes to be created as new ones\u2019.)\n\n\nPrepping your data\n\n\nWe're now going to manipulate the data a bit in order to get it ready for the transformation. In this data, we have women, and we have organizations. We need to tell Gephi which rows are the organizations. In Peter's original data input phase, he decided to use a unique number for each woman so that he would minimize data entry errors (misspellings and so on), as well as control for the use of maiden and married names. Miss Eliza Smith might become, at a later date, Mrs. George Doe. In Peter's scheme, this is the same person (remember in module 3 in the TEI exercise how we dealt with such issues?).\n\n\n\n\nOn your NODES page in the data laboratory, add new column, make it boolean. Call it \u2018organization\u2019\n\n\n\n\n\n\n\n\nIn the Filter box, type [a-z], and select Id \u2013 this filters out all the women. (What would you have to do to filter out the organizations? Remember, this is a regex search!)\n\n\nTick off the check boxes in the \u2018organization\u2019 columns.\n\n\n\n\n\n\n(I note a typo in the image above, 'a-b'. that should be, 'a-z').\n\n\nSave this as \nwomen-organizations-2-mode.gephi\n.\n\n\nPro tip\n: always export your data from gephi (file \n export) in .net or .graphml or .gefx format as the .gephi format (which is your only option under file \n save as) is unstable. That is, sometimes gephi won't read .gephi files! I did say this was \nbeta\n software).\n\n\nTransforming the network\n\n\nAt this point, you have a two mode network in gephi. You could click on the 'overview' panel and play with some of the layouts and begin to form impressions about the nature of your data. Remember though any metrics calculated at this point would be largely spurious. Let's transform this two-mode network so that we can explore how women are connected to other women via shared membership.\n\n\n\n\nOn the multimode networks projection tab,\n+ click load attributes.\n+ in \u2018attribute type\u2019, select organization\n+ in left matrix, select \u2018false \u2013 true\u2019 (or \u2018null \u2013 true\u2019)\n+ in right matrix, select \u2018true \u2013 false\u2019. (or \u2018true \u2013 null\u2019) (do you see why this is the case? what would selecting the inverse accomplish?)\n+ select \u2018remove edges\u2019 and \u2018remove nodes\u2019.\n+ Once you hit \u2018run\u2019, organizations will be removed from your bipartite network, leaving you with a single-mode network. hit \u2018run\u2019.\n+ save as \nwomen-to-women-network.gephi\n \nand\n export as \nwomen-to-women.net\n\n\nNB\n if your nodes data table is blank, your filter might still be active. make sure the filter box is clear. You should be left with a list of women (ie, a list of nodes where the identiers are numbers, per Peter's schema).\n\n\nAt this point, you could re-start Gephi and reload your \u2018women-organizations-2-mode.gephi\u2019 file and re-run the multimode networks projection so that you are left with an organization to organization network. Do this, and save and export with appropriate file names.\n\n\nExploring this network\n\n\nPeter's data has a number of \nattributes\n describing it, including the membership year. So let's see what this network of women looks like in 1902.\n\n\n\n\nUnder the filters tab at the right hand side of the Gephi interface, select \u2018attributes \u2013 equal\u2019 and then drag \u20181902\u2019 to the queries box.\n\n\nIn \u2018pattern\u2019 enter [0-9] and tick the \u2018use regex\u2019 box.\n\n\nclick ok, click \u2018filter\u2019.\n\n\n\n\nYou should now have a network with 188 nodes and 8728 edges, showing the women who were active in 1902.\n\n\nLet\u2019s learn something about this network. Under the statistics tab at the right hand side of the Gephi interface,\n\n\n\n\n\n\nRun \u2018avg. path length\u2019 by clicking on \u2018run\u2019\n\n\n\n\n\n\nIn the pop up that opens, select \u2018undirected\u2019 (as we know nothing about directionality in this network; we simply know that two women were members of the same organization at the same time. Note also that if the same pair were members of the more than one organziation, the weight of their connection will be corresponding stronger).\n\n\n\n\n\n\nclick ok.\n\n\n\n\n\n\nrun \u2018modularity\u2019 to look for subgroups. make sure \u2018randomize\u2019 and \u2018use weights\u2019 are selected. Leave \u2018resolution\u2019 at 1.0\n\n\n\n\n\n\nWe selected 'average path length' because one of the byproducts of this routine is 'betweeness centrality'. We're making an assumption here that a woman who has a high betweeness centrality score was in a position to affect information flow in 1902 society. Modularity looks at similar patterns of connections to cluster women who have more-or-less similar connections into groups.\n\n\nLet\u2019s visualize what we\u2019ve just learned.\n\n\n\n\nOn the \u2018partition\u2019 tab, over on the left hand side of the \u2018overview\u2019 screen, click on nodes, then click the green arrows beside \u2018choose a partition parameter\u2019.\n\n\nClick on \u2018choose a partition parameter\u2019. Scroll down to modularity class. The different groups will be listed, with their colours and their % composition of the network.\n\n\nHit \u2018apply\u2019 to recolour your network graph.\n\n\n\n\nLet\u2019s resize the nodes to show off betweeness-centrality (to figure out which woman was in the greatest position to influence flows of information in this network.) \n\n\n\n\nClick \u2018ranking\u2019. (It's on the left hand side of the interface, beside 'partition' and just below 'overview'.\n\n\nClick \u2018nodes\u2019.\n\n\nClick the down arrow on \u2018choose a rank parameter\u2019. Select \u2018betweeness centrality\u2019.\n\n\nClick the red diamond. This will resize the nodes according to their \u2018betweeness centrality\u2019.\n\n\nClick \u2018apply\u2019.\n\n\n\n\nNow, down at the bottom of the middle panel, you can click the large black \u2018T\u2019 to display labels. Do so. Click the black letter \u2018A\u2019 and select \u2018node size\u2019.\n\n\nMrs. Mary Elliot-Murray-Kynynmound and Mrs. John Henry Wilson should now dominate your network (if you go back to the original data zip, you'll be able to find Peter's key to figure out who's who). Who were these ladies? What organizations were they members of? Who were they connected to? To the archives!\n\n\nCongratulations! You\u2019ve imported historical network data into Gephi, transformed it, manipulated it, and run some analyses. Play with the settings on \u2018preview\u2019 in order to share your visualization as svg, pdf, or png.\n\n\nNow go back to your original gephi file, and recast it as organizations to organizations via shared members, to figure out which organizations were key in early 20th century Ontario\u2026 make appropriate notes in your open notebook.", 
            "title": "Multimode Networks"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#transforming-2-mode-network-data-to-1-mode", 
            "text": "Networks can be composed of all sorts of things. Trains, busses, Uber, metro - all of these can be combined into a 'transportation' network. Books, authors, editors, funders, censors, sponsors - a publishing network. Students, profs, classes, universities - an education network. Anytime you have more than one kind of  thing  (however defined) in your network, you formally have a bipartite (or tripartite, and so on, depending on the number of things) network. Visualizing such a network as nodes and edges can be a useful heuristic exercise.   But.  If you are doing this in Gephi, and you run some of Gephi's metrics (statistics) on a bipartite network, your results may not mean what you think they mean. The metrics, the algorithms assume 1-mode networks. Thus, running them on 2-mode (or more!) networks will result in ....issues.   Now in truth, it's not quite so clear-cut to simply say, 'convert every bimodal network to unimodal before running any stats' (see  Scott's discussion here ), but as a rule of thumb for when you're getting started, you'll be on much firmer methodological grounds if you transform your mutlimodal networks into a series of 1-mode networks. So convert every bimodal network to unimodal before running any statistics on your network(s).  Thus, this network:  \nProfA -  student1   (where -  is a directed relationship, 'teaches') \nProfA -  student2  \nProfB -  student3  \nProbB -  student1   ...can be transformed into  two  networks, one where profA is connected to profB by virtue of a shared student (student1). That is, profs connected by students; and the inverse: students connected by profs. You could then run your metrics, and get two different perspectives on the educational dynamics of this particular university.  In this exercise, you'll transform a network of women and social organizations into two 1-mode networks.", 
            "title": "Transforming 2-mode network data to 1-mode"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#the-data", 
            "text": "The data for this exercise comes from a former Carleton public history MA student, Peter Holdsworth. Peter lodged  a copy of his MA research on Figshare . Peter was interested in the social networks surrounding ideas of commemoration of the centenerary of the War of 1812, in 1912. He studied the membership rolls for women\u2019s service organization in Ontario both before and after that centenerary. By making his data public, Peter enables others to build upon his own research in a way not commonly done in history. (Peter can be followed on Twitter at  https://twitter.com/P_W_Holdsworth ).  Right-click and 'save link' to get  the data files you'll need for this exercise", 
            "title": "The data"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#configuring-gephi", 
            "text": "There is a plugin for Gephi that we will use to transform our network. Open Gephi. Across the top of Gephi in the menu ribbon you\u2019ll see  File Workspace View Tools Window Plugins Help .   To get and install the plugin, select  Tools   Plugins  (The top level menu item 'Plugins' is empty and not used - a useful reminder that Gephi is still in  beta ).  In the popup, under \u2018available plugins\u2019 look for \u2018MultimodeNetworksTransformation\u2019. Tick this box, then click on Install. Follow the instructions, ignore any warnings, click on \u2018finish\u2019. You may or may not need to restart Gephi to get the plugin running. If you suddenly see on the far right of ht Gephi window a new tab besid \u2018statistics\u2019, \u2018filters\u2019, called \u2018Multimode Network\u2019, then you\u2019re ok.", 
            "title": "Configuring Gephi"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#importing-the-data", 
            "text": "Under \u2018file\u2019, select -  New project.  On the data  laboratory tab, select Import-spreadsheet, and in the pop-up, make sure to select under \u2018as table: EDGES table. Select women-orgs.csv.  Click \u2018next\u2019, click finish.   On the data table, have \u2018edges\u2019 selected. This is showing you the source and the target for each link (aka \u2018edge\u2019). This implies a directionality to the relationship that we just don\u2019t know \u2013 so down below, when we get to statistics, we will always have to make sure to tell Gephi that we want the network treated as \u2018undirected\u2019. More on that below.   Loading your csv file, step 1.   Loading your CSV file, step 2    Click on \u2018copy data to other column\u2019. Select \u2018Id\u2019. In the pop-up, select \u2018Label\u2019. You now have your edges labelled.    Just as you did above, now import NODES (Women-names.csv)   making sure you're on the Nodes page in the data laboratory, copy ID to Label to that your nodes are labelled.   (nb. You can always add more attribute data to your network this way, as long as you always use a column called Id so that Gephi knows where to slot the new information. Make sure to never tick off the box labeled \u2018force nodes to be created as new ones\u2019.)", 
            "title": "Importing the data"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#prepping-your-data", 
            "text": "We're now going to manipulate the data a bit in order to get it ready for the transformation. In this data, we have women, and we have organizations. We need to tell Gephi which rows are the organizations. In Peter's original data input phase, he decided to use a unique number for each woman so that he would minimize data entry errors (misspellings and so on), as well as control for the use of maiden and married names. Miss Eliza Smith might become, at a later date, Mrs. George Doe. In Peter's scheme, this is the same person (remember in module 3 in the TEI exercise how we dealt with such issues?).   On your NODES page in the data laboratory, add new column, make it boolean. Call it \u2018organization\u2019     In the Filter box, type [a-z], and select Id \u2013 this filters out all the women. (What would you have to do to filter out the organizations? Remember, this is a regex search!)  Tick off the check boxes in the \u2018organization\u2019 columns.    (I note a typo in the image above, 'a-b'. that should be, 'a-z').  Save this as  women-organizations-2-mode.gephi .  Pro tip : always export your data from gephi (file   export) in .net or .graphml or .gefx format as the .gephi format (which is your only option under file   save as) is unstable. That is, sometimes gephi won't read .gephi files! I did say this was  beta  software).", 
            "title": "Prepping your data"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#transforming-the-network", 
            "text": "At this point, you have a two mode network in gephi. You could click on the 'overview' panel and play with some of the layouts and begin to form impressions about the nature of your data. Remember though any metrics calculated at this point would be largely spurious. Let's transform this two-mode network so that we can explore how women are connected to other women via shared membership.   On the multimode networks projection tab,\n+ click load attributes.\n+ in \u2018attribute type\u2019, select organization\n+ in left matrix, select \u2018false \u2013 true\u2019 (or \u2018null \u2013 true\u2019)\n+ in right matrix, select \u2018true \u2013 false\u2019. (or \u2018true \u2013 null\u2019) (do you see why this is the case? what would selecting the inverse accomplish?)\n+ select \u2018remove edges\u2019 and \u2018remove nodes\u2019.\n+ Once you hit \u2018run\u2019, organizations will be removed from your bipartite network, leaving you with a single-mode network. hit \u2018run\u2019.\n+ save as  women-to-women-network.gephi   and  export as  women-to-women.net  NB  if your nodes data table is blank, your filter might still be active. make sure the filter box is clear. You should be left with a list of women (ie, a list of nodes where the identiers are numbers, per Peter's schema).  At this point, you could re-start Gephi and reload your \u2018women-organizations-2-mode.gephi\u2019 file and re-run the multimode networks projection so that you are left with an organization to organization network. Do this, and save and export with appropriate file names.", 
            "title": "Transforming the network"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#exploring-this-network", 
            "text": "Peter's data has a number of  attributes  describing it, including the membership year. So let's see what this network of women looks like in 1902.   Under the filters tab at the right hand side of the Gephi interface, select \u2018attributes \u2013 equal\u2019 and then drag \u20181902\u2019 to the queries box.  In \u2018pattern\u2019 enter [0-9] and tick the \u2018use regex\u2019 box.  click ok, click \u2018filter\u2019.   You should now have a network with 188 nodes and 8728 edges, showing the women who were active in 1902.  Let\u2019s learn something about this network. Under the statistics tab at the right hand side of the Gephi interface,    Run \u2018avg. path length\u2019 by clicking on \u2018run\u2019    In the pop up that opens, select \u2018undirected\u2019 (as we know nothing about directionality in this network; we simply know that two women were members of the same organization at the same time. Note also that if the same pair were members of the more than one organziation, the weight of their connection will be corresponding stronger).    click ok.    run \u2018modularity\u2019 to look for subgroups. make sure \u2018randomize\u2019 and \u2018use weights\u2019 are selected. Leave \u2018resolution\u2019 at 1.0    We selected 'average path length' because one of the byproducts of this routine is 'betweeness centrality'. We're making an assumption here that a woman who has a high betweeness centrality score was in a position to affect information flow in 1902 society. Modularity looks at similar patterns of connections to cluster women who have more-or-less similar connections into groups.  Let\u2019s visualize what we\u2019ve just learned.   On the \u2018partition\u2019 tab, over on the left hand side of the \u2018overview\u2019 screen, click on nodes, then click the green arrows beside \u2018choose a partition parameter\u2019.  Click on \u2018choose a partition parameter\u2019. Scroll down to modularity class. The different groups will be listed, with their colours and their % composition of the network.  Hit \u2018apply\u2019 to recolour your network graph.   Let\u2019s resize the nodes to show off betweeness-centrality (to figure out which woman was in the greatest position to influence flows of information in this network.)    Click \u2018ranking\u2019. (It's on the left hand side of the interface, beside 'partition' and just below 'overview'.  Click \u2018nodes\u2019.  Click the down arrow on \u2018choose a rank parameter\u2019. Select \u2018betweeness centrality\u2019.  Click the red diamond. This will resize the nodes according to their \u2018betweeness centrality\u2019.  Click \u2018apply\u2019.   Now, down at the bottom of the middle panel, you can click the large black \u2018T\u2019 to display labels. Do so. Click the black letter \u2018A\u2019 and select \u2018node size\u2019.  Mrs. Mary Elliot-Murray-Kynynmound and Mrs. John Henry Wilson should now dominate your network (if you go back to the original data zip, you'll be able to find Peter's key to figure out who's who). Who were these ladies? What organizations were they members of? Who were they connected to? To the archives!  Congratulations! You\u2019ve imported historical network data into Gephi, transformed it, manipulated it, and run some analyses. Play with the settings on \u2018preview\u2019 in order to share your visualization as svg, pdf, or png.  Now go back to your original gephi file, and recast it as organizations to organizations via shared members, to figure out which organizations were key in early 20th century Ontario\u2026 make appropriate notes in your open notebook.", 
            "title": "Exploring this network"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/", 
            "text": "Making a map website with Leaflet.js\n\n\nThe \nleaflet.js\n library allows you to create quite nice interactive maps in a web-browser, that are also mobile friendly. Here, we'll build a map that uses our georectified map as the base layer (rather than as an image overlay). I won't go into the details, but rather will provide you enough guidance to get going. The documentation for Leaflet is quite extensive, and many other tutorials abound.\n\n\nSetup\n\n\n\n\ncreate a new github repository for this exercise. Create a new branch called \ngh-pages\n. We will be putting our html on the gh-pages branch, so that \nyour username\n/github.io/\nrepo\nmap.html\n can serve us up the webpage when we're done.\n\n\nLeaflet comes with a number of \nexcellent tutorials\n. We're going to look at the \nfirst one\n. Go to the \nleaflet quick-start tutorial\n and read through it carefully. In essence, you create a webpage that draws its instructions on how to handle geographic data and how to style that data from the leaflet.js source. That way, the browser knows how to render all the geographic information you're about to give it. Do you see where leaflet is calling on geographic information? This bit:\n\n\n\n\nL.tileLayer('http://{s}.tiles.mapbox.com/v3/MapID/{z}/{x}/{y}.png', {\n    attribution: 'Map data \ncopy; \na href=\nhttp://openstreetmap.org\nOpenStreetMap\n/a\n contributors, \na href=\nhttp://creativecommons.org/licenses/by-sa/2.0/\nCC-BY-SA\n/a\n, Imagery \u00a9 \na href=\nhttp://mapbox.com\nMapbox\n/a\n',\n    maxZoom: 18\n}).addTo(map);\n\n\n\n\nis calling on a background layer from the mapbox service. Instead of using mapbox, We can slot the url to the georectified map we made in module 4 in there! That is, swap out the url from the line beginning \nL.tileLayer\n. You'd change the copyright notice, etc, too, obviously.\n\n\nThe other bits of code that create callouts, polygons, and so on, are all using decimal degrees to locate the drawn elements on top of that map.\n\n\nThis bit:\n\n\nL.marker([51.5, -0.09]).addTo(map)\n            .bindPopup(\nb\nHello world!\n/b\nbr /\nI am a popup.\n).openPopup();\n\n\n\n\ncan be copied and repeated in the document, with new coordinates in decimal degrees for each new point. In the second line, between the quotation marks, you can use regular html to style your text, include pictures, and so on.\n\n\nIn a new browser window, open the \nexample map for the quickstart guide\n, and then right-click \n view source.\n\n\nSo let's get started.\n\n\n\n\nCreate a new html document in your gh-pages branch of your repo.\n\n\nCopy the html from the quickstart map (right-click and select 'view source' on this page: \nleafletjs.com/examples/quick-start/example.html\n\n\nPaste the code in your new html document in your gh-pages branch of your repo. Call it 'map.html' and commit your changes.\n\n\nChange the source map to point to a georectified map you made in module 4. Using the Ottawa Fire Insurance map I used as an example in module 4, I created \nthis map\n. Right click and view my page source to see what I changed up.  \nNB\n You could keep the basic mapbox service base map, and render the Ottawa Fire Insurance map as an overlay [reference documentation]http://leafletjs.com/reference-1.0.3.html#imageoverlay). Or you could do a series of overlays, showing the change in the city over time. (My favourite example of a leaflet-powered historical map visualization is the \nSlave Revolt in Jamaica project\n by Vincent Brown). But you don't necessarily have to do this.\n\n\nAdd a series of markers with historical information by duplicating and then changing up the \nL.marker\n settings to your own data. Commit your changes!\n\n\n\n\nThis all just makes the map. The rest of the webpage would have to be styled as you would normally for a webpage. That is, you'd probably want to add an explanation about what the map shows, how it was created, and how the user ought to interact with it, links to your source data, and so on. The easiest place to add all that kind of information would be between these two tags in the page source:\n\n\n/script\n\n\n/body\n\n\n\n\n\nGoing further\n\n\nLet's say you have a whole bunch of information that you want to represent on the map. Perhaps it's in a well organized csv file, with a latitude and a longitude column in decimal degrees. Adding points one at a time to the map as described above would take ages. Instead, let's convert that csv to geojson, and then use \nbootleaf\n to make a map.\n\n\nBootleaf is a template that uses a common html template package, '\nBootstrap\n' as a wrapper for a leaflet powered map that draws its points of interest from a geojson file. Here's how you'd get this up and running.\n\n\n\n\nGo to the \ngithub repo for bootleaf\n.\n\n\nFork a copy to a new repo (you have to be logged into github.com) by hitting the 'fork' button.\n\n\nIn your copy of bootleaf, you now have a gh-pages version of the site. If you go to \nyourusername\n.github.io/bootleaf\n you should see an active version of the map.\n\n\nNow, the map is grabbing its data from a series of geojson files. You can use \nthis service\n to convert your csv to geojson. There are other services.\n\n\nClone your repository in your desktop (by pressing the clone your repo in desktop).\n\n\nOpen your desktop client, and make sure you're in the gh-pages branch\n\n\nUsing your windows explorer or mac finder, put your newly created geojson file in the data folder.\n\n\ncommit and sync your changes.\n\n\n\n\nTo add your data to the dropdown menu, you need to change the code in the index.html file:\n\n\n  \nli class=\ndropdown\n\n                \na class=\ndropdown-toggle\n id=\ndownloadDrop\n href=\n#\n role=\nbutton\n data-toggle=\ndropdown\ni class=\nfa fa-cloud-download white\n/i\nnbsp;\nnbsp;Download \nb class=\ncaret\n/b\n/a\n\n                \nul class=\ndropdown-menu\n\n                  \nli\na href=\ndata/boroughs.geojson\n download=\nboroughs.geojson\n target=\n_blank\n data-toggle=\ncollapse\n data-target=\n.navbar-collapse.in\ni class=\nfa fa-download\n/i\nnbsp;\nnbsp;Boroughs\n/a\n/li\n\n                  \nli\na href=\ndata/subways.geojson\n download=\nsubways.geojson\n target=\n_blank\n data-toggle=\ncollapse\n data-target=\n.navbar-collapse.in\ni class=\nfa fa-download\n/i\nnbsp;\nnbsp;Subway Lines\n/a\n/li\n\n                  \nli\na href=\ndata/DOITT_THEATER_01_13SEPT2010.geojson\n download=\ntheaters.geojson\n target=\n_blank\n data-toggle=\ncollapse\n data-target=\n.navbar-collapse.in\ni class=\nfa fa-download\n/i\nnbsp;\nnbsp;Theaters\n/a\n/li\n\n                  \nli\na href=\ndata/DOITT_MUSEUM_01_13SEPT2010.geojson\n download=\nmuseums.geojson\n target=\n_blank\n data-toggle=\ncollapse\n data-target=\n.navbar-collapse.in\ni class=\nfa fa-download\n/i\nnbsp;\nnbsp;Museums\n/a\n/li\n\n                \n/ul\n\n\n\n\n\nand since you probably don't want that other stuff, you could delete it.\n\n\nYou could change the 'about' pop-up here:\n\n\ndiv class=\ntab-pane fade active in\n id=\nabout\n\n                \np\nA simple, responsive template for building web mapping applications with \na href=\nhttp://getbootstrap.com/\nBootstrap 3\n/a\n, \na href=\nhttp://leafletjs.com/\n target=\n_blank\nLeaflet\n/a\n, and \na href=\nhttp://twitter.github.io/typeahead.js/\n target=\n_blank\ntypeahead.js\n/a\n. Open source, MIT licensed, and available on \na href=\nhttps://github.com/bmcbride/bootleaf\n target=\n_blank\nGitHub\n/a\n.\n/p\n\n                \ndiv class=\npanel panel-primary\n\n                  \ndiv class=\npanel-heading\nFeatures\n/div\n\n                  \nul class=\nlist-group\n\n                    \nli class=\nlist-group-item\nFullscreen mobile-friendly map template with responsive navbar and modal placeholders\n/li\n\n                    \nli class=\nlist-group-item\njQuery loading of external GeoJSON files\n/li\n\n                    \nli class=\nlist-group-item\nLogical multiple layer marker clustering via the \na href=\nhttps://github.com/Leaflet/Leaflet.markercluster\n target=\n_blank\nleaflet marker cluster plugin\n/a\n/li\n\n                    \nli class=\nlist-group-item\nElegant client-side multi-layer feature search with autocomplete using \na href=\nhttp://twitter.github.io/typeahead.js/\n target=\n_blank\ntypeahead.js\n/a\n/li\n\n                    \nli class=\nlist-group-item\nResponsive sidebar feature list synced with map bounds, which includes sorting and filtering via \na href=\nhttp://listjs.com/\n target=\n_blank\nlist.js\n/a\n/li\n\n                    \nli class=\nlist-group-item\nMarker icons included in grouped layer control via the \na href=\nhttps://github.com/ismyrnow/Leaflet.groupedlayercontrol\n target=\n_blank\ngrouped layer control plugin\n/a\n/li\n\n                  \n/ul\n\n                \n/div\n\n\n\n\n\nAnd you have to remove this:\n\n\n    \n!-- Remove this maptiks analytics code from your BootLeaf implementation --\n\n    \nscript src=\n//cdn.maptiks.com/maptiks-leaflet.min.js\n/script\n\n    \nscript\nmaptiks.trackcode='c7ca251e-9c17-47ef-ac33-e0fb24e05976';\n/script\n\n    \n!-- End maptiks analytics code --\n\n\n\n\n\nNow the really hard part: putting your own base maps in. To do this, you have to find, and modify, a file called app.js. You should be able to find it by following this path: bootleaf/assets/js/app.js\n\n\nYou need to change these lines to point to your maps\n\n\n/* Basemap Layers */\nvar mapquestOSM = L.tileLayer(\nhttp://{s}.mqcdn.com/tiles/1.0.0/osm/{z}/{x}/{y}.png\n, {\n  maxZoom: 19,\n  subdomains: [\notile1\n, \notile2\n, \notile3\n, \notile4\n],\n  attribution: 'Tiles courtesy of \na href=\nhttp://www.mapquest.com/\n target=\n_blank\nMapQuest\n/a\n \nimg src=\nhttp://developer.mapquest.com/content/osm/mq_logo.png\n. Map data (c) \na href=\nhttp://www.openstreetmap.org/\n target=\n_blank\nOpenStreetMap\n/a\n contributors, CC-BY-SA.'\n});\nvar mapquestOAM = L.tileLayer(\nhttp://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\n, {\n  maxZoom: 18,\n  subdomains: [\noatile1\n, \noatile2\n, \noatile3\n, \noatile4\n],\n  attribution: 'Tiles courtesy of \na href=\nhttp://www.mapquest.com/\n target=\n_blank\nMapQuest\n/a\n. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n});\nvar mapquestHYB = L.layerGroup([L.tileLayer(\nhttp://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\n, {\n  maxZoom: 18,\n  subdomains: [\noatile1\n, \noatile2\n, \noatile3\n, \noatile4\n]\n}), L.tileLayer(\nhttp://{s}.mqcdn.com/tiles/1.0.0/hyb/{z}/{x}/{y}.png\n, {\n  maxZoom: 19,\n  subdomains: [\noatile1\n, \noatile2\n, \noatile3\n, \noatile4\n],\n  attribution: 'Labels courtesy of \na href=\nhttp://www.mapquest.com/\n target=\n_blank\nMapQuest\n/a\n \nimg src=\nhttp://developer.mapquest.com/content/osm/mq_logo.png\n. Map data (c) \na href=\nhttp://www.openstreetmap.org/\n target=\n_blank\nOpenStreetMap\n/a\n contributors, CC-BY-SA. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n})]);\n\n\n\n\n...and then you'd have to go through the rest of that file and change up the .geojson pointers to point to your own data.\n\n\nHere is a \ntemplate for mapping with leaflet\n, drawing all of your point data and ancillary information from a csv file. Study the index.html file carefully to identify which lines you'd modify to change the base map, and to identify how elements in the csv are being rendered on the screen. \nHere's an example that a former student made\n.\n\n\n(\nfurther reading\n Here's a nice piece on using \nTilemill\n by Arian Katsimbras to make beautiful maps.)", 
            "title": "Leaflet"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#making-a-map-website-with-leafletjs", 
            "text": "The  leaflet.js  library allows you to create quite nice interactive maps in a web-browser, that are also mobile friendly. Here, we'll build a map that uses our georectified map as the base layer (rather than as an image overlay). I won't go into the details, but rather will provide you enough guidance to get going. The documentation for Leaflet is quite extensive, and many other tutorials abound.", 
            "title": "Making a map website with Leaflet.js"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#setup", 
            "text": "create a new github repository for this exercise. Create a new branch called  gh-pages . We will be putting our html on the gh-pages branch, so that  your username /github.io/ repo map.html  can serve us up the webpage when we're done.  Leaflet comes with a number of  excellent tutorials . We're going to look at the  first one . Go to the  leaflet quick-start tutorial  and read through it carefully. In essence, you create a webpage that draws its instructions on how to handle geographic data and how to style that data from the leaflet.js source. That way, the browser knows how to render all the geographic information you're about to give it. Do you see where leaflet is calling on geographic information? This bit:   L.tileLayer('http://{s}.tiles.mapbox.com/v3/MapID/{z}/{x}/{y}.png', {\n    attribution: 'Map data  copy;  a href= http://openstreetmap.org OpenStreetMap /a  contributors,  a href= http://creativecommons.org/licenses/by-sa/2.0/ CC-BY-SA /a , Imagery \u00a9  a href= http://mapbox.com Mapbox /a ',\n    maxZoom: 18\n}).addTo(map);  is calling on a background layer from the mapbox service. Instead of using mapbox, We can slot the url to the georectified map we made in module 4 in there! That is, swap out the url from the line beginning  L.tileLayer . You'd change the copyright notice, etc, too, obviously.  The other bits of code that create callouts, polygons, and so on, are all using decimal degrees to locate the drawn elements on top of that map.  This bit:  L.marker([51.5, -0.09]).addTo(map)\n            .bindPopup( b Hello world! /b br / I am a popup. ).openPopup();  can be copied and repeated in the document, with new coordinates in decimal degrees for each new point. In the second line, between the quotation marks, you can use regular html to style your text, include pictures, and so on.  In a new browser window, open the  example map for the quickstart guide , and then right-click   view source.", 
            "title": "Setup"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#so-lets-get-started", 
            "text": "Create a new html document in your gh-pages branch of your repo.  Copy the html from the quickstart map (right-click and select 'view source' on this page:  leafletjs.com/examples/quick-start/example.html  Paste the code in your new html document in your gh-pages branch of your repo. Call it 'map.html' and commit your changes.  Change the source map to point to a georectified map you made in module 4. Using the Ottawa Fire Insurance map I used as an example in module 4, I created  this map . Right click and view my page source to see what I changed up.   NB  You could keep the basic mapbox service base map, and render the Ottawa Fire Insurance map as an overlay [reference documentation]http://leafletjs.com/reference-1.0.3.html#imageoverlay). Or you could do a series of overlays, showing the change in the city over time. (My favourite example of a leaflet-powered historical map visualization is the  Slave Revolt in Jamaica project  by Vincent Brown). But you don't necessarily have to do this.  Add a series of markers with historical information by duplicating and then changing up the  L.marker  settings to your own data. Commit your changes!   This all just makes the map. The rest of the webpage would have to be styled as you would normally for a webpage. That is, you'd probably want to add an explanation about what the map shows, how it was created, and how the user ought to interact with it, links to your source data, and so on. The easiest place to add all that kind of information would be between these two tags in the page source:  /script  /body   Going further  Let's say you have a whole bunch of information that you want to represent on the map. Perhaps it's in a well organized csv file, with a latitude and a longitude column in decimal degrees. Adding points one at a time to the map as described above would take ages. Instead, let's convert that csv to geojson, and then use  bootleaf  to make a map.  Bootleaf is a template that uses a common html template package, ' Bootstrap ' as a wrapper for a leaflet powered map that draws its points of interest from a geojson file. Here's how you'd get this up and running.   Go to the  github repo for bootleaf .  Fork a copy to a new repo (you have to be logged into github.com) by hitting the 'fork' button.  In your copy of bootleaf, you now have a gh-pages version of the site. If you go to  yourusername .github.io/bootleaf  you should see an active version of the map.  Now, the map is grabbing its data from a series of geojson files. You can use  this service  to convert your csv to geojson. There are other services.  Clone your repository in your desktop (by pressing the clone your repo in desktop).  Open your desktop client, and make sure you're in the gh-pages branch  Using your windows explorer or mac finder, put your newly created geojson file in the data folder.  commit and sync your changes.   To add your data to the dropdown menu, you need to change the code in the index.html file:     li class= dropdown \n                 a class= dropdown-toggle  id= downloadDrop  href= #  role= button  data-toggle= dropdown i class= fa fa-cloud-download white /i nbsp; nbsp;Download  b class= caret /b /a \n                 ul class= dropdown-menu \n                   li a href= data/boroughs.geojson  download= boroughs.geojson  target= _blank  data-toggle= collapse  data-target= .navbar-collapse.in i class= fa fa-download /i nbsp; nbsp;Boroughs /a /li \n                   li a href= data/subways.geojson  download= subways.geojson  target= _blank  data-toggle= collapse  data-target= .navbar-collapse.in i class= fa fa-download /i nbsp; nbsp;Subway Lines /a /li \n                   li a href= data/DOITT_THEATER_01_13SEPT2010.geojson  download= theaters.geojson  target= _blank  data-toggle= collapse  data-target= .navbar-collapse.in i class= fa fa-download /i nbsp; nbsp;Theaters /a /li \n                   li a href= data/DOITT_MUSEUM_01_13SEPT2010.geojson  download= museums.geojson  target= _blank  data-toggle= collapse  data-target= .navbar-collapse.in i class= fa fa-download /i nbsp; nbsp;Museums /a /li \n                 /ul   and since you probably don't want that other stuff, you could delete it.  You could change the 'about' pop-up here:  div class= tab-pane fade active in  id= about \n                 p A simple, responsive template for building web mapping applications with  a href= http://getbootstrap.com/ Bootstrap 3 /a ,  a href= http://leafletjs.com/  target= _blank Leaflet /a , and  a href= http://twitter.github.io/typeahead.js/  target= _blank typeahead.js /a . Open source, MIT licensed, and available on  a href= https://github.com/bmcbride/bootleaf  target= _blank GitHub /a . /p \n                 div class= panel panel-primary \n                   div class= panel-heading Features /div \n                   ul class= list-group \n                     li class= list-group-item Fullscreen mobile-friendly map template with responsive navbar and modal placeholders /li \n                     li class= list-group-item jQuery loading of external GeoJSON files /li \n                     li class= list-group-item Logical multiple layer marker clustering via the  a href= https://github.com/Leaflet/Leaflet.markercluster  target= _blank leaflet marker cluster plugin /a /li \n                     li class= list-group-item Elegant client-side multi-layer feature search with autocomplete using  a href= http://twitter.github.io/typeahead.js/  target= _blank typeahead.js /a /li \n                     li class= list-group-item Responsive sidebar feature list synced with map bounds, which includes sorting and filtering via  a href= http://listjs.com/  target= _blank list.js /a /li \n                     li class= list-group-item Marker icons included in grouped layer control via the  a href= https://github.com/ismyrnow/Leaflet.groupedlayercontrol  target= _blank grouped layer control plugin /a /li \n                   /ul \n                 /div   And you have to remove this:       !-- Remove this maptiks analytics code from your BootLeaf implementation -- \n     script src= //cdn.maptiks.com/maptiks-leaflet.min.js /script \n     script maptiks.trackcode='c7ca251e-9c17-47ef-ac33-e0fb24e05976'; /script \n     !-- End maptiks analytics code --   Now the really hard part: putting your own base maps in. To do this, you have to find, and modify, a file called app.js. You should be able to find it by following this path: bootleaf/assets/js/app.js  You need to change these lines to point to your maps  /* Basemap Layers */\nvar mapquestOSM = L.tileLayer( http://{s}.mqcdn.com/tiles/1.0.0/osm/{z}/{x}/{y}.png , {\n  maxZoom: 19,\n  subdomains: [ otile1 ,  otile2 ,  otile3 ,  otile4 ],\n  attribution: 'Tiles courtesy of  a href= http://www.mapquest.com/  target= _blank MapQuest /a   img src= http://developer.mapquest.com/content/osm/mq_logo.png . Map data (c)  a href= http://www.openstreetmap.org/  target= _blank OpenStreetMap /a  contributors, CC-BY-SA.'\n});\nvar mapquestOAM = L.tileLayer( http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg , {\n  maxZoom: 18,\n  subdomains: [ oatile1 ,  oatile2 ,  oatile3 ,  oatile4 ],\n  attribution: 'Tiles courtesy of  a href= http://www.mapquest.com/  target= _blank MapQuest /a . Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n});\nvar mapquestHYB = L.layerGroup([L.tileLayer( http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg , {\n  maxZoom: 18,\n  subdomains: [ oatile1 ,  oatile2 ,  oatile3 ,  oatile4 ]\n}), L.tileLayer( http://{s}.mqcdn.com/tiles/1.0.0/hyb/{z}/{x}/{y}.png , {\n  maxZoom: 19,\n  subdomains: [ oatile1 ,  oatile2 ,  oatile3 ,  oatile4 ],\n  attribution: 'Labels courtesy of  a href= http://www.mapquest.com/  target= _blank MapQuest /a   img src= http://developer.mapquest.com/content/osm/mq_logo.png . Map data (c)  a href= http://www.openstreetmap.org/  target= _blank OpenStreetMap /a  contributors, CC-BY-SA. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n})]);  ...and then you'd have to go through the rest of that file and change up the .geojson pointers to point to your own data.  Here is a  template for mapping with leaflet , drawing all of your point data and ancillary information from a csv file. Study the index.html file carefully to identify which lines you'd modify to change the base map, and to identify how elements in the csv are being rendered on the screen.  Here's an example that a former student made .  ( further reading  Here's a nice piece on using  Tilemill  by Arian Katsimbras to make beautiful maps.)", 
            "title": "So let's get started."
        }, 
        {
            "location": "/supporting materials/glitch/", 
            "text": "Glitching files messses with our expectations of what digital data should be. There is actually quite a large body of literature on the why and how of glitching (\ntry this for a place to start\n). For us as digital historians, glitching digital images or other documents reminds of us the ephemerality of digital data. It might also raise other questions about the composition of historical photography, and the ways that no image is an objective record of the past. In the exercise below, you build on what you learned about APIs and Regex to download images from the Open Context repository of archaeological data. Then, you use a script that will perform the same manipulations on every image in your folder. Finally, you will use another script to create a static website with all of your images, a gallery of glitch.\n\n\nSpend some time on the articles and resources curated \nhere\n to begin to explore the philosophy and aesthetic of Glitch.\n\n\nWorkflow we're going for:\n\n\n\n\nGet images from Open Context (\nocget.sh\n)\n\n\nGlitch them (\ndo-glitch.sh\n)\n\n\nMake a website out of them (\nexpose\n)\n\n\nPush them to github Pages (\ngit\n)\n\n\n\n\nOpen Context\n\n\nOpen Context publishes archaeological data on the web. Archaeology generates vast amounts of information, both through excavation and through analysis. Open Context exists to publish curated versions of this data with unique digital object identifiers so that the source data of archaeology can be re-examined and re-studied in the future. Archaeology, uniquely amongst the historical disciplines, tends to destroy its subject matter, so reproducibility and open access data are extremely important issues. At their \nAPI\n page, they explain how to programmatically obtain information from their site. Here's an example \nsearch\n. Open that up, give it a play right now, and see what kinds of information exist. Try to craft a search that retrieves some interesting information.\n\n\nOnce you've crafted a search that retrieves something of interest, it's time to build a script that will retrieve that information for you. Remember the exercise that queried the Canadiana api? Below I have modified that script to grab 10 records from the 'animal bone' category on Open Context. You could use that as a basis for your own search.\n\n\n\n\nIn DHBox, create a new folder with \nmkdir\n for this exercise, then \ncd\n into it. Open the nano editor and paste the script below in a new file.\n\n\nMake sure to read the comments, because there are a few lines you have to customize. (You'll get file not found errors if you don't pay attention!) Save it as \nocget.sh\n, then use \n$chmod 755 ocget.sh\n to make it executable. Then \n$ mkdir images\n so you have a place to put the images into.\n\n\n\n\nThe line beginning with 'sed' searches for the \nthumbnail\n key, and marks it off with a tilde. The \n|\n character is called a 'pipe' and it pipes the output of the command before it into the input of the command after it. The first pipe passes the output of that first sed command to grep, which finds the lines marked with the tilde to the tr (trim) command, which then deletes all of the other information; the next pipe passes to a series of sed commands which then get rid of whitespace and the word thumbnail, the tilde, and any commas, leaving us with a list of direct URLs to the thumbnails.\n\n\nThe wget command grabs each thumbnail in the list in turn, waiting 2 seconds between requests, and using a limited amount of bandwidth (we're good digital citizens, remember).\n\n\n#! /bin/bash\n\n# we already know how to ask the api for the infromation we're interested in, from studying opencontex.org' api documentation. Below we ask for a 10 records related to animal bones\ncurl 'https://opencontext.org/media-search/.json?prop=rel--oc-gen-cat-object||rel--oc-gen-cat-animal-bone\nresponse=uri-meta\nrows=10' \n results.txt\n\n# the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks \n\\\n (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase \nkey:\n and delete it too.\n\nsed -r 's/(.+\\bthumbnail\\b.+)/~\\1/g' results.txt | grep '~'| tr -d \n\\\n | tr -d \n{\n | tr -d \n}\n | tr -s \n \n | sed '/^\\s*$/d' | tr -d ' ' | sed 's/\\bthumbnail:\\b//g' | sed 's/,//g' | sed 's/~//g'\n thumbnailstograb.txt\n\n# gotta grab 'em all.\n\n# in the line below, put the full path to your images file, eg mine is home/shawngraham/oc/images. Yours might be home/yourname/oc/images\nwget -i thumbnailstograb.txt -P path/to/images --limit-rate=20k -w 2\n\n\n\n\nHey, images keep appearing even though I ctrl+c to stop the download\n I hear some of you say. Well, that does't necessarily stop anything; when you ran the command, you started a \nprocess\n and it might still be ticking away in the background. If this seems to be happening to you, type \nps ux\n to get a list of running processes. If wget is still running in the backround, you'll see it listed in the right-most column. The first number in that row is its PIDnumber; to kill it, type \nkill -9 PIDnumber\n where you swap in the actual PIDnumber.\n\n\nAlso\n, before you re-run this script, you need to delete the 'results.txt' file and the 'thumbnailstograb.txt' file, like so: \nrm results.txt\n etc. Otherwise, it'll just add your results to the end of the previous results, making your list longer and longer...\n\n\nAn image glitch script in Python\n\n\nThe next step is to glitch the image. Go to \nhttps://github.com/Xpktro/bndr\n. This is a python package for mucking about the bits inside a jpg. Install it at the command line with \n$ pip install bndr\n.\n\n\nAlso, make a new folder for the output of this process: \nmkdir out\n.\n\n\nNow, this code is meant to be run on one file at a time, like this: \n$ bndrimg photo-name.jpg\n. Typing in each file name by hand would take a very long time to glitch everything. So instead, we write a little script that we'll call \ndo-glitch.sh\n. In Nano, paste the following:\n\n\n#! /bin/bash\n# do-glitch.sh\n# run the glitch script on each image in the images folder\nfor file in images/*.jpg; do bndrimg \n$file\n; done\n# move the glitched files to the output folder\nfind ./ -name '*out.png' -exec cp -prv '{}' 'out/' ';'\n\n\n\n\nThis isn't the most elegant script, but it works, and frankly, that's all that matters sometimes.\n\n\nThen change the permissions so we can run it: \n$ chmod 755 do-glitch.sh\n. This script reads each file name in the images folder, and passes them one at a time to the bndr command (you can't run it when you're \ninside\n the images folder, remember). Then, we move just the glitched images to the 'out' folder. Download some of the pics to your own machine using the filemanager to see your glitched art!\n\n\nA static website generator for photography\n\n\nFinally, let's turn that folder of pictures into a website. We're going to use the Expos\u00e9 site generator, which you can get at \nhttps://github.com/Jack000/expose\n. Take a moment to read through the details of that package.\n\n\nUse the \ncd\n command to get back up to your main directory (ie, don't do this when you're in your images folder). Grab the code:\n\n$ git clone https://github.com/Jack000/Expose.git\n\n\nThe expose script relies on some helpers and so on in its folder. But we would like to be able to run that command no matter which folder we're in. We tell the computer that when we type \nexpose\n we \nactually\n mean, the version that lives in that location where we just downloaded it. That is to say, we use the \nalias\n command. On my machine, it looks like this:\n\nalias expose=/home/shawngraham/oc/Expose/expose.sh\n On your machine, it might be in a different location. Note also that the Expose repo is uppercase E, while the command is lowercase e. \nalso\n every time you start a session, you'll need to do this alias command or else the computer won't know what you're talking about.\n\n\nTo generate the static site, cd into your \nout\n folder that holds your glitched images, and at the prompt, type \n$ expose\n. Ta da! You now have a fully functional website showing off your glitch art.\n\n\nYou can customize it to use an alternative theme, a few bells and whistles; see the Expose documentation. One thing that you should do is to add captions for your images. You write these as either .txt or .md files, but make sure the file name for the caption is the same as for the image \nand\n put it in the same folder as the source images! You do this \nbefore\n you run the expose script. So \nbeaker-pic.png\n would have as a caption file \nbeaker-pic.txt\n or \nbeaker-pic.md\n. You specify where the caption goes on the image in the YAML; that is, the metadata for your caption. So, if we opened 'beaker-pic.md' we might see,\n\n\n---\ntop: 30\nleft: 5\nwidth: 30\nheight: 20\ntextcolor: #ffffff\n---\n# A Picture of a Beaker\nBeakers were used to carry rare aromatic herbs and spices...\n\n\n\n\n\nMore possibilities for sorting out the position of the text are discussed in the Expose documentation.\n\n\nHost the site on github\n\n\nYou can push the code for your site to github, and then use github's gh-pages feature to serve it up as a live website! Expose creates all of the code for the website inside a new folder called \n_site\n.\n\n\n\n\n$ cd _site\n\n\nTurn this folder into a git repository: \ngit init\n.\n\n\nGo to your account on Github.com (in another browser window). Click on the \n+\n at the top-right to make a new repository. Call it whatever you want, but \ndo not\n initialize it with a readme.\n\n\nBack at the command line in your \n_site\n folder, type \n$ git add .\n This stages all of the files and subfolders for a new commit.\n\n\nMake a commit message: \n$ git commit -m \"first commit\"\n\n\nTell git where your remote repository is: \n$ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE.git\n\n\nPush your materials to Github: \n$ git push -u origin master\n (Git might ask for your account username and password)\n\n\nOnce that finishes, go back to Github. Reload the page at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE. You should see your changes\n\n\nOn the button where it says 'Branch: Master', click on the down arrow. In the box where it says faintly 'Find or create a branch', type \ngh-pages\n . This is a special branch which tells Github, 'serve this up as actual code when the user goes to the special github.io version of github'.\n\n\nClick on the gearwheel icon, to go to the settings page (You can also find it at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE/settings). Scroll down to the box that says 'Github Pages'. In the green box, it says 'you're site is published at https://YOUR-ACCOUNT.github.io/YOUR-NEW-REPO-YOU-JUST-MADE'. Click on that link and you'll see your site! Here's \nmine\n.\n\n\n\n\nConclusion\n\n\nI see no reason why digital history cannot bleed into art. Art is meant to provoke, to prompt reflection, discussion, controversy. Glitching images confounds our expectation of what digital data are supposed to be, supposed to do. If you choose your pictures carefully, glitching can tell a story as profound as any essay.", 
            "title": "Glitch"
        }, 
        {
            "location": "/supporting materials/glitch/#workflow-were-going-for", 
            "text": "Get images from Open Context ( ocget.sh )  Glitch them ( do-glitch.sh )  Make a website out of them ( expose )  Push them to github Pages ( git )", 
            "title": "Workflow we're going for:"
        }, 
        {
            "location": "/supporting materials/glitch/#open-context", 
            "text": "Open Context publishes archaeological data on the web. Archaeology generates vast amounts of information, both through excavation and through analysis. Open Context exists to publish curated versions of this data with unique digital object identifiers so that the source data of archaeology can be re-examined and re-studied in the future. Archaeology, uniquely amongst the historical disciplines, tends to destroy its subject matter, so reproducibility and open access data are extremely important issues. At their  API  page, they explain how to programmatically obtain information from their site. Here's an example  search . Open that up, give it a play right now, and see what kinds of information exist. Try to craft a search that retrieves some interesting information.  Once you've crafted a search that retrieves something of interest, it's time to build a script that will retrieve that information for you. Remember the exercise that queried the Canadiana api? Below I have modified that script to grab 10 records from the 'animal bone' category on Open Context. You could use that as a basis for your own search.   In DHBox, create a new folder with  mkdir  for this exercise, then  cd  into it. Open the nano editor and paste the script below in a new file.  Make sure to read the comments, because there are a few lines you have to customize. (You'll get file not found errors if you don't pay attention!) Save it as  ocget.sh , then use  $chmod 755 ocget.sh  to make it executable. Then  $ mkdir images  so you have a place to put the images into.   The line beginning with 'sed' searches for the  thumbnail  key, and marks it off with a tilde. The  |  character is called a 'pipe' and it pipes the output of the command before it into the input of the command after it. The first pipe passes the output of that first sed command to grep, which finds the lines marked with the tilde to the tr (trim) command, which then deletes all of the other information; the next pipe passes to a series of sed commands which then get rid of whitespace and the word thumbnail, the tilde, and any commas, leaving us with a list of direct URLs to the thumbnails.  The wget command grabs each thumbnail in the list in turn, waiting 2 seconds between requests, and using a limited amount of bandwidth (we're good digital citizens, remember).  #! /bin/bash\n\n# we already know how to ask the api for the infromation we're interested in, from studying opencontex.org' api documentation. Below we ask for a 10 records related to animal bones\ncurl 'https://opencontext.org/media-search/.json?prop=rel--oc-gen-cat-object||rel--oc-gen-cat-animal-bone response=uri-meta rows=10'   results.txt\n\n# the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks  \\  (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase  key:  and delete it too.\n\nsed -r 's/(.+\\bthumbnail\\b.+)/~\\1/g' results.txt | grep '~'| tr -d  \\  | tr -d  {  | tr -d  }  | tr -s     | sed '/^\\s*$/d' | tr -d ' ' | sed 's/\\bthumbnail:\\b//g' | sed 's/,//g' | sed 's/~//g'  thumbnailstograb.txt\n\n# gotta grab 'em all.\n\n# in the line below, put the full path to your images file, eg mine is home/shawngraham/oc/images. Yours might be home/yourname/oc/images\nwget -i thumbnailstograb.txt -P path/to/images --limit-rate=20k -w 2  Hey, images keep appearing even though I ctrl+c to stop the download  I hear some of you say. Well, that does't necessarily stop anything; when you ran the command, you started a  process  and it might still be ticking away in the background. If this seems to be happening to you, type  ps ux  to get a list of running processes. If wget is still running in the backround, you'll see it listed in the right-most column. The first number in that row is its PIDnumber; to kill it, type  kill -9 PIDnumber  where you swap in the actual PIDnumber.  Also , before you re-run this script, you need to delete the 'results.txt' file and the 'thumbnailstograb.txt' file, like so:  rm results.txt  etc. Otherwise, it'll just add your results to the end of the previous results, making your list longer and longer...", 
            "title": "Open Context"
        }, 
        {
            "location": "/supporting materials/glitch/#an-image-glitch-script-in-python", 
            "text": "The next step is to glitch the image. Go to  https://github.com/Xpktro/bndr . This is a python package for mucking about the bits inside a jpg. Install it at the command line with  $ pip install bndr .  Also, make a new folder for the output of this process:  mkdir out .  Now, this code is meant to be run on one file at a time, like this:  $ bndrimg photo-name.jpg . Typing in each file name by hand would take a very long time to glitch everything. So instead, we write a little script that we'll call  do-glitch.sh . In Nano, paste the following:  #! /bin/bash\n# do-glitch.sh\n# run the glitch script on each image in the images folder\nfor file in images/*.jpg; do bndrimg  $file ; done\n# move the glitched files to the output folder\nfind ./ -name '*out.png' -exec cp -prv '{}' 'out/' ';'  This isn't the most elegant script, but it works, and frankly, that's all that matters sometimes.  Then change the permissions so we can run it:  $ chmod 755 do-glitch.sh . This script reads each file name in the images folder, and passes them one at a time to the bndr command (you can't run it when you're  inside  the images folder, remember). Then, we move just the glitched images to the 'out' folder. Download some of the pics to your own machine using the filemanager to see your glitched art!", 
            "title": "An image glitch script in Python"
        }, 
        {
            "location": "/supporting materials/glitch/#a-static-website-generator-for-photography", 
            "text": "Finally, let's turn that folder of pictures into a website. We're going to use the Expos\u00e9 site generator, which you can get at  https://github.com/Jack000/expose . Take a moment to read through the details of that package.  Use the  cd  command to get back up to your main directory (ie, don't do this when you're in your images folder). Grab the code: $ git clone https://github.com/Jack000/Expose.git  The expose script relies on some helpers and so on in its folder. But we would like to be able to run that command no matter which folder we're in. We tell the computer that when we type  expose  we  actually  mean, the version that lives in that location where we just downloaded it. That is to say, we use the  alias  command. On my machine, it looks like this: alias expose=/home/shawngraham/oc/Expose/expose.sh  On your machine, it might be in a different location. Note also that the Expose repo is uppercase E, while the command is lowercase e.  also  every time you start a session, you'll need to do this alias command or else the computer won't know what you're talking about.  To generate the static site, cd into your  out  folder that holds your glitched images, and at the prompt, type  $ expose . Ta da! You now have a fully functional website showing off your glitch art.  You can customize it to use an alternative theme, a few bells and whistles; see the Expose documentation. One thing that you should do is to add captions for your images. You write these as either .txt or .md files, but make sure the file name for the caption is the same as for the image  and  put it in the same folder as the source images! You do this  before  you run the expose script. So  beaker-pic.png  would have as a caption file  beaker-pic.txt  or  beaker-pic.md . You specify where the caption goes on the image in the YAML; that is, the metadata for your caption. So, if we opened 'beaker-pic.md' we might see,  ---\ntop: 30\nleft: 5\nwidth: 30\nheight: 20\ntextcolor: #ffffff\n---\n# A Picture of a Beaker\nBeakers were used to carry rare aromatic herbs and spices...  More possibilities for sorting out the position of the text are discussed in the Expose documentation.", 
            "title": "A static website generator for photography"
        }, 
        {
            "location": "/supporting materials/glitch/#host-the-site-on-github", 
            "text": "You can push the code for your site to github, and then use github's gh-pages feature to serve it up as a live website! Expose creates all of the code for the website inside a new folder called  _site .   $ cd _site  Turn this folder into a git repository:  git init .  Go to your account on Github.com (in another browser window). Click on the  +  at the top-right to make a new repository. Call it whatever you want, but  do not  initialize it with a readme.  Back at the command line in your  _site  folder, type  $ git add .  This stages all of the files and subfolders for a new commit.  Make a commit message:  $ git commit -m \"first commit\"  Tell git where your remote repository is:  $ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE.git  Push your materials to Github:  $ git push -u origin master  (Git might ask for your account username and password)  Once that finishes, go back to Github. Reload the page at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE. You should see your changes  On the button where it says 'Branch: Master', click on the down arrow. In the box where it says faintly 'Find or create a branch', type  gh-pages  . This is a special branch which tells Github, 'serve this up as actual code when the user goes to the special github.io version of github'.  Click on the gearwheel icon, to go to the settings page (You can also find it at https://github.com/YOUR-ACCOUNT/YOUR-NEW-REPO-YOU-JUST-MADE/settings). Scroll down to the box that says 'Github Pages'. In the green box, it says 'you're site is published at https://YOUR-ACCOUNT.github.io/YOUR-NEW-REPO-YOU-JUST-MADE'. Click on that link and you'll see your site! Here's  mine .", 
            "title": "Host the site on github"
        }, 
        {
            "location": "/supporting materials/glitch/#conclusion", 
            "text": "I see no reason why digital history cannot bleed into art. Art is meant to provoke, to prompt reflection, discussion, controversy. Glitching images confounds our expectation of what digital data are supposed to be, supposed to do. If you choose your pictures carefully, glitching can tell a story as profound as any essay.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/supporting materials/quick-intro-r/", 
            "text": "A quick introduction to R and R studio\n\n\nR is a powerful language for statistical exploring, visualizing, and manipulating all kinds of data, including textual. It is however not the most intutive of environments to work in. In which case, \nRStudio\n is what we need. Fortunately, you already have this installed for you in your DHBox; however, if you want to have it on your own computer, go get R \nfrom here\n and then the \nRStudio\n. In DHBox, you sign into RStudio with your DHBox credentials.\n\n\nWhy do we bother with this? Why not just use Excel? One word: reproducibility. In Excel, the sequence of clicking that one does to do anything is next to \nimpossible to effectively communicate\n to someone else. Excel also was built for business applications, and those biases are built into its dna (\nwhich can have some nasty effects\n.)\n\n\nR allows us to do scripted analyses. We write out the sequence of transformations or analyses to be done, making a kind of program that we can then feed our data into. Once we have a workflow that does what we need it to do, it becomes trivial to re-use the code on other datasets. What's more, when we do an analysis, we can publish the scripts and the data. We don't have to taken an author's word for it: we can re-run the analysis ourselves or build upon it.\n\n\nThis course only touches in the lightest manner on the potential for R for doing digital history. Please see Lincoln Mullen's \nComputational Historical Thinking with Applications in R\n for more instruction.\n\n\nFor now, we'll do a quick whistle-stop tour of using R Studio to do some analysis in R.\n\n\n\n\nYou should always keep your research materials (scripts and associated data) organized in separate project folders. R Studio makes this easy for you. When you open R Studio to start a new project, click where it says 'project (none)' at the right hand side of the interface. Click on the down arrow, and select new R project. Follow the prompts, and create it in a new directory. R keeps track of what you're up to in a project file, so that you can pick up where you left off. \nYou should also keep your code under version control as well\n so that you can recover from disaster and/or share your code/data with collaborators; click on that link to get version control set up. (Sometimes, it might happen that RStudio crashes (this can be, in DHBox, related to memory issues). If that happens - if it just seems to 'hang' (and you've waited several minutes), you can refresh your browser, and go back to DHBox. You'll probably have to re-open your project file. Use the file panel at bottom right to find your *.rproj file (your R project file) and click on it to re-open. Save your work often, from the File \n Save menu.)\n\n\nR Studio divides the screen up into four neat panes. The top left is for writing your analytical script (or code; I will use the two words interchangeably), the bottom left is the console where the analysis actually happens; top right is an environment pane which will show you the variables you've created, your history (commands you've run), and (once it's configured) git; indeed, other tools and plugins will appear as tabs here. The bottom right gives you a preview of any plots or charts you create; once you create one if you click 'zoom' the chart will open in its own pop up so that you can see it better. The file explorer and the help text also appear under tabs in this box.\n\n\nYou write your script in the script box, and then you can get R Studio to run the code one line at a time by clicking on code \n run line (it runs the line of code where you left the cursor). If you select a number of lines of code and hit run line, all of that code will run. R scripts are text files that use .r as their file extension.\n\n\nThe console is where the action happens. Click down in the console. Type \n3 + 5\n and hit enter. R Studio will run the calculation: \n[1] 8\n! The [1] indicates that this is the first result. Now type \na = 3\n and hit enter. Over in the top right, the 'environment' pane updates to tell us that yes, a has a value of 3. Now type \nb = 5\n. The environment updates accordingly. Now type \na + b\n. Hey, we can do algebra! Anything you can do in excel, we can do in code here in R Studio. A very good introduction to how R (the language) works is at \nTry R\n, an interactive tutorial in your browser. Go give some of that a shot.", 
            "title": "Quick Intro to R"
        }, 
        {
            "location": "/supporting materials/quick-intro-r/#a-quick-introduction-to-r-and-r-studio", 
            "text": "R is a powerful language for statistical exploring, visualizing, and manipulating all kinds of data, including textual. It is however not the most intutive of environments to work in. In which case,  RStudio  is what we need. Fortunately, you already have this installed for you in your DHBox; however, if you want to have it on your own computer, go get R  from here  and then the  RStudio . In DHBox, you sign into RStudio with your DHBox credentials.  Why do we bother with this? Why not just use Excel? One word: reproducibility. In Excel, the sequence of clicking that one does to do anything is next to  impossible to effectively communicate  to someone else. Excel also was built for business applications, and those biases are built into its dna ( which can have some nasty effects .)  R allows us to do scripted analyses. We write out the sequence of transformations or analyses to be done, making a kind of program that we can then feed our data into. Once we have a workflow that does what we need it to do, it becomes trivial to re-use the code on other datasets. What's more, when we do an analysis, we can publish the scripts and the data. We don't have to taken an author's word for it: we can re-run the analysis ourselves or build upon it.  This course only touches in the lightest manner on the potential for R for doing digital history. Please see Lincoln Mullen's  Computational Historical Thinking with Applications in R  for more instruction.  For now, we'll do a quick whistle-stop tour of using R Studio to do some analysis in R.   You should always keep your research materials (scripts and associated data) organized in separate project folders. R Studio makes this easy for you. When you open R Studio to start a new project, click where it says 'project (none)' at the right hand side of the interface. Click on the down arrow, and select new R project. Follow the prompts, and create it in a new directory. R keeps track of what you're up to in a project file, so that you can pick up where you left off.  You should also keep your code under version control as well  so that you can recover from disaster and/or share your code/data with collaborators; click on that link to get version control set up. (Sometimes, it might happen that RStudio crashes (this can be, in DHBox, related to memory issues). If that happens - if it just seems to 'hang' (and you've waited several minutes), you can refresh your browser, and go back to DHBox. You'll probably have to re-open your project file. Use the file panel at bottom right to find your *.rproj file (your R project file) and click on it to re-open. Save your work often, from the File   Save menu.)  R Studio divides the screen up into four neat panes. The top left is for writing your analytical script (or code; I will use the two words interchangeably), the bottom left is the console where the analysis actually happens; top right is an environment pane which will show you the variables you've created, your history (commands you've run), and (once it's configured) git; indeed, other tools and plugins will appear as tabs here. The bottom right gives you a preview of any plots or charts you create; once you create one if you click 'zoom' the chart will open in its own pop up so that you can see it better. The file explorer and the help text also appear under tabs in this box.  You write your script in the script box, and then you can get R Studio to run the code one line at a time by clicking on code   run line (it runs the line of code where you left the cursor). If you select a number of lines of code and hit run line, all of that code will run. R scripts are text files that use .r as their file extension.  The console is where the action happens. Click down in the console. Type  3 + 5  and hit enter. R Studio will run the calculation:  [1] 8 ! The [1] indicates that this is the first result. Now type  a = 3  and hit enter. Over in the top right, the 'environment' pane updates to tell us that yes, a has a value of 3. Now type  b = 5 . The environment updates accordingly. Now type  a + b . Hey, we can do algebra! Anything you can do in excel, we can do in code here in R Studio. A very good introduction to how R (the language) works is at  Try R , an interactive tutorial in your browser. Go give some of that a shot.", 
            "title": "A quick introduction to R and R studio"
        }, 
        {
            "location": "/supporting materials/git-rstudio/", 
            "text": "Using Git with rstudio-pubs-static\n\n\nBefore you can use Git to keep track of your changes to your R project, you need to tell the Git program (which keeps snapshots of your changes) who you are. Do this with these two commands via the command line:\n\n$ git config --global user.email \"you@example.com\"``\n$ git config --global user.name \"Your Name\"`\n\n\nGo back to R Studio. Under 'Tools' select 'Version Control' then 'project setup'. Under 'Version control system' select 'Git'.\n\n\nYou now have a new option in the pane at top right, beside 'environment' and 'history': 'git'. Click on 'git'.\n\n\nThe panel now displays all of the files created in this project folder. Tick off the ones you want to commit. Then click on the 'Commit' button.\n\n\n\n\nA new window opens called 'RStudio: Review Changes'. This window shows you a preview of the text of each file, in green where material has been added, red where material has been deleted (these are the 'difs'). Add a commit message int the top right 'commit message' box.\n\n\nYou've now made a local commit to your git repository! If things go horribly wrong, you can roll back the changes. Now, let's setup your github repo for this.\n\n\nGo to your github account. Make a new repository; initialize it with a readme.md\n\n\nBack in RStudio, click on the 'more' gearwheel on the Git tab. Select shell (you could do this from the command line too, when you're in your project folder). This will open up a box into which you can type commands; we're going to tell Git the location of our remote repository, add that info into the config, and do two pulls.\n\n\nopen shell\n\n\n$ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-REPO.git\n$ git config remote.origin.url https://github.com/YOUR-ACCOUNT/YOUR-REPO.git\n$ git pull -u origin master\n$ git pull -u origin master\n\n\n\n\nAnd you now can push your changes to your remote repository whenever you make a new commit. There is a variation of markdown called RMarkdown that enables you to embed working R code into a document, and then 'knit' it into HTML or slide shows or pdfs. When you push those to a Github repo, you are now making data publications! The official \nR Markdown information is here\n.", 
            "title": "Git"
        }, 
        {
            "location": "/supporting materials/git-rstudio/#using-git-with-rstudio-pubs-static", 
            "text": "Before you can use Git to keep track of your changes to your R project, you need to tell the Git program (which keeps snapshots of your changes) who you are. Do this with these two commands via the command line: $ git config --global user.email \"you@example.com\"`` $ git config --global user.name \"Your Name\"`  Go back to R Studio. Under 'Tools' select 'Version Control' then 'project setup'. Under 'Version control system' select 'Git'.  You now have a new option in the pane at top right, beside 'environment' and 'history': 'git'. Click on 'git'.  The panel now displays all of the files created in this project folder. Tick off the ones you want to commit. Then click on the 'Commit' button.   A new window opens called 'RStudio: Review Changes'. This window shows you a preview of the text of each file, in green where material has been added, red where material has been deleted (these are the 'difs'). Add a commit message int the top right 'commit message' box.  You've now made a local commit to your git repository! If things go horribly wrong, you can roll back the changes. Now, let's setup your github repo for this.  Go to your github account. Make a new repository; initialize it with a readme.md  Back in RStudio, click on the 'more' gearwheel on the Git tab. Select shell (you could do this from the command line too, when you're in your project folder). This will open up a box into which you can type commands; we're going to tell Git the location of our remote repository, add that info into the config, and do two pulls.  open shell  $ git remote add origin https://github.com/YOUR-ACCOUNT/YOUR-REPO.git\n$ git config remote.origin.url https://github.com/YOUR-ACCOUNT/YOUR-REPO.git\n$ git pull -u origin master\n$ git pull -u origin master  And you now can push your changes to your remote repository whenever you make a new commit. There is a variation of markdown called RMarkdown that enables you to embed working R code into a document, and then 'knit' it into HTML or slide shows or pdfs. When you push those to a Github repo, you are now making data publications! The official  R Markdown information is here .", 
            "title": "Using Git with rstudio-pubs-static"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/", 
            "text": "Topic Modeling in R, DHBox version\n\n\nIn this exercise, we're going to grab the Colonial Newspaper Database from my github page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi or other such packaged). At the appropriate point, I show you how to import a directory of texts rather than a single file of data, and to feed that into the script.\n\n\nGo to your DHBox, and click on RStudio. At the right hand side where it says 'project (none)', click and create a new project in a new empty directory. (If you want to put this directory under version control with git, so that you can push your work to your github account, please read \nthese instructions\n.)\n\n\nIn the script panel (top left; click on the green plus side and select new R script if this pane isn't open) paste the following code and then run each line by putting the cursor in the line and hitting code \n run lines.\n\n\ninstall.packages(\nmallet\n)\nlibrary(\nmallet\n)\ninstall.packages('RCurl')\nlibrary(RCurl)\n\n\n\n\nIn the future, now that you've installed these packages you won't have to again, so you can comment them out by placing a # in front.\n\n\nImporting data directly from the web\n\n\nMelodee Beals has been using TEI to markup newspaper articles, creating the Colonial Newspapers Database (which she shared on github). We then used Github Pages and an XLST stylesheet to convert that database into a table of comma-separated values, a copy of which is at https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv. We are now going to topic model the text of those newspaper articles, to see what patterns of discourse may lie within.\n\n\nNow we want to tell R Studio to grab our data from our github page. The thing is, R Studio can easily grab materials from websites where the url is http; but when it is https (as it is with github), things get a bit more fussy. So what we do is use a special package to grab the data, and then shove it into a variable that we can then tease apart for our analysis.\n\n\nx \n- getURL(\nhttps://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv\n, .opts = list(ssl.verifypeer = FALSE))\n\n\n\n\nThat line reaches out to the webpage and grabs the information and puts it into a variable called \nx\n.\n\n\ndocuments \n- read.csv(text = x, col.names=c(\nArticle_ID\n, \nNewspaper Title\n, \nNewspaper City\n, \nNewspaper Province\n, \nNewspaper Country\n, \nYear\n, \nMonth\n, \nDay\n, \nArticle Type\n, \nText\n, \nKeywords\n), colClasses=rep(\ncharacter\n, 3), sep=\n,\n, quote=\n)\n\n\n\n\nNow we've created a variable called \ndocuments\n and the \nread.csv\n command read all of the data pulled into x, and tells R that \ndocuments\n has columns called \"Newspaper Title\" etc. When we only want information from a particular column, we modify the variable slightly, eg \ndocuments$Keywords\n would only look at the information in the keywords column. Let's go on a brief digression and actually do that, and see what we learn about this corpus:\n\n\ncounts \n- table(documents$Newspaper.City)\n\n\n\n\nWe tell R to make a new variable called 'counts', and fill it with the information from the column 'newspaper city' in 'documents'. It counts them up! Let's make a simple barplot:\n\n\nbarplot(counts, main=\nCities\n, xlab=\nNumber of Articles\n)\n\n\n\n\nThe plot will appear in the bottom right pane of RStudio. You can click on 'zoom' to see the plot in a popup window. You can also export it as a png or pdf file. Clearly, we\u2019re getting an Edinburgh/Glasgow perspective on things. And somewhere in our data, there\u2019s a mispelled \u2018Edinbugh\u2019. Do you see any other error(s) in the plot? How would you correct it(them)?\n\n\nLet's do the same thing for year, and count the number of articles per year in this corpus:\n\n\nyears \n- table(documents$Year)\nbarplot(years, main=\nPublication Year\n, xlab=\nYear\n, ylab=\nNumber of Articles\n)\n\n\n\n\nThere\u2019s a lot of material in 1789, another peak around 1819, againg in the late 1830s. We can ask ourselves now: is this an artefact of the data, or of our collection methods? This would be a question a reviewer would want answers to. Let\u2019s assume for now that these two plots are \u2018true\u2019 - that, for whatever reasons, only Edinburgh and Glasgow were concerned with these colonial reports, and that they were particulary interested during those three periods. This is already an interesting question that we as historians would want to explore. Try making some more visualizations like this of other aspects of the data. What other patterns do you see that are worth investigating?\n\n\nNow, let's return to getting our data ready to create a topic model. In the line below, note that there is a file called 'en.txt' that it wants to load up. You need to create this file; it's a list of stopwords, or common words that we think do not add value to our model (words like 'the', 'and', 'of' and so on.) The decision of which words to exclude from our analysis is of course a theoretical position...\n\n\nTo create that file, click on the 'new file' icon in the tool ribbon and select new text file. This will open a blank file in the edit window here. Copy and paste the list of words at \nen.txt\n into that blank file and save it as en.txt. This file was put together by Matt Jockers.\n\n\nmallet.instances \n- mallet.import(documents$Article_ID, documents$Text, \nen.txt\n, token.regexp = \n\\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L}\n)\n\n\n\n\nThat line above passes the article ID and the text of our newspaper articles to the Mallet routine.  The stopwords list is generic; it might need to be curated to take into account the pecularities of your data. You might want to create your own, one for each project given the particulars of your project. Note that Jockers compiled hist stoplist for his research in literary history of the 19th century. Your mileage may vary! Finally, the last bit after \u2018token.regexp\u2019 applies a regular expression against our newspaper articles, cleaning them up.\n\n\nReading data from a directory\n\n\nThis is an alternative way of ingesting documents for topic modeling. Earlier, you learned how to use wget and some other scripts to download full text documents from Canadiana.org as well as from the Provincial Archives in Quebec (the Shawville Equity). The code below loads those documents into Mallet, after which you can proceed to build a topic model. In the command line, cd into your folder that has your downloaded materials. At the command line, cd into your folder, and type $ pwd to get the full path. Copy it, go back to RStudio, and paste it into the line below between the \" marks.\n\n\ndocuments \n- mallet.read.dir(\n/home/shawngraham/equity\n)\n\n# your path will probably look like /home/your-account-in-dhbox/your-folder-of-materials\n\nmallet.instances \n- mallet.import(documents$id, documents$text, \nen.txt\n, token.regexp = \n\\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L}\n)\n\n\n\n\nnb do either one or the other, but not both: read from a file, where each row contains the complete text of the document, or read from a folder where each file contains the complete text of the document.\n\n\nBuilding the topic model\n\n\nThe 'correct' number of topics is going to require trial-and-error. You don't want too few, because that would hide the complexity of your data; too many, and you're starting to split hair.\n\n\n#set the number of desired topics\nnum.topics \n- 20\ntopic.model \n- MalletLDA(num.topics)\n\n\n\n\nNow we\u2019ve told Mallet how many topics to search for; this is a number you\u2019d want to fiddle with, to find the \u2018best\u2019 number of topics. The next line creates a variable \u2018topic.model\u2019 which will eventually be filled by Mallet using the LDA approach, for 20 topics. Let\u2019s get some info on our topic model, on our distribution of words in these materials.\n\n\ntopic.model$loadDocuments(mallet.instances)\n## Get the vocabulary, and some statistics about word frequencies.\n## These may be useful in further curating the stopword list.\nvocabulary \n- topic.model$getVocabulary()\nword.freqs \n- mallet.word.freqs(topic.model)\nhead(word.freqs)\n\n\n\n\nIt is handy to write our output to our project folder periodically; that way you can bring it into other programs if you want to visualize it or explore it further, or if you simply want to have a record of what was going on at a particular point. The line below uses the write.csv command, where the first element is the variable and the second the filename.\n\n\nwrite.csv(word.freqs, \nword-freqs.csv\n )\n\n\n\n\nWord frequencies are handy to look at because it will tell you if you've got words that are 'drowning out' the others. Some of these words you might want to consider adding to your stop words list (and thus, going back to where we first created the mallet.instances and restarting the analysis). By the way, do you see how you might create a bar plot of word frequencies?\n\n\nNow we hit the heavy lifting: generating a topic model. Some of the comments below are very technical; just make sure to run each line of code! (Original code here came from Ben Marwick's analysis of the \nDay of Archaeology\n.)\n\n\n## Optimize hyperparameters every 20 iterations,\n## after 50 burn-in iterations.\ntopic.model$setAlphaOptimization(20, 50)\n## Now train a model. Note that hyperparameter optimization is on, by default.\n## We can specify the number of iterations. Here we'll use a large-ish round number.\n## When you run the next line, a *lot* of information will scroll through your console.\n## Just be patient and wait til it hits that 1000 iteration.\ntopic.model$train(1000)\n## Run through a few iterations where we pick the best topic for each token,\n## rather than sampling from the posterior distribution.\ntopic.model$maximize(10)\n## Get the probability of topics in documents and the probability of words in topics.\n## By default, these functions return raw word counts. Here we want probabilities,\n## so we normalize, and add \nsmoothing\n so that nothing has exactly 0 probability.\ndoc.topics \n- mallet.doc.topics(topic.model, smoothed=T, normalized=T)\ntopic.words \n- mallet.topic.words(topic.model, smoothed=T, normalized=T)\n\n\n\n\nCongratulations! You now have a topic model. Let\u2019s look at some of our topics. What are the top words in topic 7? Notice that R indexes from 1, so this will be the topic that mallet called topic 6:\n\n\nmallet.top.words(topic.model, topic.words[7,])\n\n\n\n\nNow we\u2019ll write the distribution of the topics by document (ie newspaper article) to a csv file that we could explore/visualize with other tools. Then, we\u2019ll take a look at the key words describing each topic.\n\n\ntopic.docs \n- t(doc.topics)\ntopic.docs \n- topic.docs / rowSums(topic.docs)\nwrite.csv(topic.docs, \ntopics-docs.csv\n )\n# that file enables you to see what topics are most present in what issues/documents\n\n## Get a vector containing short names for the topics\ntopics.labels \n- rep(\n, num.topics)\nfor (topic in 1:num.topics) topics.labels[topic] \n- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=\n \n)\n# have a look at keywords for each topic\ntopics.labels\n\nwrite.csv(topics.labels, \ntopics-labels.csv\n)\n\n\n\n\nSome interesting patterns suggest themselves already! But a list of words doesn\u2019t capture the relative importance of particular words in particular topics. A word might appear in more than one topic, for instance, but really dominate one rather than the other. When you examine the csv files, you'll notice that each document is given a series of percentages; these add up to 1, and are indicating the percentage which the different topics contribute to the overall composition of that document. Look for the largest numbers to get a sense of what's going on. We could ask R to cluster similarly composed documents together though...\n\n\nA simple histogram\n\n\nplot(hclust(dist(topic.words)), labels=topics.labels)\n\n\n\n\nDo you see any interesting clusters? Topics that end up in the same clusters we interpret as being related in some fashion. The plot is a bit crowded; in RStudio you can open it in a new window by clickining 'zoom' to see the dendrogram more clearly. You can also google 'hclust cran-r' to find tutorials to make a better plot. One thing we can do is to plot it again without labels, to see the structure a bit better:\n\n\nplot(hclust(dist(topic.words)))\n\n\n\n\nNow, if we want to get really fancy, we can make a network visualization of how topics interlink due to their distribution in documents. The next bit of code does that, and saves in .graphml format, which packages like Gephi http://gephi.org can read.\n\n\ntopic_docs \n- data.frame(topic.docs)\nnames(topic_docs) \n- documents$article_id\n\ninstall.packages(\ncluster\n)\nlibrary(cluster)\ntopic_df_dist \n- as.matrix(daisy(t(topic_docs), metric = \neuclidean\n, stand = TRUE))\n# Change row values to zero if less than row minimum plus row standard deviation\n# keep only closely related documents and avoid a dense spagetti diagram\n# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)\ntopic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) )) \n 0 ] \n- 0\n\n\n\n\ninstall.packages(\nigraph\n)\n# the line above would normally install igraph. However, the latest version is not compatible\n# with this version of R. Thus, go to command line in DHBox, cd to the R folder, cd x86_64-pc-linux-gnu-library, cd 3.0 folder.\n# wget the older version of igraph that'll work: https://cran.r-project.org/src/contrib/Archive/igraph/igraph_0.6-3.tar.gz\n# then, at command line, run the following R command: $ R CMD INSTALL igraph_0.6-3.tar.gz\n# this'll install igraph. Indeed, for any package you can look for older versions of it by slotting in\n# the name of the package in the url above and browsing the archive.\n# Remember, we're working with R version 3.03, from 2013, so we need stuff earlier than that.\n\n# once installed, call it:\nlibrary(igraph)\n\n# we transform the information from the previous code block into a network\ng \n- as.undirected(graph.adjacency(topic_df_dist))\n\n# then we specify the layout and the number of iterations to make it pretty\nlayout1 \n- layout.fruchterman.reingold(g, niter=100)\n\n#then we plot it out\nplot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color= \ngrey\n, edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA)\n\n\n\n\nWhen you look at this network, you can see clusters of documents by virtue of largely shared topics. We export this data in a text format called 'graphml', which can be opened by any text editor, and visualized in nearly any network analysis program for further refinement and analysis. It might be interesting to explore why some issues are so topically focussed, for instance.\n\n\nwrite.graph(g, file=\ncnd.graphml\n, format=\ngraphml\n)\n\n\n\n\nThere are many ways of visualizing and transforming our data. This document only captures a small fraction of the kinds of things you could do. Another good exploration is at http://bridge.library.wisc.edu/hw1a-Rcoding-Jockers.html. Ben Marwick does really fun things with the Day of Archaeology blog posts https://github.com/benmarwick/dayofarchaeology and indeed, some of the code above comes from Marwick\u2019s explorations. Keep your R scripts in your open notebook, and somebody might come along and use them, cite them, improve them, share them! Keep also all your data. Here\u2019s an example from my own work https://github.com/shawngraham/ferguson.", 
            "title": "TM, DHBox"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#topic-modeling-in-r-dhbox-version", 
            "text": "In this exercise, we're going to grab the Colonial Newspaper Database from my github page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi or other such packaged). At the appropriate point, I show you how to import a directory of texts rather than a single file of data, and to feed that into the script.  Go to your DHBox, and click on RStudio. At the right hand side where it says 'project (none)', click and create a new project in a new empty directory. (If you want to put this directory under version control with git, so that you can push your work to your github account, please read  these instructions .)  In the script panel (top left; click on the green plus side and select new R script if this pane isn't open) paste the following code and then run each line by putting the cursor in the line and hitting code   run lines.  install.packages( mallet )\nlibrary( mallet )\ninstall.packages('RCurl')\nlibrary(RCurl)  In the future, now that you've installed these packages you won't have to again, so you can comment them out by placing a # in front.", 
            "title": "Topic Modeling in R, DHBox version"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#importing-data-directly-from-the-web", 
            "text": "Melodee Beals has been using TEI to markup newspaper articles, creating the Colonial Newspapers Database (which she shared on github). We then used Github Pages and an XLST stylesheet to convert that database into a table of comma-separated values, a copy of which is at https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv. We are now going to topic model the text of those newspaper articles, to see what patterns of discourse may lie within.  Now we want to tell R Studio to grab our data from our github page. The thing is, R Studio can easily grab materials from websites where the url is http; but when it is https (as it is with github), things get a bit more fussy. So what we do is use a special package to grab the data, and then shove it into a variable that we can then tease apart for our analysis.  x  - getURL( https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv , .opts = list(ssl.verifypeer = FALSE))  That line reaches out to the webpage and grabs the information and puts it into a variable called  x .  documents  - read.csv(text = x, col.names=c( Article_ID ,  Newspaper Title ,  Newspaper City ,  Newspaper Province ,  Newspaper Country ,  Year ,  Month ,  Day ,  Article Type ,  Text ,  Keywords ), colClasses=rep( character , 3), sep= , , quote= )  Now we've created a variable called  documents  and the  read.csv  command read all of the data pulled into x, and tells R that  documents  has columns called \"Newspaper Title\" etc. When we only want information from a particular column, we modify the variable slightly, eg  documents$Keywords  would only look at the information in the keywords column. Let's go on a brief digression and actually do that, and see what we learn about this corpus:  counts  - table(documents$Newspaper.City)  We tell R to make a new variable called 'counts', and fill it with the information from the column 'newspaper city' in 'documents'. It counts them up! Let's make a simple barplot:  barplot(counts, main= Cities , xlab= Number of Articles )  The plot will appear in the bottom right pane of RStudio. You can click on 'zoom' to see the plot in a popup window. You can also export it as a png or pdf file. Clearly, we\u2019re getting an Edinburgh/Glasgow perspective on things. And somewhere in our data, there\u2019s a mispelled \u2018Edinbugh\u2019. Do you see any other error(s) in the plot? How would you correct it(them)?  Let's do the same thing for year, and count the number of articles per year in this corpus:  years  - table(documents$Year)\nbarplot(years, main= Publication Year , xlab= Year , ylab= Number of Articles )  There\u2019s a lot of material in 1789, another peak around 1819, againg in the late 1830s. We can ask ourselves now: is this an artefact of the data, or of our collection methods? This would be a question a reviewer would want answers to. Let\u2019s assume for now that these two plots are \u2018true\u2019 - that, for whatever reasons, only Edinburgh and Glasgow were concerned with these colonial reports, and that they were particulary interested during those three periods. This is already an interesting question that we as historians would want to explore. Try making some more visualizations like this of other aspects of the data. What other patterns do you see that are worth investigating?  Now, let's return to getting our data ready to create a topic model. In the line below, note that there is a file called 'en.txt' that it wants to load up. You need to create this file; it's a list of stopwords, or common words that we think do not add value to our model (words like 'the', 'and', 'of' and so on.) The decision of which words to exclude from our analysis is of course a theoretical position...  To create that file, click on the 'new file' icon in the tool ribbon and select new text file. This will open a blank file in the edit window here. Copy and paste the list of words at  en.txt  into that blank file and save it as en.txt. This file was put together by Matt Jockers.  mallet.instances  - mallet.import(documents$Article_ID, documents$Text,  en.txt , token.regexp =  \\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L} )  That line above passes the article ID and the text of our newspaper articles to the Mallet routine.  The stopwords list is generic; it might need to be curated to take into account the pecularities of your data. You might want to create your own, one for each project given the particulars of your project. Note that Jockers compiled hist stoplist for his research in literary history of the 19th century. Your mileage may vary! Finally, the last bit after \u2018token.regexp\u2019 applies a regular expression against our newspaper articles, cleaning them up.", 
            "title": "Importing data directly from the web"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#reading-data-from-a-directory", 
            "text": "This is an alternative way of ingesting documents for topic modeling. Earlier, you learned how to use wget and some other scripts to download full text documents from Canadiana.org as well as from the Provincial Archives in Quebec (the Shawville Equity). The code below loads those documents into Mallet, after which you can proceed to build a topic model. In the command line, cd into your folder that has your downloaded materials. At the command line, cd into your folder, and type $ pwd to get the full path. Copy it, go back to RStudio, and paste it into the line below between the \" marks.  documents  - mallet.read.dir( /home/shawngraham/equity )\n\n# your path will probably look like /home/your-account-in-dhbox/your-folder-of-materials\n\nmallet.instances  - mallet.import(documents$id, documents$text,  en.txt , token.regexp =  \\\\p{L}[\\\\p{L}\\\\p{P}]+\\\\p{L} )  nb do either one or the other, but not both: read from a file, where each row contains the complete text of the document, or read from a folder where each file contains the complete text of the document.", 
            "title": "Reading data from a directory"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#building-the-topic-model", 
            "text": "The 'correct' number of topics is going to require trial-and-error. You don't want too few, because that would hide the complexity of your data; too many, and you're starting to split hair.  #set the number of desired topics\nnum.topics  - 20\ntopic.model  - MalletLDA(num.topics)  Now we\u2019ve told Mallet how many topics to search for; this is a number you\u2019d want to fiddle with, to find the \u2018best\u2019 number of topics. The next line creates a variable \u2018topic.model\u2019 which will eventually be filled by Mallet using the LDA approach, for 20 topics. Let\u2019s get some info on our topic model, on our distribution of words in these materials.  topic.model$loadDocuments(mallet.instances)\n## Get the vocabulary, and some statistics about word frequencies.\n## These may be useful in further curating the stopword list.\nvocabulary  - topic.model$getVocabulary()\nword.freqs  - mallet.word.freqs(topic.model)\nhead(word.freqs)  It is handy to write our output to our project folder periodically; that way you can bring it into other programs if you want to visualize it or explore it further, or if you simply want to have a record of what was going on at a particular point. The line below uses the write.csv command, where the first element is the variable and the second the filename.  write.csv(word.freqs,  word-freqs.csv  )  Word frequencies are handy to look at because it will tell you if you've got words that are 'drowning out' the others. Some of these words you might want to consider adding to your stop words list (and thus, going back to where we first created the mallet.instances and restarting the analysis). By the way, do you see how you might create a bar plot of word frequencies?  Now we hit the heavy lifting: generating a topic model. Some of the comments below are very technical; just make sure to run each line of code! (Original code here came from Ben Marwick's analysis of the  Day of Archaeology .)  ## Optimize hyperparameters every 20 iterations,\n## after 50 burn-in iterations.\ntopic.model$setAlphaOptimization(20, 50)\n## Now train a model. Note that hyperparameter optimization is on, by default.\n## We can specify the number of iterations. Here we'll use a large-ish round number.\n## When you run the next line, a *lot* of information will scroll through your console.\n## Just be patient and wait til it hits that 1000 iteration.\ntopic.model$train(1000)\n## Run through a few iterations where we pick the best topic for each token,\n## rather than sampling from the posterior distribution.\ntopic.model$maximize(10)\n## Get the probability of topics in documents and the probability of words in topics.\n## By default, these functions return raw word counts. Here we want probabilities,\n## so we normalize, and add  smoothing  so that nothing has exactly 0 probability.\ndoc.topics  - mallet.doc.topics(topic.model, smoothed=T, normalized=T)\ntopic.words  - mallet.topic.words(topic.model, smoothed=T, normalized=T)  Congratulations! You now have a topic model. Let\u2019s look at some of our topics. What are the top words in topic 7? Notice that R indexes from 1, so this will be the topic that mallet called topic 6:  mallet.top.words(topic.model, topic.words[7,])  Now we\u2019ll write the distribution of the topics by document (ie newspaper article) to a csv file that we could explore/visualize with other tools. Then, we\u2019ll take a look at the key words describing each topic.  topic.docs  - t(doc.topics)\ntopic.docs  - topic.docs / rowSums(topic.docs)\nwrite.csv(topic.docs,  topics-docs.csv  )\n# that file enables you to see what topics are most present in what issues/documents\n\n## Get a vector containing short names for the topics\ntopics.labels  - rep( , num.topics)\nfor (topic in 1:num.topics) topics.labels[topic]  - paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=   )\n# have a look at keywords for each topic\ntopics.labels\n\nwrite.csv(topics.labels,  topics-labels.csv )  Some interesting patterns suggest themselves already! But a list of words doesn\u2019t capture the relative importance of particular words in particular topics. A word might appear in more than one topic, for instance, but really dominate one rather than the other. When you examine the csv files, you'll notice that each document is given a series of percentages; these add up to 1, and are indicating the percentage which the different topics contribute to the overall composition of that document. Look for the largest numbers to get a sense of what's going on. We could ask R to cluster similarly composed documents together though...", 
            "title": "Building the topic model"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-dhbox/#a-simple-histogram", 
            "text": "plot(hclust(dist(topic.words)), labels=topics.labels)  Do you see any interesting clusters? Topics that end up in the same clusters we interpret as being related in some fashion. The plot is a bit crowded; in RStudio you can open it in a new window by clickining 'zoom' to see the dendrogram more clearly. You can also google 'hclust cran-r' to find tutorials to make a better plot. One thing we can do is to plot it again without labels, to see the structure a bit better:  plot(hclust(dist(topic.words)))  Now, if we want to get really fancy, we can make a network visualization of how topics interlink due to their distribution in documents. The next bit of code does that, and saves in .graphml format, which packages like Gephi http://gephi.org can read.  topic_docs  - data.frame(topic.docs)\nnames(topic_docs)  - documents$article_id\n\ninstall.packages( cluster )\nlibrary(cluster)\ntopic_df_dist  - as.matrix(daisy(t(topic_docs), metric =  euclidean , stand = TRUE))\n# Change row values to zero if less than row minimum plus row standard deviation\n# keep only closely related documents and avoid a dense spagetti diagram\n# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)\ntopic_df_dist[ sweep(topic_df_dist, 1, (apply(topic_df_dist,1,min) + apply(topic_df_dist,1,sd) ))   0 ]  - 0  install.packages( igraph )\n# the line above would normally install igraph. However, the latest version is not compatible\n# with this version of R. Thus, go to command line in DHBox, cd to the R folder, cd x86_64-pc-linux-gnu-library, cd 3.0 folder.\n# wget the older version of igraph that'll work: https://cran.r-project.org/src/contrib/Archive/igraph/igraph_0.6-3.tar.gz\n# then, at command line, run the following R command: $ R CMD INSTALL igraph_0.6-3.tar.gz\n# this'll install igraph. Indeed, for any package you can look for older versions of it by slotting in\n# the name of the package in the url above and browsing the archive.\n# Remember, we're working with R version 3.03, from 2013, so we need stuff earlier than that.\n\n# once installed, call it:\nlibrary(igraph)\n\n# we transform the information from the previous code block into a network\ng  - as.undirected(graph.adjacency(topic_df_dist))\n\n# then we specify the layout and the number of iterations to make it pretty\nlayout1  - layout.fruchterman.reingold(g, niter=100)\n\n#then we plot it out\nplot(g, layout=layout1, edge.curved = TRUE, vertex.size = 1, vertex.color=  grey , edge.arrow.size = 0, vertex.label.dist=0.5, vertex.label = NA)  When you look at this network, you can see clusters of documents by virtue of largely shared topics. We export this data in a text format called 'graphml', which can be opened by any text editor, and visualized in nearly any network analysis program for further refinement and analysis. It might be interesting to explore why some issues are so topically focussed, for instance.  write.graph(g, file= cnd.graphml , format= graphml )  There are many ways of visualizing and transforming our data. This document only captures a small fraction of the kinds of things you could do. Another good exploration is at http://bridge.library.wisc.edu/hw1a-Rcoding-Jockers.html. Ben Marwick does really fun things with the Day of Archaeology blog posts https://github.com/benmarwick/dayofarchaeology and indeed, some of the code above comes from Marwick\u2019s explorations. Keep your R scripts in your open notebook, and somebody might come along and use them, cite them, improve them, share them! Keep also all your data. Here\u2019s an example from my own work https://github.com/shawngraham/ferguson.", 
            "title": "A simple histogram"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-yourmachine/", 
            "text": "Topic Modeling in R On Your Own machine\n\n\nIn this exercise, we're going to grab an archived copy of Melodee Beals' Colonial Newspaper Database from my github page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi). The walkthrough \ncan be found here\n. Each gray block is something to copy-and-paste into your script window in R Studio. Then, put the cursor at the start of the first line, and hit ctrl+enter to get RStudio to execute each line. In the walkthrough, when you get to another gray block, just copy and paste it into your script window after the earlier block. Work your way through the walkthrough. The walkthrough gives you an indication of what the output should look like as you move through it. (The walkthrough was written inside R, and then turned into HTML using an R package called 'Knitr'. You can see that this has implications for open research! For reference, \nhere's the original Rmd (R markdown) file that generated the walkthrough\n.)\n\n\nWhen you start RStudio the first time for this exercise\n make sure to create a new project in a new directory.\n\n\nBy the way: when you run this line: \ntopic.model$train(1000)\n your console will fill up with data as it iterates 1000 times over the entire corpus, fitting a topic model to it. This is as it should be!\n\n\nIn this way, you'll build up an entire script for topic modeling materials you find on the web. You can then save your script and upload it to your open notebook. In the future, you'd be able to make just a few changes here and there in order to grab and explore different data.\n\n\nMake a note in your open notebook about your process and your observations.\n\n\nGoing further\n If you wanted to use that script on the materials you collected in module 2, you would have to tell R to load up those materials from a directory, rather than by reading a csv file. Take a look at \nmy script for topic modeling the Ferguson Grand Jury documents\n, especially this line:\n\n\ndocuments \n- mallet.read.dir(\"originaldocs/1000chunks/\")\n\n\nYou feed it the path to your documents. If you are on a windows machine, the path would look a bit different, for instance:\n\n\n\"C:\\\\research\\\\originaldocs\\\\1000chunks\\\\\"", 
            "title": "TM, Local"
        }, 
        {
            "location": "/supporting materials/topicmodel-r-yourmachine/#topic-modeling-in-r-on-your-own-machine", 
            "text": "In this exercise, we're going to grab an archived copy of Melodee Beals' Colonial Newspaper Database from my github page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi). The walkthrough  can be found here . Each gray block is something to copy-and-paste into your script window in R Studio. Then, put the cursor at the start of the first line, and hit ctrl+enter to get RStudio to execute each line. In the walkthrough, when you get to another gray block, just copy and paste it into your script window after the earlier block. Work your way through the walkthrough. The walkthrough gives you an indication of what the output should look like as you move through it. (The walkthrough was written inside R, and then turned into HTML using an R package called 'Knitr'. You can see that this has implications for open research! For reference,  here's the original Rmd (R markdown) file that generated the walkthrough .)  When you start RStudio the first time for this exercise  make sure to create a new project in a new directory.  By the way: when you run this line:  topic.model$train(1000)  your console will fill up with data as it iterates 1000 times over the entire corpus, fitting a topic model to it. This is as it should be!  In this way, you'll build up an entire script for topic modeling materials you find on the web. You can then save your script and upload it to your open notebook. In the future, you'd be able to make just a few changes here and there in order to grab and explore different data.  Make a note in your open notebook about your process and your observations.  Going further  If you wanted to use that script on the materials you collected in module 2, you would have to tell R to load up those materials from a directory, rather than by reading a csv file. Take a look at  my script for topic modeling the Ferguson Grand Jury documents , especially this line:  documents  - mallet.read.dir(\"originaldocs/1000chunks/\")  You feed it the path to your documents. If you are on a windows machine, the path would look a bit different, for instance:  \"C:\\\\research\\\\originaldocs\\\\1000chunks\\\\\"", 
            "title": "Topic Modeling in R On Your Own machine"
        }, 
        {
            "location": "/supporting materials/netviz/", 
            "text": "Using igraph to visualize network data\n\n\nIn this exercise, we are doing a quick first pass on the network data generated from the Republic of Texas correspondence. I am providing you with the \nedge list\n, the links of to - from that we generated earlier. I am also providing you the \nnode list\n, the list of individuals extracted from that edge list. If we keep our nodes and edges separated in csv tables, we can add other \nattributes\n later (things like date, or number of times a pair of individuals corresponded ie weight, or gender, or location, or what have you) to create different views or analyses. I used open refine (see the \nlesson on open refine\n) to clean this data up for you. Remember, 80 percent of all digital history work involves cleaning data!\n\n\nHere is the data that you need; download these, and use the filemanager to load them into your dhbox into your new R project for this tutorial.\n\n\n\n\nlist of links\n\n\nlist of nodes\n\n\n\n\nThis tutorial will illustrate to you some of the ways the \nigraph\n package can be used to create quick visualizations or to generate network statistics. These of course on their own mean very little: this is where your historical sensibility comes into play, triangulating this generated data with other things you know about the historical context of the time, place and players!\n\n\nRemember to make a new project in RStudio by clicking the down arrow on the R button at the right hand side of the R Studio interface; start the project in a new folder. Then using the DHBox filemanager, upload the nodes and links data from your computer to that folder you just created.\n\n\nInstalling igraph\n\n\nOnce that's accomplished, go back into R Studio and start a new script:\n\n\n# install igraph; this might take a long time\n# you only run this line the first time you install igraph:\ninstall.packages('igraph')\n\n# a lot of stuff gets downloaded and installed.\n\n# now tell RStudio you want to use the igraph pacakge and its functions:\nlibrary('igraph')\n\n\n\n\ngetting the data into your project\n\n\nFor future reference, we're adapting our script from a more in-depth \ntutorial\n.\n\n\nBringing data into R is straightforward. We create a variable 'nodes' and a variable for 'links' and load the data from our csv files into them:\n\n\n# now let's load up the data by putting the csv files into nodes and links.\nnodes \n- read.csv(\ntexasnodes.csv\n, header=T, as.is=T)\nlinks \n- read.csv(\ntexaslinks.csv\n, header=T, as.is=T)\n\n\n\n\nWe can examine the start or end of a variable with the head or tail command; these look at the first few lines at the start ('head') or end ('tail') of the variable, eg:\n\n\n#examine data\nhead(nodes)\n\nhead(links)\n\nnrow(nodes); length(unique(nodes$id))\n# which gives the number of nodes in our data\n\nnrow(links); nrow(unique(links[,c(\nsource\n, \ntarget\n)]))\n# which gives the number of sources, and number of targets\n# which means some people sent more than one letter, and some people received more than one letter\n\n\n\n\nLet's rearrange things so that instead of this:\n\n\nAlice -\n Bob, 1 letter\nAlice -\n Bob, 1 letter\nAlice -\n Bob, 1 letter\n\n\n\n\nwe have this:\n\n\nAlice -\n Bob, 3 letters\n\n\n\n\nThat is, we're going to count up the number of times a particular relationship exists, and assign that count to the weight column. Much tidier all around! We do that like this:\n\n\nlinks \n- aggregate(links[,3], links[,-3], sum)\n\nlinks \n- links[order(links$from, links$to),]\n\ncolnames(links)[3] \n- \nweight\n\n\nrownames(links) \n- NULL\n\nhead(links)\n\n\n\n\nYou should see this:\n\n\n head(links)\n           source            target weight\n1      James Webb       Abb6 Anduz6      1\n2   A. de Saligny Abner S. Lipscomb      1\n3     E. W. Moore Abner S. Lipscomb      1\n4  James Hamilton Abner S. Lipscomb     14\n5     James Treat Abner S. Lipscomb     11\n6 Nathaniel Amory Abner S. Lipscomb      1\n\n\n\n\n\nNow, let's tell R to stitch our links together into a network object (a data frame) that it can visualize and analyze:\n\n\n# let's make a net\n# notice that we are telling igraph that the network is directed, that the relationship Alice to Bob is different than Bob's to Alice (Alice is the _sender_, and Bob is the _receiver_)\n\nnet \n- graph_from_data_frame(d=links, vertices=nodes, directed=T)\n\n# type 'net' again and run the line to see how the network is represented.\nnet\n\n# let's visualizae it\nplot(net, edge.arrow.size=.4,vertex.label=NA)\n\n# two quite distinct groupings, it would appear.\n\n\n\n\n\nBefore we jump down the rabbit hole of visualization, let's recognize right now that \nvisualizing\n a network is only rarely of analytical value. The value of network analysis comes from the various questions we can now start posing of our data when it is represented as a network. In this correspondence network, who is in the centre of the web? To whom would information flow? To whom would information leak? Are there cliques or ingroups? When we identify such individuals, how does that confirm or confound our expectations of the period and place?\n\n\nMany different kinds of metrics can be calculated (and \nthis tutorial will show you how\n) but it's always worth remembering that a metric is only meaningful for a given network when we're dealing with like things - a network of people who write letters to one another; a network of banks that swap mortgages with one another. These are called 'one mode networks'. A network of people connected to the banks they use - a two mode network, because it connects two different kinds of things - might be useful to \nvisualize\n but the metrics calculated might not be \nvalid\n if the metric was designed to work on a one-mode network (for more on this and allied issues, see \nScott Weingart\n).\n\n\nGiven our correspondence network, let's imagine that 'closeness' (eg a measure of how central a person is) and 'betweenness' (a measure of how many different strands of the network pass through this person) are the most historically interesting. Further, we're going to try to determine if there are subgroups in our network, cliques.\n\n\n## the 'degree' of a node is the count of its connections. In this code chunk, we calculate degree, then make both a histogram of the counts and a plot of the network where we size the nodes proportionately to their degree. What do we learn from these two visualizations?\ndeg \n- degree(net, mode=\nall\n)\nhist(deg, breaks=1:vcount(net)-1, main=\nHistogram of node degree\n)\nplot(net, vertex.size=deg*2, vertex.label = NA)\n## write this info to file for safekeeping\nwrite.csv(deg, 'degree.csv')\n\n\n\n\nNow we'll look at closeness. If you know the width or \ndiameter\n of your network (the maximum number of steps to get across it), then the node that is on average the shortest number of steps from all the others is the one that is most close. We calculate it like this:\n\n\nclosepeople \n- closeness(net, mode=\nall\n, weights=NA)\nsort(closepeople, decreasing = T) # so that we see who is most close first\nwrite.csv(closepeople, 'closeness.csv') # so we have it on file.\n\n\n\n\n\nWe can ask which individuals are hubs, and which are authorities. In the lingo, 'hubs' are individuals with many outgoing links (they \nsent\n lots of letters) while 'authorities' are individuals who \nreceived\n lots of letters. In the code below,\n\n\nhs \n- hub_score(net, weights=NA)$vector\nas \n- authority_score(net, weights=NA)$vector\n\npar(mfrow=c(1,2))\n\n# vertex.label.cex sets the size of the label; play with the sizes until you see something appealing.\nplot(net, vertex.size=hs*40, vertex.label.cex =.2, edge.arrow.size=.1, main=\nHubs\n)\nplot(net, vertex.size=as*20, vertex.label = NA, edge.arrow.size=.1, main=\nAuthorities\n)\n\n\n\n\nCan you work out what command to give to write the hub score or the authority scores to a file?\n\n\nLet's look for 'modules' within this network. Broadly speaking, these are clumps of nodes that have more or less the same pattern of ties between them, \nwithin\n the group, than without.\n\n\ncfg \n- cluster_fast_greedy(as.undirected(net))\n\nlapply(cfg, function(x) write.table( data.frame(x), 'cfg.csv'  , append= T, sep=',' ))\n\n\n\n\nWe create a new variable called 'cfg' and get the 'cluster_fast_greedy' algorithm to perform its calculations. The next line writes the groups to a file, separating each group with an x. (If you tried 'write.csv' as before, you'll get an error message because the output of the algorithm gives a different kind of data type. R is fussy like this.) Examine that file - what groups do you spot? What might these mean, if you went back to the content of the original letters?\n\n\nVisualization\n\n\nThe line below will plot out our network, colouring it by the communities discerned above:\n\n\nplot(cfg, net, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\nCommunities\n)\n\n\n\n\nYou can export the plot by clicking on the 'export' button in the plot panel, to pdf or to png. But this is a pretty ugly network. We need to apply a \nlayout\n to try to make it more visually understandable. There are many different layout options in igraph. We'll assign the layout we want to a variable, and then we'll give that variable to the plot command:\n\n\nl1 \n- layout_with_fr(net)\nplot(cfg, net, layout=l1, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main=\nCommunities\n)\n\n\n\n\nThe 'layout_with_fr' is calling the 'Fruchterman-Reingold' algorithm, a kind of layout that imagines each node as a repulsive power, pushing against all the other nodes (it's part of a family of layouts called 'Forced Atlast'). That also means that if you ran the plot command a couple of times, the nodes might end up in different places each time - they are jostling for relative position, and this positioning itself carries no meaning in and of itself (so if a node is at the top of the screen, or the centre of the screen, don't read that to mean anything about importance).\n\n\nWe'll finish with a piece of code that will draw thumbnails of your network using every possible layout in the package. This will give you a sense of which layouts might best suit the historical story you wish to tell. Good luck! Remember to save your script, and upload the entire project folder to your github repository.\n\n\nlayouts \n- grep(\n^layout_\n, ls(\npackage:igraph\n), value=TRUE)[-1]\n\n# Remove layouts that do not apply to our graph.\n\nlayouts \n- layouts[!grepl(\nbipartite|merge|norm|sugiyama|tree\n, layouts)]\n\npar(mfrow=c(3,3), mar=c(1,1,1,1))\n\nfor (layout in layouts) {\n\n  print(layout)\n  V(net)$label \n- NA\n  l \n- do.call(layout, list(net))\n\n  plot(net, edge.arrow.mode=0, layout=l, main=layout) }", 
            "title": "igraph"
        }, 
        {
            "location": "/supporting materials/netviz/#using-igraph-to-visualize-network-data", 
            "text": "In this exercise, we are doing a quick first pass on the network data generated from the Republic of Texas correspondence. I am providing you with the  edge list , the links of to - from that we generated earlier. I am also providing you the  node list , the list of individuals extracted from that edge list. If we keep our nodes and edges separated in csv tables, we can add other  attributes  later (things like date, or number of times a pair of individuals corresponded ie weight, or gender, or location, or what have you) to create different views or analyses. I used open refine (see the  lesson on open refine ) to clean this data up for you. Remember, 80 percent of all digital history work involves cleaning data!  Here is the data that you need; download these, and use the filemanager to load them into your dhbox into your new R project for this tutorial.   list of links  list of nodes   This tutorial will illustrate to you some of the ways the  igraph  package can be used to create quick visualizations or to generate network statistics. These of course on their own mean very little: this is where your historical sensibility comes into play, triangulating this generated data with other things you know about the historical context of the time, place and players!  Remember to make a new project in RStudio by clicking the down arrow on the R button at the right hand side of the R Studio interface; start the project in a new folder. Then using the DHBox filemanager, upload the nodes and links data from your computer to that folder you just created.", 
            "title": "Using igraph to visualize network data"
        }, 
        {
            "location": "/supporting materials/netviz/#installing-igraph", 
            "text": "Once that's accomplished, go back into R Studio and start a new script:  # install igraph; this might take a long time\n# you only run this line the first time you install igraph:\ninstall.packages('igraph')\n\n# a lot of stuff gets downloaded and installed.\n\n# now tell RStudio you want to use the igraph pacakge and its functions:\nlibrary('igraph')", 
            "title": "Installing igraph"
        }, 
        {
            "location": "/supporting materials/netviz/#getting-the-data-into-your-project", 
            "text": "For future reference, we're adapting our script from a more in-depth  tutorial .  Bringing data into R is straightforward. We create a variable 'nodes' and a variable for 'links' and load the data from our csv files into them:  # now let's load up the data by putting the csv files into nodes and links.\nnodes  - read.csv( texasnodes.csv , header=T, as.is=T)\nlinks  - read.csv( texaslinks.csv , header=T, as.is=T)  We can examine the start or end of a variable with the head or tail command; these look at the first few lines at the start ('head') or end ('tail') of the variable, eg:  #examine data\nhead(nodes)\n\nhead(links)\n\nnrow(nodes); length(unique(nodes$id))\n# which gives the number of nodes in our data\n\nnrow(links); nrow(unique(links[,c( source ,  target )]))\n# which gives the number of sources, and number of targets\n# which means some people sent more than one letter, and some people received more than one letter  Let's rearrange things so that instead of this:  Alice -  Bob, 1 letter\nAlice -  Bob, 1 letter\nAlice -  Bob, 1 letter  we have this:  Alice -  Bob, 3 letters  That is, we're going to count up the number of times a particular relationship exists, and assign that count to the weight column. Much tidier all around! We do that like this:  links  - aggregate(links[,3], links[,-3], sum)\n\nlinks  - links[order(links$from, links$to),]\n\ncolnames(links)[3]  -  weight \n\nrownames(links)  - NULL\n\nhead(links)  You should see this:   head(links)\n           source            target weight\n1      James Webb       Abb6 Anduz6      1\n2   A. de Saligny Abner S. Lipscomb      1\n3     E. W. Moore Abner S. Lipscomb      1\n4  James Hamilton Abner S. Lipscomb     14\n5     James Treat Abner S. Lipscomb     11\n6 Nathaniel Amory Abner S. Lipscomb      1  Now, let's tell R to stitch our links together into a network object (a data frame) that it can visualize and analyze:  # let's make a net\n# notice that we are telling igraph that the network is directed, that the relationship Alice to Bob is different than Bob's to Alice (Alice is the _sender_, and Bob is the _receiver_)\n\nnet  - graph_from_data_frame(d=links, vertices=nodes, directed=T)\n\n# type 'net' again and run the line to see how the network is represented.\nnet\n\n# let's visualizae it\nplot(net, edge.arrow.size=.4,vertex.label=NA)\n\n# two quite distinct groupings, it would appear.  Before we jump down the rabbit hole of visualization, let's recognize right now that  visualizing  a network is only rarely of analytical value. The value of network analysis comes from the various questions we can now start posing of our data when it is represented as a network. In this correspondence network, who is in the centre of the web? To whom would information flow? To whom would information leak? Are there cliques or ingroups? When we identify such individuals, how does that confirm or confound our expectations of the period and place?  Many different kinds of metrics can be calculated (and  this tutorial will show you how ) but it's always worth remembering that a metric is only meaningful for a given network when we're dealing with like things - a network of people who write letters to one another; a network of banks that swap mortgages with one another. These are called 'one mode networks'. A network of people connected to the banks they use - a two mode network, because it connects two different kinds of things - might be useful to  visualize  but the metrics calculated might not be  valid  if the metric was designed to work on a one-mode network (for more on this and allied issues, see  Scott Weingart ).  Given our correspondence network, let's imagine that 'closeness' (eg a measure of how central a person is) and 'betweenness' (a measure of how many different strands of the network pass through this person) are the most historically interesting. Further, we're going to try to determine if there are subgroups in our network, cliques.  ## the 'degree' of a node is the count of its connections. In this code chunk, we calculate degree, then make both a histogram of the counts and a plot of the network where we size the nodes proportionately to their degree. What do we learn from these two visualizations?\ndeg  - degree(net, mode= all )\nhist(deg, breaks=1:vcount(net)-1, main= Histogram of node degree )\nplot(net, vertex.size=deg*2, vertex.label = NA)\n## write this info to file for safekeeping\nwrite.csv(deg, 'degree.csv')  Now we'll look at closeness. If you know the width or  diameter  of your network (the maximum number of steps to get across it), then the node that is on average the shortest number of steps from all the others is the one that is most close. We calculate it like this:  closepeople  - closeness(net, mode= all , weights=NA)\nsort(closepeople, decreasing = T) # so that we see who is most close first\nwrite.csv(closepeople, 'closeness.csv') # so we have it on file.  We can ask which individuals are hubs, and which are authorities. In the lingo, 'hubs' are individuals with many outgoing links (they  sent  lots of letters) while 'authorities' are individuals who  received  lots of letters. In the code below,  hs  - hub_score(net, weights=NA)$vector\nas  - authority_score(net, weights=NA)$vector\n\npar(mfrow=c(1,2))\n\n# vertex.label.cex sets the size of the label; play with the sizes until you see something appealing.\nplot(net, vertex.size=hs*40, vertex.label.cex =.2, edge.arrow.size=.1, main= Hubs )\nplot(net, vertex.size=as*20, vertex.label = NA, edge.arrow.size=.1, main= Authorities )  Can you work out what command to give to write the hub score or the authority scores to a file?  Let's look for 'modules' within this network. Broadly speaking, these are clumps of nodes that have more or less the same pattern of ties between them,  within  the group, than without.  cfg  - cluster_fast_greedy(as.undirected(net))\n\nlapply(cfg, function(x) write.table( data.frame(x), 'cfg.csv'  , append= T, sep=',' ))  We create a new variable called 'cfg' and get the 'cluster_fast_greedy' algorithm to perform its calculations. The next line writes the groups to a file, separating each group with an x. (If you tried 'write.csv' as before, you'll get an error message because the output of the algorithm gives a different kind of data type. R is fussy like this.) Examine that file - what groups do you spot? What might these mean, if you went back to the content of the original letters?", 
            "title": "getting the data into your project"
        }, 
        {
            "location": "/supporting materials/netviz/#visualization", 
            "text": "The line below will plot out our network, colouring it by the communities discerned above:  plot(cfg, net, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main= Communities )  You can export the plot by clicking on the 'export' button in the plot panel, to pdf or to png. But this is a pretty ugly network. We need to apply a  layout  to try to make it more visually understandable. There are many different layout options in igraph. We'll assign the layout we want to a variable, and then we'll give that variable to the plot command:  l1  - layout_with_fr(net)\nplot(cfg, net, layout=l1, vertex.size = 1, vertex.label.cex =.2, edge.arrow.size=.1, main= Communities )  The 'layout_with_fr' is calling the 'Fruchterman-Reingold' algorithm, a kind of layout that imagines each node as a repulsive power, pushing against all the other nodes (it's part of a family of layouts called 'Forced Atlast'). That also means that if you ran the plot command a couple of times, the nodes might end up in different places each time - they are jostling for relative position, and this positioning itself carries no meaning in and of itself (so if a node is at the top of the screen, or the centre of the screen, don't read that to mean anything about importance).  We'll finish with a piece of code that will draw thumbnails of your network using every possible layout in the package. This will give you a sense of which layouts might best suit the historical story you wish to tell. Good luck! Remember to save your script, and upload the entire project folder to your github repository.  layouts  - grep( ^layout_ , ls( package:igraph ), value=TRUE)[-1]\n\n# Remove layouts that do not apply to our graph.\n\nlayouts  - layouts[!grepl( bipartite|merge|norm|sugiyama|tree , layouts)]\n\npar(mfrow=c(3,3), mar=c(1,1,1,1))\n\nfor (layout in layouts) {\n\n  print(layout)\n  V(net)$label  - NA\n  l  - do.call(layout, list(net))\n\n  plot(net, edge.arrow.mode=0, layout=l, main=layout) }", 
            "title": "Visualization"
        }
    ]
}