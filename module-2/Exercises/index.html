<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Exercises - HIST3814o Crafting Digital History</title>
  

  <link rel="shortcut icon" href="../../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Exercises";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
  <script src="../../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> HIST3814o Crafting Digital History</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../introduction/crafting-digital-history/">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>1. Open Access Research</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../module-1/Open-Access-Research/">Why you should be open</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../module-1/Exercises/">Exercises</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>2. Finding Data</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../Finding Data/">How do we find data?</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">Exercises</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#module-2-exercises">Module 2: Exercises</a></li>
                
            
                <li class="toctree-l3"><a href="#background">Background</a></li>
                
            
                <li class="toctree-l3"><a href="#exercise-1-the-dream-case">EXERCISE 1: The Dream Case</a></li>
                
            
                <li class="toctree-l3"><a href="#exercise-2-wget">EXERCISE 2: Wget</a></li>
                
            
                <li class="toctree-l3"><a href="#exercise-3-tei">EXERCISE 3: TEI</a></li>
                
            
                <li class="toctree-l3"><a href="#exercise-4-apis">EXERCISE 4: APIs</a></li>
                
            
                <li class="toctree-l3"><a href="#exercise-5-mining-twitter">EXERCISE 5: Mining Twitter</a></li>
                
            
                <li class="toctree-l3"><a href="#exercise-6-using-tesseract-to-turn-a-pdf-into-text">EXERCISE 6: Using Tesseract to turn a pdf into text</a></li>
                
                    <li><a class="toctree-l4" href="#wget-command-to-grab-the-equity">wget command to grab The Equity</a></li>
                
            
            </ul>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>3. Fixing Data</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../module-3/Wrangling Data/">Data is messy</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../module-3/Exercises/">Exercises</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>4. Analysis</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../module-4/Seeing Patterns/">Seeing Patterns</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../module-4/Exercises/">Exercises</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>5. Visualization</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../module-5/Humanities Visualization/">Communicating your Findings</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../module-5/Exercises/">Exercises</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../conclusion/conclusion/">6. Final Thoughts</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Appendices</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/tei/">Text Encoding</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/ner/">NER</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/open-refine/">Open Refine</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/regex/">Regular Expressions (regex)</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/regex-ner/">Going further with regex</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/regexex/">Regex & the Republic of Texas</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/cyoa.txt/">CYOA</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/geoparsing-w-python.txt/">Geoparsing with Python</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/gephi.txt/">SNA with Gephi</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/graphing-the-net.txt/">Graphing the Net</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/multimode-networks.txt/">Multimode Networks</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/leaflet.txt/">Leaflet</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/glitch/">Glitch</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/quick-intro-r/">Quick Intro to R</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/git-rstudio/">Git</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/topicmodel-r-dhbox/">TM, DHBox</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/topicmodel-r-yourmachine/">TM, Local</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../supporting materials/netviz/">igraph</a>
        
    </li>

        
    </ul>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">HIST3814o Crafting Digital History</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>2. Finding Data &raquo;</li>
        
      
    
    <li>Exercises</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="https://github.com/shawngraham/dhworkbook/tree/gh-pages" class="icon icon-github"> Edit on GitHub</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="module-2-exercises">Module 2: Exercises</h1>
<p><em>All five exercises are on this page. Don't forget to scroll. If you have difficulties, or if the instructions need clarification, please click the 'issues' button and leave a note. Feel free to fork and improve these instructions, if you are so inclined. Remember, these exercises get progressively more difficult, and will require you to either download materials or read materials on other websites. Give yourself plenty of time. Try them all, and remember you can turn to your classmates for help. Work together!</em></p>
<p><em>DO NOT suffer in silence as you try these exercises! Annotate, ask for help, set up an appointment, or find me in person.</em></p>
<h1 id="background">Background</h1>
<p>Where do we go to find data? Part of that problem is solved by knowing what question you are asking, and what <em>kinds</em> of data would help solve that question. Let's assume that you have a pretty good question you want an answer to - say, something concerning social and household history in early Ottawa, like what was the role of 'corner' stores (if such things exist?) in fostering a sense of neighbourhood - and begin thinking about how you'd find data to explore that question.</p>
<p>The exercises in this module cover:</p>
<ul>
<li>The Dream Case</li>
<li>Wget</li>
<li>Writing a program to extract data from a webpage</li>
<li>Collecting data from Twitter</li>
</ul>
<p>There is so much data available; with these methods, we can gather enormous amounts that will let us see large-scale macroscopic patterns. At the same time, it allows us to dive into the details with comparative ease. The thing is, not all digital data are created equally. Google has spent millions digitizing <em>everything</em>; newspapers have digitized their own collections. Genealogists and local historical societies upload yoinks of digitized photographs, wills, local tax records, <a href="http://www.bytown.net/">you-name-it</a>, <em>every day</em>. But, consider what Milligan has to say about <a href="http://ianmilligan.ca/2012/03/26/illusionary-order-cautionary-notes-for-online-newspapers/">'illusionary order'</a>:</p>
<blockquote>
<p>[...] poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it’s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I’ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box.</p>
</blockquote>
<p>Ask yourself: what are some of the key dangers? Reflect: how have you used digitized resources uncritically in the past? Remember: 'To digitize' doesn't - or shouldn't - mean uploading a photograph of a document. There's a lot more going on that that. We'll get to that in a moment.</p>
<h1 id="exercise-1-the-dream-case">EXERCISE 1: The Dream Case</h1>
<p>In the dream case, your data are not just images, but are actually sorted and structured into some kind of pre-existing database. There are choices made in the <em>creation</em> of the database, but a good database, a good project, will lay out for you their decision making, their corpora, and how they've dealt with ambiguity and so on. You search using a robust interface, and you get a well-formed spreadsheet of data in return. Two examples of 'dream case' data:</p>
<ul>
<li><a href="http://edh-www.adw.uni-heidelberg.de/inschrift/suche">Epigraphic Database Heidelberg</a></li>
<li><a href="http://www.cwgc.org/find-war-dead.aspx">Commwealth War Graves Commission, Find War Dead</a></li>
</ul>
<p>Explore both databases. Perform a search of interest to you. In the case of the epigraphic database, if you've done any Latin, try searching for terms related to occupations; or you could search '<a href="http://www.latin-dictionary.org/Latin-English-Dictionary/figlina">Figlin*</a>'. In the CWGC database, search your own surname. Download your results. You now have data that you can explore! Using the nano text editor in your DHBox, make a record (or records) of what you searched, the URL for your search &amp; its results, and where you're keeping your data. Lodge a copy of this record in your repository.</p>
<h1 id="exercise-2-wget">EXERCISE 2: Wget</h1>
<p>You've already encountered wget in the introduction to this workbook, when you were setting up your DHBox to use Pandoc. In this exercise, I want you to do <a href="http://programminghistorian.org/lessons/automated-downloading-with-wget">Ian Milligan's wget tutorial at the Programming Historian</a> to learn more about the power of this command, and how to wield that power properly. Skip ahead to step 2, since your DHBox already has wget installed. (If you want to continue to use wget after this course is over, you will have to install it on your own machine, obviously).</p>
<p>Once you've completed Milligan's tutorial, remember to put your history into a new markdown file, and to lodge a copy of it in your repository.</p>
<p>Now that you're <em>au fait</em> with wget, I want you to use wget to download the Shawville Equity from the Quebec provincial archives in a responsible and respectful manner. Otherwise, you will look like a bot attacking their site. The data is in this location: <code>http://collections.banq.qc.ca:8008/jrn03/equity/src/</code>. Make a new directory: <code>$ mkdir equity</code> and then cd into it: <code>$ cd equity</code>. Make sure you're in the directory by typing <code>$ pwd</code>. Work out the command to download the Equity data; configure your command to retrieve only the txt files (as the pdfs are large and will take much time and bandwidth to acquire.) (Ok, I'm not heartless - scroll to the bottom of this page to see what the command should be).</p>
<p>Add your command to your history file, and lodge it in your repository.</p>
<p>Open some of the text files in Nano. How good, how careful was the 'object character recognition'? Part of the point of working with the Equity files is to show that even with horrible 'digitization', we can still extract useful insights. Digitization is more than simply throwing automatically generated files online. Good digitization requires scholarly work and effort! We will learn how to do this properly in the next exercise.</p>
<h1 id="exercise-3-tei">EXERCISE 3: TEI</h1>
<p>Digitization requires human intervention. This can be as straightforward as correcting errors or adjusting the scanner settings when we do OCR, or it can be the rather more involved work of adding a layer of semantic information to the text. When we mark up a text with the semantic hooks and signs that explain we are talking about 'London, Ontario' rather than 'London, UK', we've made the text a whole lot more useful for other scholars or tools. In this exercise, you will do some basic marking up of a text using standards from the <a href="http://www.tei-c.org/index.xml">Text Encoding Initiative</a>. (Some of the earliest digital history work was along these lines). The TEI exercise requires carefully attention to detail. Read through it before you try it. In this exercise, you'll transcribe a page from an abolitionist's pamphlet. You'll also think about ways of transforming the resulting XML into other formats. Make notes in a file to upload to your repository, and upload your XML and your XSL file to your own repository as well. (As an added optional challenge, create a gh-pages branch and figure out the direct URL to your XML file, and email that URL to me).</p>
<p>The exercise may be found <a href="../../supporting materials/tei/">here</a>.</p>
<p>I will note that a perfectly fine final project for HIST3814 might be to use this exercise as a model to markup a single issue of the Equity and suggest ways this material might be explored. Remember to make (and lodge in your repository) a file detailing the work you've done and any issues you've run into.</p>
<h1 id="exercise-4-apis">EXERCISE 4: APIs</h1>
<p>Sometimes, a website will have what is called an 'application programming interface'. In essence, this lets a program on your computer talk to the computer serving the website you're interested in, such that the website gives you the data that you're looking for.</p>
<p>That is, instead of <em>you</em> punching in the search terms, and copying and pasting the results, you get the computer to do it. More or less. The thing is, the results come back to you in a machine-readable format - often, JSON, which is a kind of text format. It looks like this:
<img alt="Imgur" src="http://i.imgur.com/LtZWyle.png" /></p>
<p>The 'Canadiana Discovery Portal' has tonnes of materials related to Canada's history, from a wide variety of sources. Its search page is at: http://search.canadiana.ca/</p>
<ul>
<li>Go there, and search "ottawa" and set the date range to 1800 to 1900. Hit enter. You are presented with a page of results -56 249 results! That's a lot of data. But do you notice the address bar of your browser? It'll say something like this:</li>
</ul>
<p>http://search.canadiana.ca/search?q=ottawa&amp;field=&amp;df=1900&amp;dt=1900</p>
<p>Your search query has been put into the URL. You're looking at the API! Everything after /search is a command that you are sending to the Canadiana server.</p>
<p>Scroll through the results, and you'll see a number just before the ?</p>
<p>http://search.canadiana.ca/search/2?df=1800&amp;dt=1900&amp;q=ottawa&amp;field=</p>
<p>http://search.canadiana.ca/search/3?df=1800&amp;dt=1900&amp;q=ottawa&amp;field=</p>
<p>http://search.canadiana.ca/search/4?df=1800&amp;dt=1900&amp;q=ottawa&amp;field=</p>
<p>....all the way up to 5625 (ie, 10 results per page, so 56249 / 10).</p>
<p>If you go to http://search.canadiana.ca/support/api you can see the full list of options. What we are particularly interested in now is the bit that looks like this:</p>
<p><code>&amp;fmt=json</code></p>
<p>Add that to your query URL. How different the results now look! What's nice here is that the data is formatted in a way that makes sense to a machine - which we'll learn more about in due course.</p>
<p>If you look back at the full list of API options, you'll see at the bottom that one of the options is 'retrieving individual item records'; the key for that is a field called 'oocihm'. If you look at your page of json results, and scroll through them, you'll see that each individual record has its own oocihm number. If we could get a list of those, we'd be able to programmatically slot them into the commands for retrieving individual item records:</p>
<p>http://search.canadiana.ca/view/oocihm.16278/?r=0&amp;s=1&amp;fmt=json&amp;api_text=1</p>
<p>The problem is: how to retrieve those oocihm numbers. The answer is, 'we write a program'. And the program that you want can be <a href="http://ianmilligan.ca/api-example-sh/">found here</a>. Study that program carefully. There are a number of useful things happening in there, notably 'curl', 'jq', 'sed', 'awk'. curl  is a program for downloading webpages, jq for dealing with json, and sed and awk for searching within and cleaning up text. If this all sounds greek to you, there is an excellent gentle introduction over at <a href="http://williamjturkel.net/2013/06/15/basic-text-analysis-with-command-line-tools-in-linux/">William Turkel's blog</a>.</p>
<p>So here's what we're going to do.</p>
<ol>
<li>We need jq. We install it into our DHBox with <code>$ sudo apt-get install jq -y</code></li>
<li>We need to create a program. Make a new directory for this exercise like so: <code>$ mkdir m2e4</code>. Then, change into that directory by typing <code>$ cd m2e4</code>. Make sure that's where you are by typing <code>$ pwd</code>. Now, make an empty file for our program with <code>$ touch canadiana.sh</code>. Touch makes an empty file; the .sh in the filename indicates that this is a shell script.</li>
<li>Open the empty file with <code>$ nano canadiana.sh</code>. Now, the program that Ian Milligan wrote makes calls to the API that <em>used to live</em> at eco.canadiana.ca. But note the error message <a href="http://eco.canadiana.ca/view/oocihm.16278/?r=0&amp;s=1&amp;fmt=json&amp;api_text=1">here</a>. So we have to change Milligan's script so that it points to the API at search.canadiana.ca. Copy the script below into your empty canadiana.sh. If you want, adjust the search parameters (in the line starting with <code>pages</code>) for material you're more interested in.</li>
</ol>
<pre><code class="bash">#! /bin/bash

pages=$(curl 'http://search.canadiana.ca/search?q=ottawa*&amp;field=&amp;so=score&amp;df=1914&amp;dt=1918&amp;fmt=json' | jq '.pages')

# this goes into the results and reads the value of 'pages' in each one of them.
# it then tells us how many pages we're going to have.

echo &quot;Pages:&quot;$pages

# this says 'for each one of these pages, download the 'key' value on each page'

for i in $(seq 1 $pages)
do
        curl 'http://search.canadiana.ca/search/'${i}'?q=montenegr*&amp;field=&amp;so=score&amp;df=1914&amp;dt=1918&amp;fmt=json' | jq '.docs[] | {key}' &gt;&gt; results.txt
done

# the next two lines take the results.txt file that is quite messy and clean it up. I'll try to explain what they all mean. Basically, tr deletes a given character - so we delete quotation marks &quot;\&quot;&quot; (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), to erase spaces, and to find the phrase &quot;key:&quot; and delete it too.

sed -e 's/\&quot;key\&quot;: \&quot;//g' results.txt | tr -d &quot;\&quot;&quot; | tr -d &quot;{&quot; | tr -d &quot;}&quot; | tr -s &quot; &quot; | sed '/^\s*$/d' | tr -d ' ' &gt; cleanlist.txt
# this adds a prefix and a suffix.

awk '$0=&quot;search.canadiana.ca/view/&quot;$0' cleanlist.txt| awk '{print $0 &quot;/1?r=0&amp;s=1&amp;fmt=json&amp;api_text=1&quot;}' &gt; urlstograb.txt

# then if we want we can take those URLs and output them all to a big text file for analysis.

wget -i urlstograb.txt -O output.txt
</code></pre>

<p>Hit ctrl+x to exit Nano, and save the changes.</p>
<p>Before we can run this program, we have to tell DHBox that it is alright to run it. We do that by changing the 'permissions' on the file, like so:</p>
<p><code>$ chmod 755 canadiana.sh</code></p>
<p>And now we can run the program:</p>
<p><code>$ ./canadiana.sh</code></p>
<p>Ta da! You now have a pretty powerful tool now for grabbing data from one of the largest portals for Canadian history! Download your output.txt file to your computer via the file manager and have a look at it. Make sure to make a file noting what you've done, commands you've made, etc, and lodge it in your repository.</p>
<h1 id="exercise-5-mining-twitter">EXERCISE 5: Mining Twitter</h1>
<p>Ed Summers is part of a project called '<a href="http://www.docnow.io/">Documenting the Now</a>' which is developing tools to collect and understand the historical materials being shared (and lost) on social media. One component of Documenting the Now is the Twitter Archiving Tool, '<a href="https://github.com/DocNow/twarc">Twarc</a>'. In this exercise, you are going to use Twarc to create an archive of Tweets relevant to a current trending news topic.</p>
<ol>
<li>First of all, you need to set up a Twitter account, if you haven't already got one. Do so, but make sure to minimize any personal information that is exposed. For instance, don't make your handle the same as your real name. Turn off geolocation. Do not give your actual location in the profile. View the settings, and make sure all of the privacy settings are dialed down. For the time being, you <em>do</em> have to associate a cell phone number with your account. You can delete that once you've done the next step.</li>
<li>Go to <a href="https://apps.twitter.com/">https://apps.twitter.com/</a> and click on ‘new app’. Then, on the ‘new application’ page, just give your app a name like my-twarc’ or similar, and website use the Crafting Digital History site url (although for our purposes any website will do). You don’t need to fill in any of the rest of the fields. Continue on to the next page (tick off the box saying you’ve read the developer code of behaviour). This next page shows you all the details about your new application.</li>
<li>Click on the ‘keys and access tokens’ tab. Copy the consumer key, the consumer secret to a text file. Click on the ‘create access tokens’ at the bottom of the page. This generates an access token and an access secret. Copy those to your text file, save it. <strong>do not put this file in your repo or leave it online anywhere</strong> <img alt="Imgur" src="http://i.imgur.com/mM4hZNN.png" /></li>
<li>In your DHbox, at the command line, type <code>$ sudo pip install twarc</code>. Twarc is written in python, which is already installed in DHbox. 'Pip' is a package manager for installing new python modules and packages. If you forget the <code>sudo</code>, you will get an error to the effect that you don't have permission. So sudo!</li>
<li>Now type <code>$ twarc configure</code> and give it the information it asks for (your consumer secret etc).</li>
<li>You're now ready to search. For instance, <code>$ twarc search canada150 &gt; search.json</code> will search Twitter for posts using the canada150 hashtag. Note that Twitter only gives access to the last two weeks or so via search. For grabbing the stream <em>as an event happens</em> you'd use the <code>twarc stream</code> command - see the Twarc documentation for more.</li>
<li>It might take some time for the search to happen. You can always force-stop the search by hitting ctrl+c. If you do that though there could be an error in the formatting of the file which will throw an error when you get to step 10. You can still open the json in a text editor though, but you will have to go to the end of the file and fix the formatting.</li>
<li>The data being collected is in json format. That is, a list of 'keys' and 'values'. This is a handy format for computers, and some data visualization platforms require data in this format. For our purposes we might want to transform the json into a csv (comma separated) table - a spreadsheet.</li>
<li>We will install a command that can convert the json to csv format like so: <code>$ sudo npm install json2csv --save -g</code>. Full details about the command are <a href="https://github.com/zemirco/json2csv#command-line-interface">here</a>.</li>
<li>Convert your <code>search.json</code> to csv like so: <code>json2csv -i search.json -o out.csv</code></li>
<li>Examine your data either in a text editor or in a spreadsheet. Use twarc to create a file with a list of ids. Lodge this list and your history and notes in your repository.</li>
</ol>
<p>NB. Twitter forbids the sharing of the full metadata of a collection of tweets. You may however share a list of tweet IDs. See the Twarc documentation for the instructions on how to do that.</p>
<p>What can you do with this data? Examine the Twarc repository, especially its utilities. You could extract the geolocated ones and map them. You could examine the difference between 'male' and 'female' tweeters (and how problematic might that be?). In your csv, save the text of the posts to a new file and upload it to something like <a href="http://voyant-tools.org">Voyant</a> to visualize trends over time. Google for analyzes of twitter data to get some ideas.</p>
<h1 id="exercise-6-using-tesseract-to-turn-a-pdf-into-text">EXERCISE 6: Using Tesseract to turn a pdf into text</h1>
<p>We've all used pdfs - in academia, we often use a pdf to make sure that a page of text looks like an actual physical page of paper. PDFs always look the same on whatever machine they are displayed on, because they contain within themselves the complete description of what the 'page' should look like. If you've ever selected text within a pdf, you were only able to do this because there was within the pdf a text layer on top of the image layer. But when we digitize old newspapers, the pdf that results only contains the image layer, not the text. To turn that image into text, we have to do what's called 'object character recognition', or OCR. An OCR algorithm looks at the pattern of pixels in the image, and maps these against the shapes it 'knows' to be an A, or an a, or a B, or a &amp;, and so on. Cleaner, sharper printing gives better results as do hi resolution images free from noise. People who have a lot of material to OCR use some very powerful tools to identify blocks of text within the newspaper page, and then train the machine to identify these, a process beyond us just now (but see <a href="https://stackoverflow.com/questions/28591117/how-do-i-segment-a-document-using-tesseract-then-output-the-resulting-bounding-b#28640570">this q &amp; a on stackoverflow</a> if you're interested).</p>
<p>In this exercise, you'll:</p>
<ol>
<li>install the Tesseract OCR engine into your DHBox</li>
<li>download an edition of the Equity</li>
<li>install and use pdftk to burst the pdf into individual one-page files</li>
<li>install and use imagemagick to convert the pdf into tiff image format</li>
<li>use Tesseract to OCR the resulting pages.</li>
</ol>
<p>Begin by making a new director for this exercise: <code>mkdir ocr-test</code>. Change directories into it: <code>cd ocr-test</code></p>
<ol>
<li><code>$ sudo apt-get install tesseract-ocr</code> will grab the latest version of tesseract and install it into your dhbox. Enter your password when the computer asks for it.</li>
<li><code>$ sudo apt-get install imagemagick</code> to install imagemagick</li>
<li><code>$ sudo apt-get install pdftk</code> to install pdftk.</li>
<li>Now let's grab an edition of the Equity. Use wget to grab the pdf from July 4th 1950.</li>
<li>Let's burst it into individual pages. The command is <code>pdtk &lt;input file&gt; burst</code>, so <code>$ pdftk 83471_1957-07-04.pdf burst</code></li>
<li>Let's convert the first file to tiff with imagemagick's convert command: <code>$ convert -density 300 pg_0001.pdf -depth 8 -strip -background white -alpha off file.tiff</code> You want a high density image, which is what the -density and the -depth flags do; the rest of the command formats the image in a way that Tesseract expects to encounter text. This command might take a while. Just wait, be patient.</li>
<li>Extract text! <code>$ tesseract file.tiff output.txt</code> This might also take some time.</li>
</ol>
<p>Download the output.txt file with the DHBox filemanager. Open the file with a text editor (there might be a lot of white space at the start of the file, fyi). Grab the txt file created by the Provincial Archives. Is yours better or worse than theirs? Look up the <a href="https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage">Tesseract wiki</a>. What other options could you use with the tesseract command to improve the results? For future reference, here are two guides to automating bulk OCR (multiple files) with tesseract: <a href="https://diging.atlassian.net/wiki/display/DCH/Tutorial%3A+Text+Extraction+and+OCR+with+Tesseract+and+ImageMagick">Peirson's</a>, <a href="http://benschmidt.org/dighist13/?page_id=129">Schmidt's</a>.</p>
<h3 id="wget-command-to-grab-the-equity">wget command to grab The Equity</h3>
<p>Hey - you've scrolled all the way down here. Here's the wget command to nicely download the Equity txt files:
<code>wget http://collections.banq.qc.ca:8008/jrn03/equity/src/ -A .txt -r --no-parent -nd –w 2 --limit-rate=20k</code></p>
<p>When you run this command, it will go through the entire file structure a couple of times, 'rejecting' index.html etc because we've asked it to just grab the .txt files. Be patient. How would you change it to grab just the pdfs?</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../module-3/Wrangling Data/" class="btn btn-neutral float-right" title="Data is messy"/>Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Finding Data/" class="btn btn-neutral" title="How do we find data?"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
    <!-- Copyright etc -->
    </p>
  </div>

The Crafting Digital History site is built with <a href="http://www.mkdocs.org">MkDocs</a> using a theme provided by <a href="https://readthedocs.org">Read the Docs</a>. All documents are available on <a href="https://github.com/shawngraham/dhworkbook/tree/gh-pages"> GitHub</a>. I thank Reclaim Hosting for their help, support, and example in getting this site up and running.
</footer>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.1.0/jquery.fitvids.min.js"></script>
<script>
  $(document).ready(function(){
    // Target your .container, .wrapper, .post, etc.
    $(".section").fitVids();
  });
</script>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../Finding Data/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../module-3/Wrangling Data/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
